<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <meta name="author" content="Evan Linde" />
  <meta name="date" content="2017-10-19" />
  <title>Converting MACA Data for Envision and SWAT Models</title>
  <style type="text/css">code{white-space: pre;}</style>
  <style type="text/css">
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; }
code > span.dt { color: #902000; }
code > span.dv { color: #40a070; }
code > span.bn { color: #40a070; }
code > span.fl { color: #40a070; }
code > span.ch { color: #4070a0; }
code > span.st { color: #4070a0; }
code > span.co { color: #60a0b0; font-style: italic; }
code > span.ot { color: #007020; }
code > span.al { color: #ff0000; font-weight: bold; }
code > span.fu { color: #06287e; }
code > span.er { color: #ff0000; font-weight: bold; }
  </style>
  <link rel="stylesheet" href="doc/pandoc.css" type="text/css" />
  <script type="text/javascript">/*<![CDATA[*/
  /* 
  March 19, 2004 MathHTML (c) Peter Jipsen http://www.chapman.edu/~jipsen
  Released under the GNU General Public License version 2 or later.
  See the GNU General Public License (at http://www.gnu.org/copyleft/gpl.html)
  for more details.
  */
  
  function convertMath(node) {// for Gecko
    if (node.nodeType==1) {
      var newnode = 
        document.createElementNS("http://www.w3.org/1998/Math/MathML",
          node.nodeName.toLowerCase());
      for(var i=0; i < node.attributes.length; i++)
        newnode.setAttribute(node.attributes[i].nodeName,
          node.attributes[i].nodeValue);
      for (var i=0; i<node.childNodes.length; i++) {
        var st = node.childNodes[i].nodeValue;
        if (st==null || st.slice(0,1)!=" " && st.slice(0,1)!="\n") 
          newnode.appendChild(convertMath(node.childNodes[i]));
      }
      return newnode;
    }
    else return node;
  }
  
  function convert() {
    var mmlnode = document.getElementsByTagName("math");
    var st,str,node,newnode;
    for (var i=0; i<mmlnode.length; i++)
      if (document.createElementNS!=null)
        mmlnode[i].parentNode.replaceChild(convertMath(mmlnode[i]),mmlnode[i]);
      else { // convert for IE
        str = "";
        node = mmlnode[i];
        while (node.nodeName!="/MATH") {
          st = node.nodeName.toLowerCase();
          if (st=="#text") str += node.nodeValue;
          else {
            str += (st.slice(0,1)=="/" ? "</m:"+st.slice(1) : "<m:"+st);
            if (st.slice(0,1)!="/") 
               for(var j=0; j < node.attributes.length; j++)
                 if (node.attributes[j].nodeValue!="italic" &&
                   node.attributes[j].nodeValue!="" &&
                   node.attributes[j].nodeValue!="inherit" &&
                   node.attributes[j].nodeValue!=undefined)
                   str += " "+node.attributes[j].nodeName+"="+
                       "\""+node.attributes[j].nodeValue+"\"";
            str += ">";
          }
          node = node.nextSibling;
          node.parentNode.removeChild(node.previousSibling);
        }
        str += "</m:math>";
        newnode = document.createElement("span");
        node.parentNode.replaceChild(newnode,node);
        newnode.innerHTML = str;
      }
  }
  
  if (document.createElementNS==null) {
    document.write("<object id=\"mathplayer\"\
    classid=\"clsid:32F66A20-7614-11D4-BD11-00104BD3F987\"></object>");
    document.write("<?import namespace=\"m\" implementation=\"#mathplayer\"?>");
  }
  if(typeof window.addEventListener != 'undefined'){
    window.addEventListener('load', convert, false);
  }
  if(typeof window.attachEvent != 'undefined') {
    window.attachEvent('onload', convert);
  }
  /*]]>*/
  </script>
</head>
<body>
<div id="header">
<h1 class="title">Converting MACA Data for Envision and SWAT Models</h1>
<h2 class="author">Evan Linde</h2>
<h3 class="date">October 19, 2017</h3>
</div>
<div id="TOC">
<ul>
<li><a href="#introduction">Introduction</a><ul>
<li><a href="#software">Software</a></li>
<li><a href="#acknowledgement-and-disclaimer">Acknowledgement and Disclaimer</a></li>
</ul></li>
<li><a href="#macav2-metdata">MACAv2-METDATA</a><ul>
<li><a href="#downloading">Downloading</a></li>
<li><a href="#examining-the-data">Examining the Data</a><ul>
<li><a href="#ncdump">ncdump</a></li>
<li><a href="#viewing-headers-and-metadata">Viewing Headers and Metadata</a></li>
<li><a href="#viewing-coordinate-data">Viewing Coordinate Data</a></li>
<li><a href="#viewing-variable-data">Viewing Variable Data</a></li>
</ul></li>
<li><a href="#subsetting">Subsetting</a><ul>
<li><a href="#cimarron">Cimarron</a></li>
<li><a href="#kiamichi">Kiamichi</a></li>
</ul></li>
<li><a href="#converting-for-envision-models">Converting for Envision Models</a><ul>
<li><a href="#general-procedures">General Procedures</a></li>
<li><a href="#calculating-year-boundaries">Calculating Year Boundaries</a></li>
<li><a href="#kiamichi-1">Kiamichi</a></li>
<li><a href="#cimarron-1">Cimarron</a></li>
</ul></li>
<li><a href="#converting-for-swat-models">Converting for SWAT Models</a><ul>
<li><a href="#setup-steps">Setup Steps</a></li>
<li><a href="#the-actual-conversion">The Actual Conversion</a></li>
<li><a href="#first-approach-bash-methods">First Approach: Bash Methods</a></li>
<li><a href="#second-approach-python-methods">Second Approach: Python Methods</a></li>
<li><a href="#cimarron-2">Cimarron</a></li>
</ul></li>
</ul></li>
<li><a href="#metdata">METDATA</a><ul>
<li><a href="#downloading-1">Downloading</a></li>
<li><a href="#examining-the-data-1">Examining the Data</a></li>
<li><a href="#subsetting-1">Subsetting</a><ul>
<li><a href="#cimarron-3">Cimarron</a></li>
<li><a href="#kiamichi-2">Kiamichi</a></li>
</ul></li>
<li><a href="#converting-for-envision">Converting for Envision</a><ul>
<li><a href="#kiamichi-3">Kiamichi</a></li>
<li><a href="#cimarron-4">Cimarron</a></li>
</ul></li>
<li><a href="#converting-for-swat">Converting for SWAT</a><ul>
<li><a href="#cimarron-5">Cimarron</a></li>
</ul></li>
</ul></li>
<li><a href="#conclusion">Conclusion</a><ul>
<li><a href="#subsets">Subsets</a></li>
<li><a href="#ensembles">Ensembles</a></li>
<li><a href="#missing-variables">Missing Variables</a></li>
<li><a href="#useful-services">Useful Services</a><ul>
<li><a href="#maca-data-portal">MACA Data Portal</a></li>
<li><a href="#thredds-catalog-for-metdata">THREDDS Catalog for METDATA</a></li>
<li><a href="#opendap">OPeNDAP</a></li>
</ul></li>
<li><a href="#lessons-learned">Lessons Learned</a></li>
</ul></li>
</ul>
</div>
<h1 id="introduction">Introduction</h1>
<p>This is a narrative of the process we followed to make data from the <a href="https://climate.northwestknowledge.net/MACA/MACAproducts.php#MACAproductcomparison">MACAv2-METDATA</a> and <a href="https://climate.northwestknowledge.net/METDATA/">METDATA</a> datasets usable in <a href="http://envision.bioe.orst.edu/">Envision</a> and <a href="http://swat.tamu.edu/">SWAT</a> models.</p>
<p>We attempt to provide clearly written code, commentary, and syntax explanation sufficient for someone with only introductory programming experience (e.g. like having attended a <a href="https://software-carpentry.org/">Software Carpentry</a> workshop) to be able to follow and make use of the work shown here. The full explanation of a step or any command or code syntax we use should be found the first time we use it; later uses may contain shorter explanations or none at all.</p>
<h3 id="software">Software</h3>
<p>Software we used:</p>
<ol>
<li>The netCDF library/binaries (the <code>ncdump</code> command)<br /></li>
<li><a href="http://nco.sourceforge.net/">netCDF Operators</a><br /></li>
<li><a href="http://www.qgis.org/">QGIS</a><br /></li>
<li><code>bash</code> (and common command line utilities)<br /></li>
<li><code>python</code> (specifically the <a href="https://www.anaconda.com/">Anaconda</a> distribution)</li>
</ol>
<p>All the steps shown can be performed on Windows, Linux/Unix, and Mac platforms. All the software and tools we used in this process are freely available. The <code>bash</code> shell, used for most of the scripting, is not native to Windows but is available in several software packages including <a href="https://git-scm.com/">git</a>, <a href="https://www.cygwin.com/">Cygwin</a>, and <a href="http://www.msys2.org/">MSYS2</a>.</p>
<h3 id="acknowledgement-and-disclaimer">Acknowledgement and Disclaimer</h3>
<p>Computation for this project was performed on TIGER, the research &quot;cloud&quot; at the Oklahoma State University <a href="https://hpcc.okstate.edu/">High Performance Computing Center</a>.</p>
<p>This material is based on work supported by the National Science Foundation under Grant No. <a href="https://www.nsf.gov/awardsearch/showAward?AWD_ID=1301789">OIA-1301789</a>. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the National Science Foundation or Oklahoma State Regents for Higher Education.</p>
<h1 id="macav2-metdata">MACAv2-METDATA</h1>
<h2 id="downloading">Downloading</h2>
<p>Want to get:</p>
<ul>
<li>MACAv2-METDATA, daily, 2021-2099, rcps 4.5 and 8.5<br /></li>
<li>9 Variables: tasmax, tasmin, rhsmax, rhsmin, huss, pr, rsds, uas, vas<br /></li>
<li>2 RCPs: rcp45, rcp85<br /></li>
<li>16 year blocks: 2021_2025, 2026_2030, 2031_2035, 2036_2040, 2041_2045, 2046_2050, 2051_2055, 2056_2060, 2061_2065, 2066_2070, 2071_2075, 2076_2080, 2081_2085, 2086_2090, 2091_2095, 2096_2099<br /></li>
<li>20 models: bcc-csm1-1-m, bcc-csm1-1, BNU-ESM, CanESM2, CCSM4, CNRM-CM5, CSIRO-Mk3-6-0, GFDL-ESM2G, GFDL-ESM2M, HadGEM2-CC365, HadGEM2-ES365, inmcm4, IPSL-CM5A-LR, IPSL-CM5A-MR, IPSL-CM5B-LR, MIROC-ESM-CHEM, MIROC-ESM, MIROC5, MRI-CHCM3, NorESM1-M</li>
</ul>
<p>Since we are interested in ultimately creating several different regional subsets we decided to download the full CONUS data and make our own subsets rather than task MACA's servers with this. The MACA data portal provides this link for direct file access: <a href="http://climate.nkn.uidaho.edu/MACAV2METDATA/MACAV2/">http://climate.nkn.uidaho.edu/MACAV2METDATA/MACAV2/</a>.</p>
<p>Under this direct access link, we see a directory for each model and in each model directory, files named with a pattern like <code>macav2metdata_{variable}_{model}_r1i1p1_{rcp}_{year-block}_CONUS_daily.nc</code>. The files appear to be around 1GB to 2.5GB in size.</p>
<p>Since the files all seem to be predictably named, we create a bash script, <code>download.sh</code>, to automate the download process.</p>
<p><strong><code>download.sh</code></strong></p>
<pre class="sourceCode bash"><code class="sourceCode bash"><span class="co">#!/bin/bash</span>
<span class="co">#</span>
<span class="co"># Download netCDF files for MACAv2-METDATA</span>
<span class="co">#</span>

<span class="co"># Declare arrays for the models, year blocks, climate variables, and RCPs.</span>
<span class="co"># These arrays will be used to build the URLs for the netCDF files we want</span>
<span class="co"># to download.</span>

<span class="ot">MODELS=(</span><span class="st">&quot;bcc-csm1-1-m&quot;</span> <span class="st">&quot;bcc-csm1-1&quot;</span> <span class="st">&quot;BNU-ESM&quot;</span> <span class="st">&quot;CanESM2&quot;</span> <span class="st">&quot;CCSM4&quot;</span> <span class="st">&quot;CNRM-CM5&quot;</span> 
    <span class="st">&quot;CSIRO-Mk3-6-0&quot;</span> <span class="st">&quot;GFDL-ESM2G&quot;</span> <span class="st">&quot;GFDL-ESM2M&quot;</span> <span class="st">&quot;HadGEM2-CC365&quot;</span> <span class="st">&quot;HadGEM2-ES365&quot;</span> 
    <span class="st">&quot;inmcm4&quot;</span> <span class="st">&quot;IPSL-CM5A-LR&quot;</span> <span class="st">&quot;IPSL-CM5A-MR&quot;</span> <span class="st">&quot;IPSL-CM5B-LR&quot;</span> <span class="st">&quot;MIROC-ESM-CHEM&quot;</span> 
    <span class="st">&quot;MIROC-ESM&quot;</span> <span class="st">&quot;MIROC5&quot;</span> <span class="st">&quot;MRI-CGCM3&quot;</span> <span class="st">&quot;NorESM1-M&quot;</span>)

<span class="ot">YEAR_BLOCKS=(</span><span class="st">&quot;2021_2025&quot;</span> <span class="st">&quot;2026_2030&quot;</span> <span class="st">&quot;2031_2035&quot;</span> <span class="st">&quot;2036_2040&quot;</span> <span class="st">&quot;2041_2045&quot;</span> 
    <span class="st">&quot;2046_2050&quot;</span> <span class="st">&quot;2051_2055&quot;</span> <span class="st">&quot;2056_2060&quot;</span> <span class="st">&quot;2061_2065&quot;</span> <span class="st">&quot;2066_2070&quot;</span> <span class="st">&quot;2071_2075&quot;</span> 
    <span class="st">&quot;2076_2080&quot;</span> <span class="st">&quot;2081_2085&quot;</span> <span class="st">&quot;2086_2090&quot;</span> <span class="st">&quot;2091_2095&quot;</span> <span class="st">&quot;2096_2099&quot;</span>)

<span class="ot">VARS=(</span><span class="st">&quot;tasmax&quot;</span> <span class="st">&quot;tasmin&quot;</span> <span class="st">&quot;rhsmax&quot;</span> <span class="st">&quot;rhsmin&quot;</span> <span class="st">&quot;huss&quot;</span> <span class="st">&quot;pr&quot;</span> <span class="st">&quot;rsds&quot;</span> <span class="st">&quot;uas&quot;</span> <span class="st">&quot;vas&quot;</span><span class="ot">)</span>

<span class="ot">RCPS=(</span><span class="st">&quot;rcp45&quot;</span> <span class="st">&quot;rcp85&quot;</span><span class="ot">)</span>

<span class="co"># Beginning part of all the URLs we want</span>
<span class="ot">base_url=</span><span class="st">&quot;https://climate.northwestknowledge.net/MACAV2METDATA/MACAV2&quot;</span>

<span class="kw">echo</span> <span class="st">&quot;Begin: </span><span class="ot">$(</span><span class="kw">date</span><span class="ot">)</span><span class="st">&quot;</span>

<span class="kw">for</span> <span class="kw">model</span> in <span class="ot">${MODELS[@]}</span><span class="kw">;</span> <span class="kw">do</span>

    <span class="kw">echo</span> <span class="st">&quot;Start downloading model </span><span class="ot">${model}</span><span class="st"> </span><span class="ot">$(</span><span class="kw">date</span><span class="ot">)</span><span class="st">&quot;</span>
    <span class="kw">mkdir</span> -p <span class="st">&quot;</span><span class="ot">${model}</span><span class="st">&quot;</span>

    <span class="co"># Go into each model directory to download files for that model. </span>
    <span class="co"># We can get back to our current directory later with the popd command.</span>
    <span class="kw">pushd</span> <span class="st">&quot;</span><span class="ot">${model}</span><span class="st">&quot;</span>

    <span class="kw">for</span> <span class="kw">wvar</span> in <span class="ot">${VARS[@]}</span><span class="kw">;</span> <span class="kw">do</span>

        <span class="kw">for</span> <span class="kw">block</span> in <span class="ot">${YEAR_BLOCKS[@]}</span><span class="kw">;</span> <span class="kw">do</span>

            <span class="kw">for</span> <span class="kw">rcp</span> in <span class="ot">${RCPS[@]}</span><span class="kw">;</span> <span class="kw">do</span>

                <span class="co"># Build the URL we want to download using all the array </span>
                <span class="co"># variables that we&#39;re looping over.</span>
                <span class="co"># First generate the filename</span>
                <span class="ot">filename=</span><span class="st">&quot;macav2metdata_</span><span class="ot">${wvar}</span><span class="st">_</span><span class="ot">${model}</span><span class="st">_r1i1p1_</span><span class="ot">${rcp}</span><span class="st">_</span><span class="ot">${block}</span><span class="st">_CONUS_daily.nc&quot;</span>
                <span class="co"># Then the full URL</span>
                <span class="ot">url=</span><span class="st">&quot;</span><span class="ot">${base_url}</span><span class="st">/</span><span class="ot">${model}</span><span class="st">/</span><span class="ot">${filename}</span><span class="st">&quot;</span>

                <span class="co"># Start the download process backgrounded so we don&#39;t have to </span>
                <span class="co"># wait for it to finish before the next one starts.</span>
                <span class="kw">wget</span> <span class="ot">${url}</span> <span class="kw">&amp;</span>

            <span class="kw">done</span>   <span class="co"># end rcp loop</span>

            <span class="co"># Wait for backgrounded processes to finish so that we&#39;re not</span>
            <span class="co"># trying to download ALL the files at once. We&#39;re doing this just</span>
            <span class="co"># outside the rcp loop so that we only download two (length of </span>
            <span class="co"># RCPS array) files at a time.</span>
            <span class="kw">wait</span>  

        <span class="kw">done</span>  <span class="co"># end year block loop</span>

    <span class="kw">done</span>  <span class="co"># end wvar loop</span>

    <span class="co"># Go back to the directory we were working in when we ran pushd earlier</span>
    <span class="kw">popd</span>
    <span class="kw">echo</span> <span class="st">&quot;Finish downloading model </span><span class="ot">${model}</span><span class="st"> </span><span class="ot">$(</span><span class="kw">date</span><span class="ot">)</span><span class="st">&quot;</span>

<span class="kw">done</span>  <span class="co"># end model loop</span>

<span class="kw">echo</span> <span class="st">&quot;End: </span><span class="ot">$(</span><span class="kw">date</span><span class="ot">)</span><span class="st">&quot;</span></code></pre>
<p>The bash script has some extra output to give us some idea of how long everything takes.</p>
<p>The download process is expected to take a while (we're downloading 5760 files totalling over 10 TB) and we will be starting the script on TIGER's data transfer node (which won't be slowed down by the campus firewall). So we start the download script from within a <code>screen</code> session so that it won't be killed if the ssh session is interrupted.</p>
<pre class="sourceCode bash"><code class="sourceCode bash"><span class="kw">bash</span> download.sh <span class="kw">|</span> <span class="kw">tee</span> -a download.log</code></pre>
<p>Partway through the download process, we noticed that the directory for the CCSM4 model was empty. Upon further investigation, we noticed that the files under this model use the string &quot;r6i1p1&quot; instead of &quot;r1i1p1&quot; in their file names. So we made a second download script, <code>download2.sh</code>, to finish these files.</p>
<p><strong><code>download2.sh</code></strong></p>
<pre class="sourceCode bash"><code class="sourceCode bash"><span class="co">#!/bin/bash</span>
<span class="co">#</span>
<span class="co"># Download just the CCSM4 model from MACAv2-METDATA</span>
<span class="co">#</span>

<span class="ot">MODELS=(</span><span class="st">&quot;CCSM4&quot;</span><span class="ot">)</span>
<span class="ot">YEAR_BLOCKS=(</span><span class="st">&quot;2021_2025&quot;</span> <span class="st">&quot;2026_2030&quot;</span> <span class="st">&quot;2031_2035&quot;</span> <span class="st">&quot;2036_2040&quot;</span> <span class="st">&quot;2041_2045&quot;</span> 
    <span class="st">&quot;2046_2050&quot;</span> <span class="st">&quot;2051_2055&quot;</span> <span class="st">&quot;2056_2060&quot;</span> <span class="st">&quot;2061_2065&quot;</span> <span class="st">&quot;2066_2070&quot;</span> <span class="st">&quot;2071_2075&quot;</span> 
    <span class="st">&quot;2076_2080&quot;</span> <span class="st">&quot;2081_2085&quot;</span> <span class="st">&quot;2086_2090&quot;</span> <span class="st">&quot;2091_2095&quot;</span> <span class="st">&quot;2096_2099&quot;</span>)
<span class="ot">VARS=(</span><span class="st">&quot;tasmax&quot;</span> <span class="st">&quot;tasmin&quot;</span> <span class="st">&quot;rhsmax&quot;</span> <span class="st">&quot;rhsmin&quot;</span> <span class="st">&quot;huss&quot;</span> <span class="st">&quot;pr&quot;</span> <span class="st">&quot;rsds&quot;</span> <span class="st">&quot;uas&quot;</span> <span class="st">&quot;vas&quot;</span><span class="ot">)</span>
<span class="ot">RCPS=(</span><span class="st">&quot;rcp45&quot;</span> <span class="st">&quot;rcp85&quot;</span><span class="ot">)</span>

<span class="ot">base_url=</span><span class="st">&quot;https://climate.northwestknowledge.net/MACAV2METDATA/MACAV2&quot;</span>

<span class="kw">echo</span> <span class="st">&quot;Begin: </span><span class="ot">$(</span><span class="kw">date</span><span class="ot">)</span><span class="st">&quot;</span>
<span class="kw">for</span> <span class="kw">model</span> in <span class="ot">${MODELS[@]}</span><span class="kw">;</span> <span class="kw">do</span>
    <span class="kw">mkdir</span> -p <span class="st">&quot;</span><span class="ot">${model}</span><span class="st">&quot;</span>
    <span class="kw">pushd</span> <span class="st">&quot;</span><span class="ot">${model}</span><span class="st">&quot;</span>
    <span class="kw">for</span> <span class="kw">wvar</span> in <span class="ot">${VARS[@]}</span><span class="kw">;</span> <span class="kw">do</span>
        <span class="kw">for</span> <span class="kw">block</span> in <span class="ot">${YEAR_BLOCKS[@]}</span><span class="kw">;</span> <span class="kw">do</span>
            <span class="kw">for</span> <span class="kw">rcp</span> in <span class="ot">${RCPS[@]}</span><span class="kw">;</span> <span class="kw">do</span>
                <span class="ot">filename=</span><span class="st">&quot;macav2metdata_</span><span class="ot">${wvar}</span><span class="st">_</span><span class="ot">${model}</span><span class="st">_r6i1p1_</span><span class="ot">${rcp}</span><span class="st">_</span><span class="ot">${block}</span><span class="st">_CONUS_daily.nc&quot;</span>
                <span class="ot">url=</span><span class="st">&quot;</span><span class="ot">${base_url}</span><span class="st">/</span><span class="ot">${model}</span><span class="st">/</span><span class="ot">${filename}</span><span class="st">&quot;</span>
                <span class="kw">wget</span> <span class="ot">${url}</span> <span class="kw">&amp;</span>
            <span class="kw">done</span>
            <span class="kw">wait</span>
        <span class="kw">done</span>
    <span class="kw">done</span>
    <span class="kw">popd</span>
<span class="kw">done</span>
<span class="kw">echo</span> <span class="st">&quot;End: </span><span class="ot">$(</span><span class="kw">date</span><span class="ot">)</span><span class="st">&quot;</span></code></pre>
<p>Similarly, we start the <code>download2.sh</code> script in a <code>screen</code> session to finish the download.</p>
<pre class="sourceCode bash"><code class="sourceCode bash"><span class="kw">bash</span> download2.sh <span class="kw">|</span> <span class="kw">tee</span> -a download2.log</code></pre>
<p>The main download script also missed another model due to a typo. We corrected the type in the download script then also created a copy similar to <code>download2.sh</code> to download only the model that we missed.</p>
<p>The main download script downloaded 16 of the 18 models in just over two days:</p>
<pre><code>Begin: Mon Sep 25 15:58:35 CDT 2017
End: Wed Sep 27 18:39:49 CDT 2017</code></pre>
<p>The second download script (CCSM4) completed in about 1.5 hours:</p>
<pre><code>Begin: Wed Sep 27 19:08:32 CDT 2017
End: Wed Sep 27 20:36:38 CDT 2017</code></pre>
<p>After completing the downloads, we noticed that two models do not have the variables &quot;rhsmax&quot; and &quot;rhsmin&quot;.</p>
<p>These models are:</p>
<ol>
<li>CCSM4<br /></li>
<li>NorESM1-M</li>
</ol>
<p>This is actually noted in the MACA documentation at <a href="https://climate.northwestknowledge.net/MACA/GCMs.php">https://climate.northwestknowledge.net/MACA/GCMs.php</a>.</p>
<h2 id="examining-the-data">Examining the Data</h2>
<p>Some very informative information about the MACAv2-METDATA dataset can be found at<br /><a href="https://climate.northwestknowledge.net/MACA/MACAproducts.php#MACAproductcomparison">https://climate.northwestknowledge.net/MACA/MACAproducts.php#MACAproductcomparison</a>, but we'll need more detail than this for creating watershed subsets and converting data for use with our models.</p>
<h3 id="ncdump">ncdump</h3>
<p>The <code>ncdump</code> command is a utility associated with the netCDF library. It is available from <a href="http://www.unidata.ucar.edu/">Unidata</a> at <a href="https://www.unidata.ucar.edu/downloads/netcdf/index.jsp">https://www.unidata.ucar.edu/downloads/netcdf/index.jsp</a> in the form of source code or Windows binary. It is available via the package manager in many Linux distributions under either <code>netcdf</code> or <code>netcdf-bin</code>.</p>
<p>Note:</p>
<blockquote>
<p>A version of <code>ncdump</code> may also be installed by <code>conda</code> if you've used <code>conda</code> to download netCDF libraries for python. The <code>conda</code>-installed version seems to be a bit finnicky, sometimes giving errors like <code>*** ncdump: ncopen failed on ${filename}</code> where the regular version has no problems.</p>
</blockquote>
<blockquote>
<p>If you run into an error like this you can use the command <code>which -a</code> to see all the instances of <code>ncdump</code> in your <code>PATH</code>; the first line of the results is the full path to the default <code>ncdump</code> command. You can call another instance of <code>ncdump</code> by using its full path (e.g. <code>/usr/bin/ncdump</code>). You can avoid having to do this by creating an alias or editing your <code>PATH</code> variable, however these are beyond the scope of this document.</p>
</blockquote>
<h3 id="viewing-headers-and-metadata">Viewing Headers and Metadata</h3>
<p>We can view a netCDF file's header with a command like this:</p>
<pre class="sourceCode bash"><code class="sourceCode bash"><span class="kw">ncdump</span> -h file.nc</code></pre>
<p>Other options like <code>-c</code> or <code>-v variable</code> are useful for dumping coordinate information or values for a specific variable and these also include the header, but the option for the header alone, <code>-h</code>, is the most best way to get useful information without a lot of extra clutter. (The command form <code>ncdump file.nc</code> will dump the entire contents to your screen; you really don't want to do this with a large file.)</p>
<p>Here's a literal example:</p>
<pre class="sourceCode bash"><code class="sourceCode bash"><span class="kw">ncdump</span> -h CCSM4/macav2metdata_huss_CCSM4_r6i1p1_rcp45_2021_2025_CONUS_daily.nc</code></pre>
<p>And the output, i.e. the file's header / metadata:</p>
<pre><code>netcdf macav2metdata_huss_CCSM4_r6i1p1_rcp45_2021_2025_CONUS_daily {
dimensions:
    lat = 585 ;
    lon = 1386 ;
    time = 1826 ;
    crs = 1 ;
variables:
    double lat(lat) ;
        lat:long_name = &quot;latitude&quot; ;
        lat:standard_name = &quot;latitude&quot; ;
        lat:units = &quot;degrees_north&quot; ;
        lat:axis = &quot;Y&quot; ;
        lat:description = &quot;Latitude of the center of the grid cell&quot; ;
    double lon(lon) ;
        lon:long_name = &quot;longitude&quot; ;
        lon:standard_name = &quot;longitude&quot; ;
        lon:units = &quot;degrees_east&quot; ;
        lon:axis = &quot;X&quot; ;
        lon:description = &quot;Longitude of the center of the grid cell&quot; ;
    float time(time) ;
        time:units = &quot;days since 1900-01-01 00:00:00&quot; ;
        time:calendar = &quot;gregorian&quot; ;
        time:description = &quot;days since 1900-01-01&quot; ;
    float specific_humidity(time, lat, lon) ;
        specific_humidity:_FillValue = -9999.f ;
        specific_humidity:long_name = &quot;Daily Mean Near-Surface Specific Humidity&quot; ;
        specific_humidity:units = &quot;kg kg-1&quot; ;
        specific_humidity:grid_mapping = &quot;crs&quot; ;
        specific_humidity:standard_name = &quot;specific_humidity&quot; ;
        specific_humidity:height = &quot;0 m&quot; ;
        specific_humidity:cell_methods = &quot;time: mean(interval: 24 hours)&quot; ;
        specific_humidity:comments = &quot;Surface specific humidity&quot; ;
        specific_humidity:coordinates = &quot;time lon lat&quot; ;
    int crs(crs) ;
        crs:grid_mapping_name = &quot;latitude_longitude&quot; ;
        crs:longitude_of_prime_meridian = 0. ;
        crs:semi_major_axis = 6378137. ;
        crs:inverse_flattening = 298.257223563 ;

// global attributes:
        :description = &quot;Multivariate Adaptive Constructed Analogs (MACA) method, version 2.3,Dec 2013.&quot; ;
        :id = &quot;MACAv2-METDATA&quot; ;
        :naming_authority = &quot;edu.uidaho.reacch&quot; ;
        :Metadata_Conventions = &quot;Unidata Dataset Discovery v1.0&quot; ;
        :Metadata_Link = &quot;&quot; ;
        :cdm_data_type = &quot;GRID&quot; ;
        :title = &quot;Downscaled daily meteorological data of Daily Mean Near-Surface Specific Humidity from University of Miami - RSMAS (CCSM4) using the run r6i1p1 of the rcp45 scenario.&quot; ;
        :summary = &quot;This archive contains daily downscaled meteorological and hydrological projections for the Conterminous United States at 1/24-deg resolution utilizing the Multivariate Adaptive Constructed Analogs (MACA, Abatzoglou, 2012) statistical downscaling method with the METDATA (Abatzoglou,2013) training dataset. The downscaled meteorological variables are maximum/minimum temperature(tasmax/tasmin), maximum/minimum relative humidity (rhsmax/rhsmin)precipitation amount(pr), downward shortwave solar radiation(rsds), eastward wind(uas), northward wind(vas), and specific humidity(huss). The downscaling is based on the 365-day model outputs from different global climate models (GCMs) from Phase 5 of the Coupled Model Inter-comparison Project (CMIP3) utlizing the historical (1950-2005) and future RCP4.5/8.5(2006-2099) scenarios. Leap days have been added to the dataset from the average values between Feb 28 and Mar 1 in order to aid modellers.&quot; ;
        :keywords = &quot;daily precipitation, daily maximum temperature, daily minimum temperature, daily downward shortwave solar radiation, daily specific humidity, daily wind velocity, CMIP5, Gridded Meteorological Data&quot; ;
        :keywords_vocabulary = &quot;&quot; ;
        :standard_name_vocabulary = &quot;CF-1.0&quot; ;
        :history = &quot;No revisions.&quot; ;
        :comment = &quot;Surface specific humidity&quot; ;
        :geospatial_bounds = &quot;POLYGON((-124.7722 25.0631,-124.7722 49.3960, -67.0648 49.3960,-67.0648, 25.0631, -124.7722,25.0631))&quot; ;
        :geospatial_lat_min = &quot;25.0631&quot; ;
        :geospatial_lat_max = &quot;49.3960&quot; ;
        :geospatial_lon_min = &quot;-124.7722&quot; ;
        :geospatial_lon_max = &quot;-67.0648&quot; ;
        :geospatial_lat_units = &quot;decimal degrees north&quot; ;
        :geospatial_lon_units = &quot;decimal degrees east&quot; ;
        :geospatial_lat_resolution = &quot;0.0417&quot; ;
        :geospatial_lon_resolution = &quot;0.0417&quot; ;
        :geospatial_vertical_min = 0. ;
        :geospatial_vertical_max = 0. ;
        :geospatial_vertical_resolution = 0. ;
        :geospatial_vertical_positive = &quot;up&quot; ;
        :time_coverage_start = &quot;2021-01-01T00:0&quot; ;
        :time_coverage_end = &quot;2025-12-31T00:00&quot; ;
        :time_coverage_duration = &quot;P5Y&quot; ;
        :time_coverage_resolution = &quot;P1D&quot; ;
        :date_created = &quot;2014-05-15&quot; ;
        :date_modified = &quot;2014-05-15&quot; ;
        :date_issued = &quot;2014-05-15&quot; ;
        :creator_name = &quot;John Abatzoglou&quot; ;
        :creator_url = &quot;http://maca.northwestknowledge.net&quot; ;
        :creator_email = &quot;jabatzoglou@uidaho.edu&quot; ;
        :institution = &quot;University of Idaho&quot; ;
        :processing_level = &quot;GRID&quot; ;
        :project = &quot;&quot; ;
        :contributor_name = &quot;Katherine C. Hegewisch&quot; ;
        :contributor_role = &quot;Postdoctoral Fellow&quot; ;
        :publisher_name = &quot;&quot; ;
        :publisher_email = &quot;&quot; ;
        :publisher_url = &quot;&quot; ;
        :license = &quot;Creative Commons CC0 1.0 Universal Dedication(http://creativecommons.org/publicdomain/zero/1.0/legalcode)&quot; ;
        :coordinate_system = &quot;WGS84,EPSG:4326&quot; ;
}</code></pre>
<p>The parts of the header most relevant to our tasks are the <code>dimensions</code> and <code>variables</code> sections (i.e. everything before the line starting with <code>// global attributes</code> in the example above).</p>
<p>Each variable definition includes several metadata fields describing the variable data.</p>
<h3 id="viewing-coordinate-data">Viewing Coordinate Data</h3>
<p>We can view coordinate data (in the case of MACAv2-METDATA: latitude, longitude, and time) by calling ncdump with the <code>-c</code> flag.</p>
<pre class="sourceCode bash"><code class="sourceCode bash"><span class="kw">ncdump</span> -c CCSM4/macav2metdata_huss_CCSM4_r6i1p1_rcp45_2021_2025_CONUS_daily.nc</code></pre>
<p>Here's the output showing the first and last parts of each coordinate variable. (Everything else is ellipsized.)</p>
<pre><code>...
 lat = 25.0630779266357, 25.1047439575195, 25.1464099884033, 
...
    49.3543586730957, 49.3960227966309 ;

 lon = 235.227844238281, 235.269500732422, 235.311157226562, 
...
    292.851928710938, 292.893585205078, 292.935241699219 ;

 time = 44195, 44196, 44197, 44198, 44199, 44200, 44201, 44202, 44203, 44204, 
...
    46015, 46016, 46017, 46018, 46019, 46020 ;
...</code></pre>
<p>Note how the time values don't look like dates or times. This can be explained by looking back at the <code>time</code> definition from the header:</p>
<pre><code>    float time(time) ;
        time:units = &quot;days since 1900-01-01 00:00:00&quot; ;
        time:calendar = &quot;gregorian&quot; ;
        time:description = &quot;days since 1900-01-01&quot; ;</code></pre>
<p>Since this file is for 2021-2025, the lowest time value, 44195, should be for January 1, 2021. We can verify this by calculating the date 44195 days since 1900-01-01 with the GNU <code>date</code> command:</p>
<pre class="sourceCode bash"><code class="sourceCode bash"><span class="kw">date</span> --date=<span class="st">&quot;1900-01-01 +44195 days&quot;</span></code></pre>
<p>And we get the result:</p>
<pre><code>Fri Jan  1 00:00:00 CST 2021</code></pre>
<p>Also, we see something important about the longitude values. The metadata for the <code>lon</code> variable shows <code>lon:units = &quot;degrees_east&quot;</code>. Valid values could be either -180 to +180 (e.g. 90°W = -90) or 0 to 360 (e.g. 90°W = 270). Which valid range is being used is not specified in the header, but since we see values greater than 180, the files must be using the 0-360 range.</p>
<p>Note that the coordinate variables correspond to the dimensions of the file and that they appear in the definition of record variables:</p>
<pre><code>float specific_humidity(time, lat, lon) ;</code></pre>
<h3 id="viewing-variable-data">Viewing Variable Data</h3>
<p>We can view variable data by calling <code>ncdump</code> with the command option <code>-v variablename</code>. This is especially useful for (one-dimensional) coordinate variables, where we get a series of ordered values (usually in increasing order).</p>
<pre class="sourceCode bash"><code class="sourceCode bash"><span class="kw">ncdump</span> -v lat CCSM4/macav2metdata_huss_CCSM4_r6i1p1_rcp45_2021_2025_CONUS_daily.nc</code></pre>
<p>Here's the output showing the first and last parts of the variable. (Everything else is ellipsized.)</p>
<pre><code>...
 lat = 25.0630779266357, 25.1047439575195, 25.1464099884033,
...
    49.3543586730957, 49.3960227966309 ;
}</code></pre>
<p>We can also pipe the output to other commands to filter and transform it into something more useful.</p>
<p>Here we filter out the header and any remaining lines after the end of the latitude data:</p>
<pre class="sourceCode bash"><code class="sourceCode bash"><span class="kw">ncdump</span> -c CCSM4/macav2metdata_huss_CCSM4_r6i1p1_rcp45_2021_2025_CONUS_daily.nc <span class="kw">|</span> <span class="kw">sed</span> -n <span class="st">&#39;/^ lat =/,/;/p&#39;</span></code></pre>
<p>Here are the results (with most of the latitude data ellipsized):</p>
<pre><code> lat = 25.0630779266357, 25.1047439575195, 25.1464099884033, 
...
    49.3543586730957, 49.3960227966309 ;</code></pre>
<p>Explanation:</p>
<blockquote>
<p>The <code>-n</code> flag tells the <code>sed</code> command not to print lines by default. Then in the script that we give to the <code>sed</code> command, <code>/^ lat =/,/;/p</code>, we tell it to print lines starting from where it finds the pattern <code>^ lat =</code> (&quot; lat =&quot; at the beginning of a line) through the first line where it finds a semicolon. The syntax here is <code>address command</code> but without spaces and in this case the address is a range in the form of <code>start,end</code>. More info can be found under the topic <a href="https://www.gnu.org/software/sed/manual/sed.html#sed-addresses">Addresses</a> in the <code>sed</code> manual.</p>
</blockquote>
<p>Here we take the previous command a step further and format the data into one record per line:</p>
<pre class="sourceCode bash"><code class="sourceCode bash"><span class="kw">ncdump</span> -c CCSM4/macav2metdata_huss_CCSM4_r6i1p1_rcp45_2021_2025_CONUS_daily.nc <span class="kw">|</span> <span class="kw">sed</span> -n <span class="st">&#39;/^ lat =/,/\;/{s/ *lat = *//g;p}&#39;</span> <span class="kw">|</span> <span class="kw">tr</span> -d <span class="st">&#39;\n &#39;</span> <span class="kw">|</span> <span class="kw">tr</span> <span class="st">&#39;,;&#39;</span> <span class="st">&#39;\n\n&#39;</span></code></pre>
<p>And the output looks like this (with all the middle values ellipsized):</p>
<pre><code>25.0630779266357
25.1047439575195
...
49.3543586730957
49.3960227966309</code></pre>
<p>Explanation:</p>
<blockquote>
<p>Instead of just printing like in the previous command, we're telling <code>sed</code> to get rid of the &quot;lat =&quot; part and any surrounding spaces, then print. Then we're sending the output to the <code>tr</code> command to delete all line breaks and spaces. At this point the output would look like like a single line of numbers with commas between them and a semicolon at the end; this output is sent to another <code>tr</code> command to change the commas and semicolon into line breaks, giving us one number on each line.</p>
</blockquote>
<p>This can be generalized so that it's easy to use a different variable:</p>
<pre class="sourceCode bash"><code class="sourceCode bash"><span class="ot">v=</span><span class="st">&quot;lat&quot;</span>
<span class="kw">ncdump</span> -v <span class="ot">$v</span> somefile.nc <span class="kw">|</span> <span class="kw">sed</span> -n <span class="st">&#39;/^ &#39;&quot;</span><span class="ot">$v</span><span class="st">&quot;&#39; =/,/\;/{s/ *&#39;&quot;</span><span class="ot">$v</span><span class="st">&quot;&#39; = *//g;p}&#39;</span> <span class="kw">|</span> <span class="kw">tr</span> -d <span class="st">&#39;\n &#39;</span> <span class="kw">|</span> <span class="kw">tr</span> <span class="st">&#39;,;&#39;</span> <span class="st">&#39;\n\n&#39;</span></code></pre>
<p>And to view just the first and last values we could take a slightly different approach:</p>
<pre class="sourceCode bash"><code class="sourceCode bash"><span class="ot">v=</span><span class="st">&quot;lat&quot;</span>
<span class="kw">ncdump</span> -v <span class="ot">$v</span> somefile.nc <span class="kw">|</span> <span class="kw">sed</span> -n <span class="st">&#39;/^ &#39;&quot;</span><span class="ot">$v</span><span class="st">&quot;&#39; = /,/;/p&#39;</span> <span class="kw">|</span> <span class="kw">tr</span> -d <span class="st">&#39;\n&#39;</span> <span class="kw">|</span> <span class="kw">awk</span> <span class="st">&#39;{print $3,$(NF-1)}&#39;</span></code></pre>
<p>Explanation:</p>
<blockquote>
<p>Here we're going back to just using <code>sed</code> to only filter out the lines before and avter our variable, then we're using <code>tr</code> to delete all the line breaks, giving us everything on a single line. Then we're telling <code>awk</code> to print the third and next-to-last fields of its input.</p>
</blockquote>
<blockquote>
<p>Since <code>awk</code> splits the input into fields using whitespace by default, the first field, <code>$1</code>, will be <code>lat</code>, the second field will be <code>=</code>, and the third field will be the first actual value; since there's a space between the last value and the semicolon at the end, the last field, <code>$NF</code>, will be <code>;</code> and since <code>NF</code> is a variable representing a number we can subtract one to get the previous field.</p>
</blockquote>
<h2 id="subsetting">Subsetting</h2>
<p>To make some of the steps easier, we're going to make a list of all the netcdf files which we can use instead of the loops from earlier which didn't handle a few minor things.</p>
<p>To make the list, we go to the download directory (which contains a directory for each model) and run the following command in bash:</p>
<pre class="sourceCode bash"><code class="sourceCode bash"><span class="kw">find</span> ./ -mindepth 2 -name <span class="st">&#39;*.nc&#39;</span> <span class="kw">|</span> <span class="kw">sed</span> <span class="st">&#39;s/^\.\///g&#39;</span> <span class="kw">&gt;</span> file_list.txt</code></pre>
<p>Explanation:</p>
<blockquote>
<p>In the first part of this command, <code>find ./ -mindepth 2 -name '*.nc'</code>, we're specifying that we want to find files with names ending with <code>.nc</code> and that we want to look for them in subfolders of the current directory, <code>./</code>. (Files in the current directory, e.g <code>somefile.nc</code> are at depth 1.) The output from <code>find</code> looks like this:</p>
</blockquote>
<pre><code>./bcc-csm1-1-m/macav2metdata_tasmax_bcc-csm1-1-m_r1i1p1_rcp45_2021_2025_CONUS_daily.nc
./bcc-csm1-1-m/macav2metdata_tasmax_bcc-csm1-1-m_r1i1p1_rcp85_2021_2025_CONUS_daily.nc
./bcc-csm1-1-m/macav2metdata_tasmax_bcc-csm1-1-m_r1i1p1_rcp85_2026_2030_CONUS_daily.nc
...</code></pre>
<blockquote>
<p>Between the first and second parts of the command we have a pipe, <code>|</code>, to make the output from <code>find</code> be input for <code>sed</code>.</p>
</blockquote>
<blockquote>
<p>In the second part of the command, <code>sed 's/^\.\///g'</code>, we're removing <code>./</code> from the beginning of every line of output from the <code>find</code> command. The output from <code>sed</code> looks like this:</p>
</blockquote>
<pre><code>bcc-csm1-1-m/macav2metdata_tasmax_bcc-csm1-1-m_r1i1p1_rcp45_2021_2025_CONUS_daily.nc
bcc-csm1-1-m/macav2metdata_tasmax_bcc-csm1-1-m_r1i1p1_rcp85_2021_2025_CONUS_daily.nc
bcc-csm1-1-m/macav2metdata_tasmax_bcc-csm1-1-m_r1i1p1_rcp85_2026_2030_CONUS_daily.nc
...</code></pre>
<blockquote>
<p>Finally, with the <code>&gt; file_list.txt</code> part, we're redirecting the output from the <code>sed</code> command into a file instead of having it appear on screen. The output redirection we've done here, <code>&gt;</code> will cause anything already in <code>file_list.txt</code>, if it already exists, to be deleted. If we didn't want to overwrite existing data, we would use <code>&gt;&gt;</code> to <em>append</em> to an existing file. Both <code>&gt;</code> and <code>&gt;&gt;</code> will create the target file if it doesn't exist.</p>
</blockquote>
<p>We can create geographical subsets of netCDF files using the <a href="http://nco.sourceforge.net/">netCDF Operators</a> (NCO). NCO is available pre-compiled for Windows and Mac and available via the package manager in many Linux distributions; source code is also available if you prefer to compile everything yourself.</p>
<p>NCO has several different commands with a lot of common functionality among them. For our geographical subsets, we'll be using the <code>ncks</code> (netCDF Kitchen Sink) command.</p>
<p>The generic command form to greate a geographic subset of the MACAv2 netcdf files is like this:</p>
<pre><code>ncks -d lat,min_value,max_value -d lon,min_value,max_value infile.nc outfile.nc</code></pre>
<p>In this command, we're creating a <a href="http://nco.sourceforge.net/nco.html#Hyperslabs">hyperslab</a> (our geographical subset) of the data in <code>infile.nc</code> using the <em>dimensions</em> <code>lat</code> and <code>lon</code> and writing it to the output file <code>outfile.nc</code>. We know that <code>lat</code> and <code>lon</code> are the dimension names we want to use from having looked at the file headers with <code>ncdump</code>.</p>
<h3 id="cimarron">Cimarron</h3>
<p>To create the subsets for the Cimarron watershed, we first need to find the latitude and longitude ranges we need to cover.</p>
<p>We can do this in either QGIS or ArcGIS by opening the shapefile and looking at the extents.</p>
<p>Here are the directions for QGIS:</p>
<ol>
<li>Layer menu --&gt; Add Layer --&gt; Add Vector Layer<br /></li>
<li>Browse and select the appropriate shapefile (.shp)<br /></li>
<li>Layer menu --&gt; Properties<br /></li>
<li>On the Metadata tab, expand the Properties section and look for Extents</li>
</ol>
<p>This is what we see:</p>
<pre><code>xMin,yMin -100.119,35.3783 : xMax,yMax -95.9538,37.3697</code></pre>
<p>The bounds of the Cimarron watershed shape:</p>
<ul>
<li>N bound: 37.3697°<br /></li>
<li>S bound: 35.3783°<br /></li>
<li>E bound: -95.9538°<br /></li>
<li>W bound: -100.119°</li>
</ul>
<p>To find the latitude and longitude values we'll use to encompass the watershed shape, we dump the coordinate values from one of the files using the <code>ncdump</code> command:</p>
<pre class="sourceCode bash"><code class="sourceCode bash"><span class="kw">ncdump</span> -c CCSM4/macav2metdata_tasmax_CCSM4_r6i1p1_rcp85_2096_2099_CONUS_daily.nc</code></pre>
<p>From this command's output, we select the first value higher than the North and East bounds and the first value lower than the South and West bounds to use for our subset.</p>
<p>The longitude coordinates are in degrees east (just like our shapefile's extent coordinates), but unlike our shapefile, the netCDF file uses positive values (180 ... 360) for the Western hemisphere instead of negative values (-180 ... 0). So we'll need to add 360 to our longitude values to get the correct positive values.</p>
<ul>
<li>E bound: -95.9538° = 264.0462°<br /></li>
<li>W bound: -100.119° = 259.881°</li>
</ul>
<p>The coordinates we'll use for our Cimarron subset are:</p>
<ul>
<li>N bound: 37.3962135314941°<br /></li>
<li>S bound: 35.3545799255371°<br /></li>
<li>E bound: 264.060699462891°<br /></li>
<li>W bound: 259.852447509766°</li>
</ul>
<p>So the command to create a subset covering the Cimarron watershed (in netCDF 3 classic format) will look like this:</p>
<pre class="sourceCode bash"><code class="sourceCode bash"><span class="kw">ncks</span> -3 -d lat,35.3545799255371,37.3962135314941 -d lon,259.852447509766,264.060699462891 infile.nc outfile.nc</code></pre>
<p>After creating a subset using these latitude and longitude coordinates, we noticed that the end points were not necessarily included like they should be. (This actually came to our attention with Kiamichi, where the lack of the easternmost column of raster cells caused the shapefile to be not completely covered. We then confirmed the same issue with the Cimarron subsets. Our best guess is that this has something to do with floating point error.) The files created with the above command only included latitudes 35.3545799255371 - <em>37.354549407959</em> and longitudes <em>259.894104003906</em> - 264.060699462891.</p>
<p>Note, for the North boundary, 37.354549407959 actually works because the coordinates are for the center of the grid cell and 37.369727 is <em>less</em> than halfway between 37.354549407959 and 37.3962135314941.</p>
<p>Similarly, for the West boundary, 259.894104003906 works because 259.881479 is <em>more</em> than halfway between 259.852447509766 and 259.894104003906.</p>
<p>So it was not necessary to revise this command.</p>
<p>However, we <em>should</em> revise our criteria for selecting coordinates from the netCDF file. The file header specifies that latitude and longitude coordinates are for the center of the grid cells; this means that the grid cell closest to a point &quot;covers&quot; that point. So we'll want to select the values <em>closest</em> to our shapefile's bounds to use for our subset.</p>
<p>Here is the Cimarron shape shown over the subset area. Note that the Southernmost row of cells is completely outside the shapefile's extent.</p>
<p><img src="MACAv2_Derived/Cimarron.png" alt="MACAv2_Derived/Cimarron.png" /></p>
<p>Now that we've confirmed that our shapefile is completely within the subset area, we're ready to make subsets of all the files.</p>
<p>Create a new directory outside of the download directory called &quot;MACAv2_Derived&quot;.</p>
<pre class="sourceCode bash"><code class="sourceCode bash"><span class="kw">mkdir</span> ../MACAv2_Derived</code></pre>
<p>Create parent directories for all the Cimarron subset files:</p>
<pre class="sourceCode bash"><code class="sourceCode bash"><span class="kw">mkdir</span> -p ../MACAv2_Derived/Cimarron/<span class="dt">{bcc-csm1-1-m,bcc-csm1-1,BNU-ESM,CanESM2,CCSM4,CNRM-CM5,CSIRO-Mk3-6-0,GFDL-ESM2G,GFDL-ESM2M,HadGEM2-CC365,HadGEM2-ES365,inmcm4,IPSL-CM5A-LR,IPSL-CM5A-MR,IPSL-CM5B-LR,MIROC-ESM-CHEM,MIROC-ESM,MIROC5,MRI-CGCM3,NorESM1-M}</span></code></pre>
<p>Explanation:</p>
<blockquote>
<p>The syntax used in the <code>mkdir</code> command is explained in the <code>bash</code> manual under the topic <a href="https://www.gnu.org/software/bash/manual/html_node/Brace-Expansion.html">Brace Expansion</a>. To explain with an example: the command <code>mkdir bl{a,e,i,o,u}h</code> would be expanded to <code>mkdir blah bleh blih bloh bluh</code>. Bash itself expands the expression with the curly braces before the <code>mkdir</code> command does anything.</p>
</blockquote>
<p>Create the Cimarron subset files:</p>
<pre class="sourceCode bash"><code class="sourceCode bash"><span class="kw">for</span> <span class="kw">f</span> in <span class="ot">$(</span><span class="kw">cat</span> file_list.txt<span class="ot">)</span><span class="kw">;</span> <span class="kw">do</span> 
    <span class="kw">ncks</span> -3 -d lat,35.3545799255371,37.3962135314941 -d lon,259.852447509766,264.060699462891 <span class="ot">${f}</span> ../MACAv2_Derived/Cimarron/<span class="ot">${f/</span>CONUS<span class="ot">/</span>Cimarron<span class="ot">}</span><span class="kw">;</span> 
<span class="kw">done</span></code></pre>
<p>Explanation:</p>
<blockquote>
<p>Each line of the file <code>file_list</code> identifies one of the netCDF files using a relative path in the form of <em>model</em>/<em>filename</em>. Here, we're using a <a href="https://www.gnu.org/software/bash/manual/html_node/Looping-Constructs.html"><code>for</code> loop</a> to repeat our <code>ncks</code> subset command for every netCDF file in the list. (Note that this wouldn't work correctly if any of the filenames or model directories had spaces.)</p>
</blockquote>
<blockquote>
<p><code>$(cat file_list.txt)</code> This syntax is explained in the <code>bash</code> manual under the topic <a href="https://www.gnu.org/software/bash/manual/html_node/Command-Substitution.html">Command Substitution</a>.</p>
</blockquote>
<blockquote>
<p><code>${f/CONUS/Cimarron}</code> is the variable <code>${f}</code> with the substring &quot;CONUS&quot; replaced with &quot;Cimarron&quot;. This syntax (and other similar functionality) is defined in the <code>bash</code> manual under the topic <a href="https://www.gnu.org/software/bash/manual/html_node/Shell-Parameter-Expansion.html">Shell Parameter Expansion</a>.</p>
</blockquote>
<p>Creating the Cimarron subset took about 7.5 hours (with the Kiamichi subset running concurrently for about 3.33 hours, and the Kiamichi data conversion running concurrently for about 1.75 hours). The size of the Cimarron subset is about 188 GB.</p>
<h3 id="kiamichi">Kiamichi</h3>
<p>To create the subsets for the Kiamichi watershed, we first need to find the latitude and longitude ranges we need to cover.</p>
<p>We do this in QGIS by opening the shapefile and looking at the extents. Here are the steps:</p>
<ol>
<li>Layer menu --&gt; Add Layer --&gt; Add Vector Layer<br /></li>
<li>Browse and select the appropriate shapefile (.shp)<br /></li>
<li>Layer menu --&gt; Properties<br /></li>
<li>On the Metadata tab, expand the Properties section and look for Extents</li>
</ol>
<p>This is what we see:</p>
<pre><code>xMin,yMin -95.8222,33.9145 : xMax,yMax -94.456,34.8217</code></pre>
<p>The bounds of the Kiamichi watershed shape:</p>
<ul>
<li>N bound: 34.8217°<br /></li>
<li>S bound: 33.9145°<br /></li>
<li>E bound: -94.456° = 265.544°<br /></li>
<li>W bound: -95.8222° = 264.1778°</li>
</ul>
<p>The longitude coordinates in the netCDF files have positive values (180 ... 360) for the Western hemisphere, so we added 360 to our longitude value to match the files' scheme.</p>
<p>We find the latitude and longitude values which will encompass the watershed shape by using the <code>ncdump</code> command on one of the files:</p>
<pre class="sourceCode bash"><code class="sourceCode bash"><span class="kw">ncdump</span> -c CCSM4/macav2metdata_tasmax_CCSM4_r6i1p1_rcp85_2096_2099_CONUS_daily.nc</code></pre>
<p>We select our boundary coordinates by finding the next Northern and Eastern values higher than the shapefile bounds and the next Southern and Western values lower than the shapefile bounds.</p>
<p>For Kiamichi these are:</p>
<ul>
<li>N bound: 34.8545875549316<br /></li>
<li>S bound: 33.8962707519531<br /></li>
<li>E bound: 265.560668945312<br /></li>
<li>W bound: 264.14404296875</li>
</ul>
<p>So the command to create a subset covering the Kiamichi watershed (in netCDF 3 classic format) will look like this:</p>
<pre class="sourceCode bash"><code class="sourceCode bash"><span class="kw">ncks</span> -3 -d lat,33.8962707519531,34.8545875549316 -d lon,264.14404296875,265.560668945312 infile.nc outfile.nc</code></pre>
<p>After creating a subset using these latitude and longitude coordinates, we noticed that the end points were not necessarily included like they should be. (Our best guess is that this has something to do with floating point error.) So the files created with the above command only included latitudes 33.8962707519531 - <em>34.8129234313965</em> and longitudes 264.14404296875 - <em>265.519012451172</em>.</p>
<p>For the North boundary, 34.8129234313965 works because the coordinates are for the center of the grid cell and 34.821695 is <em>less</em> than halfway between 34.8129234313965 and 34.8545875549316.</p>
<p>However, the easternmost point, 265.543970, is <em>more</em> than halfway between 265.519012451172 and 265.560668945312, meaning it is outside the area covered by the raster. Therefore we need to revise our subset command.</p>
<p>The next longitude coordinate is 265.602355957031. So the <em>revised</em> command to create a subset covering the Kiamichi watershed (in netCDF 3 classic format) will look like this:</p>
<pre class="sourceCode bash"><code class="sourceCode bash"><span class="kw">ncks</span> -3 -d lat,33.8962707519531,34.8545875549316 -d lon,264.14404296875,265.602355957031 infile.nc outfile.nc</code></pre>
<p>As noted above in the Cimarron section (we were working on both watershed areas more-or-less simultaneously) we need to revise our criteria for selecting boundary points from the netCDF files. We want to use the values <em>closest</em> to our shapefile's bounds to use for our subset.</p>
<p>Here is the Kiamichi shape shown over the subset area. Note that that the Westernmost column of cells is completely outside the shapefile's extent.</p>
<p><img src="MACAv2_Derived/Kiamichi.png" alt="MACAv2_Derived/Kiamichi.png" /></p>
<p>In the &quot;MACAv2_Derived&quot; directory, create parent directories for all the Kiamichi subset files:</p>
<pre class="sourceCode bash"><code class="sourceCode bash"><span class="kw">mkdir</span> -p ../MACAv2_Derived/Kiamichi/<span class="dt">{bcc-csm1-1-m,bcc-csm1-1,BNU-ESM,CanESM2,CCSM4,CNRM-CM5,CSIRO-Mk3-6-0,GFDL-ESM2G,GFDL-ESM2M,HadGEM2-CC365,HadGEM2-ES365,inmcm4,IPSL-CM5A-LR,IPSL-CM5A-MR,IPSL-CM5B-LR,MIROC-ESM-CHEM,MIROC-ESM,MIROC5,MRI-CGCM3,NorESM1-M}</span></code></pre>
<p>Explanation:</p>
<blockquote>
<p>See the topic <a href="https://www.gnu.org/software/bash/manual/html_node/Brace-Expansion.html">Brace Expansion</a> in the <code>bash</code> manual for an explanation of the syntax used here.</p>
</blockquote>
<p>Create the Kiamichi subset files:</p>
<pre class="sourceCode bash"><code class="sourceCode bash"><span class="kw">for</span> <span class="kw">f</span> in <span class="ot">$(</span><span class="kw">cat</span> file_list.txt<span class="ot">)</span><span class="kw">;</span> <span class="kw">do</span> 
    <span class="kw">ncks</span> -3 -d lat,33.8962707519531,34.8545875549316 -d lon,264.14404296875,265.602355957031 <span class="ot">${f}</span> ../MACAv2_Derived/Kiamichi/<span class="ot">${f/</span>CONUS<span class="ot">/</span>Kiamichi<span class="ot">}</span><span class="kw">;</span> 
<span class="kw">done</span></code></pre>
<p>Explanation:</p>
<blockquote>
<p>Here we're using a <code>for</code> loop to run our <code>ncks</code> subset command on all the files. See the topics <a href="https://www.gnu.org/software/bash/manual/html_node/Looping-Constructs.html">Looping Constructs</a>, <a href="https://www.gnu.org/software/bash/manual/html_node/Command-Substitution.html">Command Substitution</a>, and <a href="https://www.gnu.org/software/bash/manual/html_node/Shell-Parameter-Expansion.html">Shell Parameter Expansion</a> in the <code>bash</code> manual for explanation of all the syntax used here.</p>
</blockquote>
<p>Creating the Kiamichi subset took about 3.33 hours (running concurrently with the Cimarron subset). The size of the Kiamichi subset is about 31 GB.</p>
<h2 id="converting-for-envision-models">Converting for Envision Models</h2>
<p>For our Envision models, these are the variables that we'll need:</p>
<table>
<thead>
<tr class="header">
<th align="left">Variable</th>
<th align="left">Units</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">maximum temperature</td>
<td align="left">°C</td>
</tr>
<tr class="even">
<td align="left">minimum temperature</td>
<td align="left">°C</td>
</tr>
<tr class="odd">
<td align="left">mean temperature</td>
<td align="left">°C</td>
</tr>
<tr class="even">
<td align="left">solar radiation</td>
<td align="left">W m<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msup><mi></mi><mrow><mo>−</mo><mn>2</mn></mrow></msup></mrow></math></td>
</tr>
<tr class="odd">
<td align="left">specific humidity</td>
<td align="left">kg kg<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msup><mi></mi><mrow><mo>−</mo><mn>1</mn></mrow></msup></mrow></math></td>
</tr>
<tr class="even">
<td align="left">wind speed</td>
<td align="left">m s<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msup><mi></mi><mrow><mo>−</mo><mn>1</mn></mrow></msup></mrow></math></td>
</tr>
<tr class="odd">
<td align="left">precipitation</td>
<td align="left">mm day<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msup><mi></mi><mrow><mo>−</mo><mn>1</mn></mrow></msup></mrow></math></td>
</tr>
</tbody>
</table>
<p>The MACAv2-METDATA dataset has all of these except for mean temperature and wind speed, but we can derive them from the provided variables.</p>
<p>From examining the header data for one of each variable's netCDF files, we can see that we'll need to do some unit conversion on the temperature variables; other than those, everything is already in the units we want. (The wind <em>vectors</em> which we'll use to derive wind speed are also in our desired units.)</p>
<p>Before we started, we knew that there was some sort of compatibility issue between the MACAv2 netCDF files and Envision. The MACA data portal mentions that the <a href="https://climate.northwestknowledge.net/MACA/data_portal.php#issuesNetCDFTab">MACAv2 files are in netCDF4 format</a>, so we assumed (correctly!) that gdal library Envision uses required netCDF 3 (classic) format. Conveniently all the netCDF operators seem to have the same options for setting output format, so all we have to do to convert everything to netCDF 3 is add the <code>-3</code> flag to one of our processing commands.</p>
<p>After a bit of testing we determined that Envision couldn't handle MACAv2's longitude coordinates &gt; 180° for the Western hemisphere, so every variable we want to use with Envision needs at least the longitude converted.</p>
<p>So here are the conversions we need to perform:</p>
<ol>
<li>Change longitude &gt; 180° to negative values (i.e. subtract 360)<br /></li>
<li>Change <em>tasmax</em> units from °K to °C (i.e. subtract 273.15)<br /></li>
<li>Change <em>tasmin</em> units from °K to °C<br /></li>
<li>Create variable for mean temperature (average of tasmax and tasmin in °C)<br /></li>
<li>Create variable for wind speed based on Northward and Eastward wind vectors (<em>uas</em> and <em>vas</em>)</li>
</ol>
<p>In addition to unit and coordinate conversions and the new variables we need to derive, Envision also wants <em>yearly</em> files.</p>
<h3 id="general-procedures">General Procedures</h3>
<p>Here are generic forms of all our conversion procedures. Where any of our changes make the existing metadata invalid we're also updating the metadata to be correct.</p>
<p><strong>Longitude conversion</strong></p>
<p>Convert positive longitude values &gt; 180° to negative values. E.g. 265°E (95°W) = -95. All we have to do here is subtract 360 from all the longitude coordinates since all our coordinates are in the Western hemisphere.</p>
<p>Use <a href="http://nco.sourceforge.net/nco.html#ncap2"><code>ncap2</code></a>, the netCDF Arithmetic Processor, for this step.</p>
<pre><code>ncap -s &#39;lon=lon-360&#39; infile.nc outfile.nc</code></pre>
<p><strong>Temperature conversion</strong></p>
<p>Convert Kelvins to degrees Celsius. We can use <code>ncap2</code> for the actual data conversion since the conversion from Kelvins to Celsius is simply arithmetic (subtracting 273.15) just like we used in converting the longitude coordinates, but then we'll want to update the metadata so that we won't be misled if we look at things later.</p>
<p>Before updating the metadata, this is what we see for the <code>air_temperature</code> variable:</p>
<pre><code>    float air_temperature(time, lat, lon) ;
        air_temperature:_FillValue = -9999.f ;
        air_temperature:long_name = &quot;Daily Maximum Near-Surface Air Temperature&quot; ;
        air_temperature:units = &quot;K&quot; ;
        air_temperature:grid_mapping = &quot;crs&quot; ;
        air_temperature:standard_name = &quot;air_temperature&quot; ;
        air_temperature:height = &quot;2 m&quot; ;
        air_temperature:cell_methods = &quot;time: maximum(interval: 24 hours)&quot; ;
        air_temperature:coordinates = &quot;time lon lat&quot; ;</code></pre>
<p>Since we're converting to Celsius, we want to change the <code>units</code> attribute to <code>C</code> instead of <code>K</code>.</p>
<p>We can do this with <a href="http://nco.sourceforge.net/nco.html#ncatted"><code>ncatted</code></a>, the netCDF Attribute Editor. In NCO terminology, we want to overwrite (<code>o</code>) the <code>units</code> attribute (<code>-a</code>) of the <code>air_temperature</code> variable; the datatype for the <code>units</code> attribute is <code>char</code> (<code>c</code>) and the value we want to give to it is <code>'C'</code>.</p>
<p>Unlike most of the NCO commands, <code>ncatted</code> can usually edit a file in place and will do this if only given one filename.</p>
<p>So our <code>ncatted</code> command looks like this:</p>
<pre><code>ncatted -a units,air_temperature,o,c,&#39;C&#39; testfile.nc</code></pre>
<p>After running this command we look at the <code>air_temperature</code> variable again and see that our command was successful (and that the attributes are now arranged in alphabetical order).</p>
<pre><code>    float air_temperature(time, lat, lon) ;
        air_temperature:_FillValue = -9999.f ;
        air_temperature:cell_methods = &quot;time: maximum(interval: 24 hours)&quot; ;
        air_temperature:coordinates = &quot;time lon lat&quot; ;
        air_temperature:grid_mapping = &quot;crs&quot; ;
        air_temperature:height = &quot;2 m&quot; ;
        air_temperature:long_name = &quot;Daily Maximum Near-Surface Air Temperature&quot; ;
        air_temperature:standard_name = &quot;air_temperature&quot; ;
        air_temperature:units = &quot;C&quot; ;</code></pre>
<p>So here are our generic commands for temperature conversion:</p>
<pre><code>ncap2 -s &#39;air_temperature=(air_temperature - 273.15)&#39; infile.nc outfile.nc
ncatted -a units,air_temperature,o,c,&#39;C&#39; infile.nc [outfile.nc]</code></pre>
<p><strong>Producing mean temperature netCDF</strong></p>
<p>Calculate the average of the maximum and minimum temperature. Because both tasmin and tasmax files use the same variable name, <code>air_temperature</code>, we can do this easily with the <a href="http://nco.sourceforge.net/nco.html#nces"><code>nces</code></a> (netCDF Ensemble Statistics) command.</p>
<p>The new mean temperature file will inherit the variable attributes from the first component file. As we can see from the <code>air_temperature</code> variable's definition (see above), we'll want to change the attributes <code>cell_methods</code> and <code>long_name</code>.</p>
<p>We can update two attributes at once with a single <code>ncatted</code> command by adding an additional <code>-a</code> option followed by the attribute info.</p>
<p>So here are our generic commands for producing a mean temperature netCDF:</p>
<pre><code>nces tasmax.nc tasmin.nc tasmean.nc
ncatted -a cell_methods,air_temperature,o,c,&#39;time: mean(interval: 24 hours)&#39; -a long_name,air_temperature,o,c,&#39;Daily Mean Near-Surface Air Temperature&#39; tasmean.nc</code></pre>
<p><strong>Producing wind speed netCDF</strong></p>
<p>The wind information provided in MACAv2-METDATA is split into two vectors: <code>northward_wind</code> in the <code>vas</code> files and <code>eastward_wind</code> in the <code>uas</code> files. We can use the familiar <a href="https://en.wikipedia.org/wiki/Pythagorean_theorem">Pythagorean theorem</a>, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>c</mi><mo>=</mo><msqrt><mrow><msup><mi>a</mi><mn>2</mn></msup><mo>+</mo><msup><mi>b</mi><mn>2</mn></msup></mrow></msqrt></mrow></math>, to calculate wind speed, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>c</mi></mrow></math>, by assigning the northward and eastward wind vectors to <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>a</mi></mrow></math> and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>b</mi></mrow></math>.</p>
<p>The procedure we followed to do this involved several steps and multiple NCO commands. We used the <code>ncks</code> command to append and extract variables, the <code>ncap2</code> command for the actual math, and the <code>ncatted</code> command to update metadata.</p>
<p>Step 1: Copy one of the files because we'll end up modifying it directly</p>
<pre><code>cp vas.nc temporary_vas.nc</code></pre>
<p>Step 2: Get the northward_wind and eastward_wind into the same netcdf file</p>
<pre><code>ncks -A uas.nc temporary_vas.nc    # Adds uas variable to temporary_vas file</code></pre>
<p>Step 3: Calculate the wind speed</p>
<pre><code>ncap2 -s &#39;wind_speed=sqrt(northward_wind^2 + eastward_wind^2)&#39; temporary_vas.nc temporary_wind.nc</code></pre>
<p>Step 4: Put the wind speed into its own file</p>
<pre><code>ncks -v wind_speed temporary_wind.nc wind_speed.nc</code></pre>
<p>Step 5: Update metadata</p>
<pre><code>ncatted -a comments,wind_speed,o,c,&#39;Surface (10m) wind speed&#39; -a long_name,wind_speed,o,c,&#39;Wind Speed&#39; -a standard_name,wind_speed,o,c,&#39;wind_speed&#39; wind_speed.nc</code></pre>
<p>Step 6: Delete temporary files</p>
<pre><code>rm temporary_vas.nc temporary_wind.nc</code></pre>
<p><strong>Producing a time subset</strong></p>
<p>This works just like creating a geographical subset except that we're just subsetting the <code>time</code> dimension rather than <code>lat</code> and <code>lon</code>.</p>
<pre><code>ncks -d time,min_value,max_value infile.nc outfile.nc</code></pre>
<p>Here are some literal commands we used to test the data conversion:</p>
<pre class="sourceCode bash"><code class="sourceCode bash"><span class="kw">ncap2</span> -3 -s <span class="st">&#39;air_temperature=(air_temperature - 273.15)&#39;</span> macav2metdata_tasmax_CCSM4_r6i1p1_rcp85_2021_2025_CONUS_daily.nc macav2metdata_tasmaxC_CCSM4_r6i1p1_rcp85_2021_2025_Kiamichi_daily.nc
<span class="kw">ncap2</span> -3 -s <span class="st">&#39;air_temperature=(air_temperature - 273.15)&#39;</span> macav2metdata_tasmin_CCSM4_r6i1p1_rcp85_2021_2025_CONUS_daily.nc macav2metdata_tasminC_CCSM4_r6i1p1_rcp85_2021_2025_Kiamichi_daily.nc
<span class="kw">ncatted</span> -a units,air_temperature,o,char,<span class="st">&#39;C&#39;</span> macav2metdata_tasmaxC_CCSM4_r6i1p1_rcp85_2021_2025_Kiamichi_daily.nc 
<span class="kw">ncatted</span> -a units,air_temperature,o,char,<span class="st">&#39;C&#39;</span> macav2metdata_tasminC_CCSM4_r6i1p1_rcp85_2021_2025_Kiamichi_daily.nc

<span class="kw">ncbo</span> -3 --op_typ=add macav2metdata_tasmaxC_CCSM4_r6i1p1_rcp85_2021_2025_Kiamichi_daily.nc macav2metdata_tasminC_CCSM4_r6i1p1_rcp85_2021_2025_Kiamichi_daily.nc temp_added.nc
<span class="kw">ncap2</span> -3 -s <span class="st">&#39;air_temperature=(air_temperature/2.0)&#39;</span> temp_added.nc macav2metdata_tasmeanC_CCSM4_r6i1p1_rcp85_2021_2025_Kiamichi_daily.nc
<span class="kw">rm</span> temp_added.nc
<span class="kw">ncatted</span> -a cell_methods,air_temperature,o,c,<span class="st">&#39;time: mean(interval: 24 hours)&#39;</span> macav2metdata_tasmeanC_CCSM4_r6i1p1_rcp85_2021_2025_Kiamichi_daily.nc

<span class="kw">cp</span> macav2metdata_vas_CCSM4_r6i1p1_rcp85_2021_2025_CONUS_daily.nc temp_vas.nc
<span class="kw">ncks</span> -A macav2metdata_uas_CCSM4_r6i1p1_rcp85_2021_2025_CONUS_daily.nc temp_vas.nc
<span class="kw">ncap2</span> -s <span class="st">&#39;wind_speed=sqrt(northward_wind^2 + eastward_wind^2)&#39;</span> temp_vas.nc temp_wind.nc
<span class="kw">ncks</span> -3 -v wind_speed temp_wind.nc macav2metdata_wind_CCSM4_r6i1p1_rcp85_2021_2025_Kiamichi_daily.nc
<span class="kw">rm</span> temp_vas.nc temp_wind.nc
<span class="kw">ncatted</span> -a comments,wind_speed,o,c,<span class="st">&#39;Surface (10m) wind speed&#39;</span> -a long_name,wind_speed,o,c,<span class="st">&#39;Wind Speed&#39;</span> -a standard_name,wind_speed,o,c,<span class="st">&#39;wind_speed&#39;</span> macav2metdata_wind_CCSM4_r6i1p1_rcp85_2021_2025_Kiamichi_daily.nc

<span class="kw">nccopy</span> -3 macav2metdata_huss_CCSM4_r6i1p1_rcp85_2021_2025_CONUS_daily.nc macav2metdata_huss_CCSM4_r6i1p1_rcp85_2021_2025_Kiamichi_daily.nc
<span class="kw">nccopy</span> -3 macav2metdata_rsds_CCSM4_r6i1p1_rcp85_2021_2025_CONUS_daily.nc macav2metdata_rsds_CCSM4_r6i1p1_rcp85_2021_2025_Kiamichi_daily.nc
<span class="kw">nccopy</span> -3 macav2metdata_pr_CCSM4_r6i1p1_rcp85_2021_2025_CONUS_daily.nc macav2metdata_pr_CCSM4_r6i1p1_rcp85_2021_2025_Kiamichi_daily.nc</code></pre>
<h3 id="calculating-year-boundaries">Calculating Year Boundaries</h3>
<p>To produce yearly files for Envision, we'll need to be able to extract each year from its year block (e.g. 2021_2025). As we noted earlier when examining the file headers, the time coordinates we have to deal with are expressed in terms of &quot;days since 1900-01-01&quot;.</p>
<p>The file headers tell us that February 29th is included on leap years, so we can't just use multiples of 365 to figure the year boundaries. The GNU <code>date</code> command includes some very nice time calculation functions and is great at adding or subtracting timespans from a date, but it doesn't help so much for giving us a timespan between two dates. Even so, it's still helpful here; we can calculate our time points in <a href="https://en.wikipedia.org/wiki/Unix_time">Unix time</a> (seconds since 1970-01-01 00:00:00 UTC) and then use simple arithmetic from there.</p>
<p>We want to select our yearly subset based on time values (rather than indices) so we want to express these time values as floating point values rather than integers. (See example 10.ii on <a href="http://research.jisao.washington.edu/data_sets/nco/">http://research.jisao.washington.edu/data_sets/nco/</a>.)</p>
<p>Here is our procedure for calculating a given date (January 1, 2021 in this example) in terms of &quot;days since 1900-01-01&quot; that are used in the MACAv2 netCDF files:</p>
<pre class="sourceCode bash"><code class="sourceCode bash"><span class="co"># Calculate reference date in unix time (will be a negative number since it&#39;s before 1970)</span>
<span class="ot">ref_date=$(</span><span class="kw">date</span> --date=<span class="st">&#39;1900-01-01&#39;</span> +%s<span class="ot">)</span>

<span class="co"># Calculate target date in unix time</span>
<span class="ot">target_date=$(</span><span class="kw">date</span> --date=<span class="st">&#39;2021-01-01&#39;</span> +%s<span class="ot">)</span>

<span class="co"># Calculate the number of seconds between the target date and reference date</span>
<span class="ot">seconds_difference=$(</span><span class="kw">echo</span> <span class="st">&quot;</span><span class="ot">${target_date}</span><span class="st"> - </span><span class="ot">${ref_date}</span><span class="st">&quot;</span> <span class="kw">|</span> <span class="kw">bc</span><span class="ot">)</span>

<span class="co"># Divide by number of seconds in a day to get number of days</span>
<span class="ot">days_difference=$(</span><span class="kw">echo</span> <span class="st">&quot;</span><span class="ot">${seconds_difference}</span><span class="st"> / 86400&quot;</span> <span class="kw">|</span> <span class="kw">bc</span><span class="ot">)</span>

<span class="co"># Print number of days (which should be an integer) with an extra &quot;.0&quot; at the end</span>
<span class="kw">echo</span> <span class="st">&quot;</span><span class="ot">${days_difference}</span><span class="st">.0&quot;</span></code></pre>
<p>Explanation:</p>
<blockquote>
<p>At each step before the last one, we're saving the output of each command into a variable by enclosing the command with <code>$( )</code>. See the topic <a href="https://www.gnu.org/software/bash/manual/html_node/Command-Substitution.html">Command Substitution</a> in the <code>bash</code> manual for more info.</p>
</blockquote>
<blockquote>
<p>In each of our date commands we're getting the date described by a string in Unix time (described by the fromatter <code>%s</code>). See the topics <a href="https://www.gnu.org/software/coreutils/manual/html_node/Date-input-formats.html">Date Input Formats</a> and <a href="https://www.gnu.org/software/coreutils/manual/html_node/Time-conversion-specifiers.html">Time Conversion Specifiers</a> in the <code>date</code> manual.</p>
</blockquote>
<blockquote>
<p>For our arithmetic steps, we're creating a string expressing the calculations we need (double-quoted instead of single-quoted so that our variables will be expanded) and piping them to the <code>bc</code> command which you can think of for our purposes as a command line calculator. This could also be done with a <a href="https://www.gnu.org/software/bash/manual/html_node/Redirections.html#Here-Strings">here string</a> instead of <code>echo</code> and a pipe.</p>
</blockquote>
<p>By running each of these steps in <code>bash</code>, we should see <code>44195.0</code> printed by the final <code>echo</code> command.</p>
<p>We can combine a couple of the steps (demonstrated separately above for clarity) into one:</p>
<pre class="sourceCode bash"><code class="sourceCode bash"><span class="ot">days_difference=$(</span><span class="kw">echo</span> <span class="st">&quot;(</span><span class="ot">${target_date}</span><span class="st"> - </span><span class="ot">${ref_date}</span><span class="st">) / 86400&quot;</span> <span class="kw">|</span> <span class="kw">bc</span><span class="ot">)</span></code></pre>
<p>To make this procedure useful, we write a script which will accept the target year as a command line argument.</p>
<p><strong><code>yearbounds.sh</code></strong></p>
<pre class="sourceCode bash"><code class="sourceCode bash"><span class="co">#!/bin/bash</span>
<span class="co">#</span>
<span class="co"># Print the first (Jan 1) and last (Dec 31) days of a given year as the</span>
<span class="co"># number of days since a reference date.</span>
<span class="co">#</span>
<span class="co"># The output will be two floating point numbers rounded to one decimal point</span>
<span class="co"># and separated with a comma. (This is intended to be used in a command</span>
<span class="co"># to create a time subset of a netCDF file.)</span>
<span class="co">#</span>
<span class="co"># This script requires the target year to be passed in as a command</span>
<span class="co"># line argument.</span>
<span class="co">#</span>

<span class="co"># the target year</span>
<span class="ot">y=</span><span class="st">&quot;</span><span class="ot">$1</span><span class="st">&quot;</span>

<span class="co"># Calculate the reference time in Unix time</span>
<span class="ot">reftime=$(</span><span class="kw">date</span> --date=<span class="st">&#39;1900-01-01&#39;</span> +%s<span class="ot">)</span>

<span class="co"># Calculate January 1st and December 31st of the given year in Unix time</span>
<span class="ot">s1=$(</span><span class="kw">date</span> --date=<span class="st">&quot;</span><span class="ot">${y}</span><span class="st">-01-01&quot;</span> +%s<span class="ot">)</span>
<span class="ot">s2=$(</span><span class="kw">date</span> --date=<span class="st">&quot;</span><span class="ot">${y}</span><span class="st">-12-31&quot;</span> +%s<span class="ot">)</span>

<span class="co"># Calculate difference from reference time and convert from seconds to days</span>
<span class="ot">d1=$(</span><span class="kw">echo</span> <span class="st">&quot;(</span><span class="ot">${s1}</span><span class="st"> - </span><span class="ot">${reftime}</span><span class="st">)/86400&quot;</span> <span class="kw">|</span> <span class="kw">bc</span><span class="ot">)</span> 
<span class="ot">d2=$(</span><span class="kw">echo</span> <span class="st">&quot;(</span><span class="ot">${s2}</span><span class="st"> - </span><span class="ot">${reftime}</span><span class="st">)/86400&quot;</span> <span class="kw">|</span> <span class="kw">bc</span><span class="ot">)</span>

<span class="co"># Print the numbers in floating point format</span>
<span class="kw">printf</span> <span class="st">&quot;%.1f,%.1f\n&quot;</span> <span class="ot">${d1}</span> <span class="ot">${d2}</span></code></pre>
<p>Here is an example:</p>
<pre class="sourceCode bash"><code class="sourceCode bash"><span class="kw">bash</span> yearbounds.sh 2099</code></pre>
<p>And the results: <code>72684.0,73048.0</code></p>
<p>Here is a literal command we used to produce yearly files in testing:</p>
<pre class="sourceCode bash"><code class="sourceCode bash"><span class="kw">for</span> <span class="kw">v</span> in huss pr rsds tasmaxC tasminC tasmeanC wind<span class="kw">;</span> <span class="kw">do</span> <span class="kw">for</span> <span class="kw">y</span> in <span class="dt">{2021..2025}</span><span class="kw">;</span> <span class="kw">do</span> <span class="ot">infile=</span><span class="st">&quot;macav2metdata_</span><span class="ot">${v}</span><span class="st">_CCSM4_r6i1p1_rcp85_2021_2025_Kiamichi_daily.nc&quot;</span>; <span class="ot">outfile=</span><span class="st">&quot;yearly/macav2metdata_</span><span class="ot">${v}</span><span class="st">_CCSM4_r6i1p1_rcp85_Kiamichi_daily_</span><span class="ot">${y}</span><span class="st">.nc&quot;</span>; <span class="ot">timecoords=$(</span><span class="kw">bash</span> yearbounds.sh <span class="ot">${y})</span>; <span class="kw">ncks</span> -d time,<span class="ot">${timecoords}</span> <span class="ot">${infile}</span> <span class="ot">${outfile}</span><span class="kw">;</span> <span class="kw">done</span>; <span class="kw">done</span></code></pre>
<p>Now that we have all our pieces (generic commands to manipulate netCDF files and a script to get year boundaries) we can create a script to convert data for Envision.</p>
<p><strong><code>maca_subset_to_envision.sh</code></strong></p>
<pre class="sourceCode bash"><code class="sourceCode bash"><span class="co">#!/bin/bash</span>
<span class="co">#</span>
<span class="co"># Convert watershed-specific subsets from MACAv2-METDATA into a format usable</span>
<span class="co"># in Envision. Envision-compatible netCDF files need to be in netCDF 3</span>
<span class="co"># (classic) format and require the valid range for longitude in degrees east</span>
<span class="co"># to be [ -180 &lt;= x &lt;= 180 ] rather than [ 0 &lt;= x &lt;= 360 ].</span>
<span class="co">#</span>
<span class="co"># Additionally we will need to change some units and derive new variables.</span>
<span class="co">#</span>
<span class="co"># Convert temperature units from Kelvins to degrees Celsius.</span>
<span class="co">#</span>
<span class="co"># Calculate mean daily temperatures as:</span>
<span class="co">#     tasmean = mean(tasmax, tasmin)</span>
<span class="co">#</span>
<span class="co"># Calculate wind speed as:</span>
<span class="co">#     speed = sqrt(northward_wind^2 + eastward_wind^2)</span>
<span class="co">#</span>
<span class="co">#</span>
<span class="co"># This script requires the watershed name to be passed in as a command line</span>
<span class="co"># argument and expects the watershed name to appear in the subset files</span>
<span class="co"># which it will convert for Envision.</span>
<span class="co">#</span>
<span class="co"># </span>
<span class="co"># Evan Linde, Oklahoma State University, 2017-10-05</span>
<span class="co">#</span>


<span class="kw">if [</span> <span class="st">&quot;</span><span class="ot">$#</span><span class="st">&quot;</span> <span class="ot">-ne</span> 1<span class="kw"> ]</span>; <span class="kw">then</span>
    <span class="kw">echo</span> <span class="st">&quot;This script requires the watershed directory name to be&quot;</span>
    <span class="kw">echo</span> <span class="st">&quot;included as a command line argument.&quot;</span>
    <span class="kw">exit</span> 1<span class="kw">;</span>
<span class="kw">fi</span>
<span class="ot">watershed=</span><span class="st">&quot;</span><span class="ot">$1</span><span class="st">&quot;</span> 

<span class="co"># Parent directories</span>
<span class="ot">sourcepdir=</span><span class="st">&quot;/data/public/datasets/MACA/MACAv2_Derived/</span><span class="ot">${watershed}</span><span class="st">&quot;</span>
<span class="ot">destpdir=</span><span class="st">&quot;/data/public/datasets/MACA/MACAv2_Derived/</span><span class="ot">${watershed}</span><span class="st">_Envision&quot;</span>


<span class="co"># Declare arrays for the variables we&#39;ll need to loop over</span>
<span class="ot">MODELS=(</span><span class="st">&quot;bcc-csm1-1-m&quot;</span> <span class="st">&quot;bcc-csm1-1&quot;</span> <span class="st">&quot;BNU-ESM&quot;</span> <span class="st">&quot;CanESM2&quot;</span> <span class="st">&quot;CCSM4&quot;</span> <span class="st">&quot;CNRM-CM5&quot;</span> 
    <span class="st">&quot;CSIRO-Mk3-6-0&quot;</span> <span class="st">&quot;GFDL-ESM2G&quot;</span> <span class="st">&quot;GFDL-ESM2M&quot;</span> <span class="st">&quot;HadGEM2-CC365&quot;</span> <span class="st">&quot;HadGEM2-ES365&quot;</span> 
    <span class="st">&quot;inmcm4&quot;</span> <span class="st">&quot;IPSL-CM5A-LR&quot;</span> <span class="st">&quot;IPSL-CM5A-MR&quot;</span> <span class="st">&quot;IPSL-CM5B-LR&quot;</span> <span class="st">&quot;MIROC-ESM-CHEM&quot;</span> 
    <span class="st">&quot;MIROC-ESM&quot;</span> <span class="st">&quot;MIROC5&quot;</span> <span class="st">&quot;MRI-CGCM3&quot;</span> <span class="st">&quot;NorESM1-M&quot;</span>)
<span class="ot">YEAR_BLOCKS=(</span><span class="st">&quot;2021_2025&quot;</span> <span class="st">&quot;2026_2030&quot;</span> <span class="st">&quot;2031_2035&quot;</span> <span class="st">&quot;2036_2040&quot;</span> <span class="st">&quot;2041_2045&quot;</span> 
    <span class="st">&quot;2046_2050&quot;</span> <span class="st">&quot;2051_2055&quot;</span> <span class="st">&quot;2056_2060&quot;</span> <span class="st">&quot;2061_2065&quot;</span> <span class="st">&quot;2066_2070&quot;</span> <span class="st">&quot;2071_2075&quot;</span> 
    <span class="st">&quot;2076_2080&quot;</span> <span class="st">&quot;2081_2085&quot;</span> <span class="st">&quot;2086_2090&quot;</span> <span class="st">&quot;2091_2095&quot;</span> <span class="st">&quot;2096_2099&quot;</span>)
<span class="ot">RCPS=(</span><span class="st">&quot;rcp45&quot;</span> <span class="st">&quot;rcp85&quot;</span><span class="ot">)</span>
<span class="co"># skipping &quot;rhsmax&quot; and &quot;rhsmin&quot;</span>
<span class="co"># We&#39;re not actually usin the VARS array in this script</span>
<span class="ot">VARS=(</span><span class="st">&quot;tasmax&quot;</span> <span class="st">&quot;tasmin&quot;</span> <span class="st">&quot;huss&quot;</span> <span class="st">&quot;pr&quot;</span> <span class="st">&quot;rsds&quot;</span> <span class="st">&quot;uas&quot;</span> <span class="st">&quot;vas&quot;</span><span class="ot">)</span>  
<span class="co"># Variables used in Envision netcdf filenames. (We are using this array.)</span>
<span class="ot">ENVISION_VARS=(</span><span class="st">&quot;huss&quot;</span> <span class="st">&quot;rsds&quot;</span> <span class="st">&quot;pr&quot;</span> <span class="st">&quot;tasmax&quot;</span> <span class="st">&quot;tasmin&quot;</span> <span class="st">&quot;tasmean&quot;</span> <span class="st">&quot;wind&quot;</span><span class="ot">)</span>


<span class="co"># Create a temp directory</span>
<span class="ot">tmpdir=$(</span><span class="kw">mktemp</span> -d<span class="ot">)</span>

<span class="kw">for</span> <span class="kw">model</span> in <span class="ot">${MODELS[@]}</span><span class="kw">;</span> <span class="kw">do</span>

    <span class="kw">for</span> <span class="kw">rcp</span> in <span class="ot">${RCPS[@]}</span><span class="kw">;</span> <span class="kw">do</span>

        <span class="kw">for</span> <span class="kw">block</span> in <span class="ot">${YEAR_BLOCKS[@]}</span><span class="kw">;</span> <span class="kw">do</span>

            <span class="ot">bfirst=${block:0:4}</span>  <span class="co"># first year in year block</span>
            <span class="ot">blast=${block:5:4}</span>   <span class="co"># last year in year block</span>
            <span class="ot">sourcedir=</span><span class="st">&quot;</span><span class="ot">${sourcepdir}</span><span class="st">/</span><span class="ot">${model}</span><span class="st">&quot;</span> <span class="co"># subset files for current model</span>
            <span class="ot">destdir=</span><span class="st">&quot;</span><span class="ot">${destpdir}</span><span class="st">/</span><span class="ot">${model}</span><span class="st">&quot;</span>     <span class="co"># output files for current model</span>
            <span class="kw">mkdir</span> -p <span class="ot">${destdir}</span>/yearly    <span class="co"># output dir for yearly files</span>

            <span class="co"># Arbitrarily using the huss file as a template for other </span>
            <span class="co"># filenames. Most models have &quot;r1i1p1&quot; but at least one has </span>
            <span class="co"># &quot;r6i1p1&quot;, so instead of a condition based on the model name, </span>
            <span class="co"># we&#39;re detecting the correct string with bash&#39;s glob expressions </span>
            <span class="co"># (i.e. the &quot;r?i1p1&quot; part). This could be a problem if more than </span>
            <span class="co"># one file matches the expression.</span>
            <span class="ot">huss_basename=$(</span><span class="kw">basename</span> <span class="ot">$(</span><span class="kw">echo</span> <span class="ot">${sourcedir}</span>/macav2metdata_huss_<span class="ot">${model}</span>_r?i1p1_<span class="ot">${rcp}</span>_<span class="ot">${block}</span>_<span class="ot">${watershed}</span>_daily.nc<span class="ot">))</span>
            <span class="co"># Prefix for yearly files; we&#39;ll add &quot;${year}.nc&quot; to the end </span>
            <span class="co"># when creating yearly files</span>
            <span class="ot">huss_y_prefix=</span><span class="st">&quot;</span><span class="ot">${huss_basename/</span>_<span class="ot">${block}</span>_<span class="ot">${watershed}</span>_daily.nc<span class="ot">/</span>_<span class="ot">${watershed}</span>_daily<span class="ot">}</span><span class="st">&quot;</span>

            <span class="co"># Set variables for the input and output files using the huss </span>
            <span class="co"># filename as a template replacing &quot;huss&quot; with the target </span>
            <span class="co"># variable name</span>
            <span class="ot">subset_huss=</span><span class="st">&quot;</span><span class="ot">${sourcedir}</span><span class="st">/</span><span class="ot">${huss_basename}</span><span class="st">&quot;</span>
            <span class="ot">envision_huss=</span><span class="st">&quot;</span><span class="ot">${destdir}</span><span class="st">/</span><span class="ot">${huss_basename}</span><span class="st">&quot;</span>
            <span class="ot">subset_rsds=</span><span class="st">&quot;</span><span class="ot">${sourcedir}</span><span class="st">/</span><span class="ot">${huss_basename/</span>huss<span class="ot">/</span>rsds<span class="ot">}</span><span class="st">&quot;</span>
            <span class="ot">envision_rsds=</span><span class="st">&quot;</span><span class="ot">${destdir}</span><span class="st">/</span><span class="ot">${huss_basename/</span>huss<span class="ot">/</span>rsds<span class="ot">}</span><span class="st">&quot;</span>
            <span class="ot">subset_pr=</span><span class="st">&quot;</span><span class="ot">${sourcedir}</span><span class="st">/</span><span class="ot">${huss_basename/</span>huss<span class="ot">/</span>pr<span class="ot">}</span><span class="st">&quot;</span>
            <span class="ot">envision_pr=</span><span class="st">&quot;</span><span class="ot">${destdir}</span><span class="st">/</span><span class="ot">${huss_basename/</span>huss<span class="ot">/</span>pr<span class="ot">}</span><span class="st">&quot;</span>
            <span class="ot">subset_tasmax=</span><span class="st">&quot;</span><span class="ot">${sourcedir}</span><span class="st">/</span><span class="ot">${huss_basename/</span>huss<span class="ot">/</span>tasmax<span class="ot">}</span><span class="st">&quot;</span>
            <span class="ot">envision_tasmax=</span><span class="st">&quot;</span><span class="ot">${destdir}</span><span class="st">/</span><span class="ot">${huss_basename/</span>huss<span class="ot">/</span>tasmax<span class="ot">}</span><span class="st">&quot;</span>
            <span class="ot">subset_tasmin=</span><span class="st">&quot;</span><span class="ot">${sourcedir}</span><span class="st">/</span><span class="ot">${huss_basename/</span>huss<span class="ot">/</span>tasmin<span class="ot">}</span><span class="st">&quot;</span>
            <span class="ot">envision_tasmin=</span><span class="st">&quot;</span><span class="ot">${destdir}</span><span class="st">/</span><span class="ot">${huss_basename/</span>huss<span class="ot">/</span>tasmin<span class="ot">}</span><span class="st">&quot;</span>
            <span class="ot">envision_tasmean=</span><span class="st">&quot;</span><span class="ot">${destdir}</span><span class="st">/</span><span class="ot">${huss_basename/</span>huss<span class="ot">/</span>tasmean<span class="ot">}</span><span class="st">&quot;</span>
            <span class="ot">subset_vas=</span><span class="st">&quot;</span><span class="ot">${sourcedir}</span><span class="st">/</span><span class="ot">${huss_basename/</span>huss<span class="ot">/</span>vas<span class="ot">}</span><span class="st">&quot;</span>
            <span class="ot">subset_uas=</span><span class="st">&quot;</span><span class="ot">${sourcedir}</span><span class="st">/</span><span class="ot">${huss_basename/</span>huss<span class="ot">/</span>uas<span class="ot">}</span><span class="st">&quot;</span>
            <span class="ot">envision_wind=</span><span class="st">&quot;</span><span class="ot">${destdir}</span><span class="st">/</span><span class="ot">${huss_basename/</span>huss<span class="ot">/</span>wind<span class="ot">}</span><span class="st">&quot;</span>

            <span class="co"># Do all the conversions</span>
            <span class="co">#    subset huss --&gt; envision huss</span>
            <span class="co">#    subset rsds --&gt; envision rsds</span>
            <span class="co">#    subset pr --&gt; envision pr</span>
            <span class="co">#    subset tasmax --&gt; envision tasmax</span>
            <span class="co">#    subset tasmin --&gt; envision tasmin</span>
            <span class="co">#    envision tasmax and tasmin --&gt; envision tasmean</span>
            <span class="co">#    subset vas and uas --&gt; envision wind</span>
            <span class="co">#</span>
            <span class="co"># We&#39;re not looping here since we&#39;re not doing the</span>
            <span class="co"># same thing for all the variables.</span>

            <span class="co"># huss (specific_humidity)</span>
            <span class="kw">ncap2</span> -3 -s <span class="st">&#39;lon=lon-360&#39;</span> <span class="ot">${subset_huss}</span> <span class="ot">${envision_huss}</span>

            <span class="co"># rsds (surface_downwelling_shortwave_flux_in_air)</span>
            <span class="kw">ncap2</span> -3 -s <span class="st">&#39;lon=lon-360&#39;</span> <span class="ot">${subset_rsds}</span> <span class="ot">${envision_rsds}</span>

            <span class="co"># pr (precipitation)</span>
            <span class="kw">ncap2</span> -3 -s <span class="st">&#39;lon=lon-360&#39;</span> <span class="ot">${subset_pr}</span> <span class="ot">${envision_pr}</span>

            <span class="co"># tasmax (air_temperature)</span>
            <span class="co"># In addition to longitude, we&#39;re also converting temperature</span>
            <span class="co"># from K to C, and then we&#39;re updating the metadata to reflect</span>
            <span class="co"># this with the ncatted command.</span>
            <span class="kw">ncap2</span> -3 -s <span class="st">&#39;lon=lon-360; air_temperature=(air_temperature - 273.15)&#39;</span> <span class="ot">${subset_tasmax}</span> <span class="ot">${envision_tasmax}</span>
            <span class="kw">ncatted</span> -a units,air_temperature,o,char,<span class="st">&#39;C&#39;</span> <span class="ot">${envision_tasmax}</span>

            <span class="co"># tasmin (air_temperature)</span>
            <span class="kw">ncap2</span> -3 -s <span class="st">&#39;lon=lon-360; air_temperature=(air_temperature - 273.15)&#39;</span> <span class="ot">${subset_tasmin}</span> <span class="ot">${envision_tasmin}</span>
            <span class="kw">ncatted</span> -a units,air_temperature,o,char,<span class="st">&#39;C&#39;</span> <span class="ot">${envision_tasmin}</span>

            <span class="co"># Calculated variable: tasmean (air_temperature)</span>
            <span class="kw">nces</span> -3 <span class="ot">${envision_tasmax}</span> <span class="ot">${envision_tasmin}</span> <span class="ot">${envision_tasmean}</span>
            <span class="kw">ncatted</span> -a cell_methods,air_temperature,o,c,<span class="st">&#39;time: mean(interval: 24 hours)&#39;</span> -a long_name,air_temperature,o,c,<span class="st">&#39;Daily Mean Near-Surface Air Temperature&#39;</span> <span class="ot">${envision_tasmean}</span>

            <span class="co"># Calculated variable: wind (wind_speed)</span>
            <span class="co"># Make a temporary copy of the vas file. (It will be modified.)</span>
            <span class="kw">cp</span> <span class="ot">${subset_vas}</span> <span class="ot">${tmpdir}</span>/vas.nc
            <span class="co"># Append the uas variable (eastward_wind) into the vas file</span>
            <span class="kw">ncks</span> -A <span class="ot">${subset_uas}</span> <span class="ot">${tmpdir}</span>/vas.nc
            <span class="co"># Calculate wind speed. The output file from this command will have</span>
            <span class="co"># three variables: northward_wind, eastward_wind, and wind_speed.</span>
            <span class="kw">ncap2</span> -s <span class="st">&#39;lon=lon-360; wind_speed=sqrt(northward_wind^2 + eastward_wind^2)&#39;</span> <span class="ot">${tmpdir}</span>/vas.nc <span class="ot">${tmpdir}</span>/wind.nc
            <span class="co"># Extract the wind_speed variable into its own file</span>
            <span class="kw">ncks</span> -3 -v wind_speed <span class="ot">${tmpdir}</span>/wind.nc <span class="ot">${envision_wind}</span>
            <span class="co"># Get rid of temporary files</span>
            <span class="kw">rm</span> <span class="ot">${tmpdir}</span>/vas.nc <span class="ot">${tmpdir}</span>/wind.nc
            <span class="co"># Update metadata. (The wind_speed variable inherited the</span>
            <span class="co"># metadata from the vas variable, northward_wind.)</span>
            <span class="kw">ncatted</span> -a comments,wind_speed,o,c,<span class="st">&#39;Surface (10m) wind speed&#39;</span> -a long_name,wind_speed,o,c,<span class="st">&#39;Wind Speed&#39;</span> -a standard_name,wind_speed,o,c,<span class="st">&#39;wind_speed&#39;</span> <span class="ot">${envision_wind}</span>

            <span class="co"># Split all the envision files into yearly files</span>
            <span class="co"># Doing a C-style for loop here because &quot;{${bfirst}..${blast}}&quot;</span>
            <span class="co"># doesn&#39;t work like it would with literals.</span>
            <span class="kw">for</span> <span class="kw">((</span>y=<span class="ot">${bfirst}</span>; y&lt;=<span class="ot">${blast}</span>; y++<span class="kw">))</span>; <span class="kw">do</span>

                <span class="co"># We can conveniently loop over the variables here since </span>
                <span class="co"># we&#39;re doing the same thing for all of them.</span>
                <span class="kw">for</span> <span class="kw">ev</span> in <span class="ot">${ENVISION_VARS[@]}</span><span class="kw">;</span> <span class="kw">do</span>

                    <span class="ot">infile=</span><span class="st">&quot;</span><span class="ot">${destdir}</span><span class="st">/</span><span class="ot">${huss_basename/</span>huss<span class="ot">/${ev}}</span><span class="st">&quot;</span>
                    <span class="ot">outfile=</span><span class="st">&quot;</span><span class="ot">${destdir}</span><span class="st">/yearly/</span><span class="ot">${huss_y_prefix/</span>huss<span class="ot">/${ev}}</span><span class="st">_</span><span class="ot">${y}</span><span class="st">.nc&quot;</span>
                    <span class="co"># Year Bounds</span>
                    <span class="co"># Get the first and last day of the given year in the form</span>
                    <span class="co"># of &quot;first,last&quot; where first and last are floating point</span>
                    <span class="co"># numbers representing &quot;days since 1900-01-01&quot;.</span>
                    <span class="ot">yb=$(</span><span class="kw">bash</span> yearbounds.sh <span class="ot">${y})</span>

                    <span class="kw">ncks</span> -d time,<span class="ot">${yb}</span> <span class="ot">${infile}</span> <span class="ot">${outfile}</span>

                <span class="kw">done</span>  <span class="co"># end of envision vars loop</span>

            <span class="kw">done</span>  <span class="co"># end of years inside year block loop</span>

        <span class="kw">done</span>  <span class="co"># end of year blocks loop</span>

    <span class="kw">done</span>  <span class="co"># end of rcps loop</span>

<span class="kw">done</span>  <span class="co"># end of models loop</span>

<span class="co"># Cleanup: remove the temp directory we created at the beginning</span>
<span class="kw">rm</span> -rf <span class="ot">${tmpdir}</span></code></pre>
<h3 id="kiamichi-1">Kiamichi</h3>
<p>Run the script to convert the subset data into Envision format:</p>
<pre class="sourceCode bash"><code class="sourceCode bash"><span class="kw">bash</span> maca_subset_to_envision.sh Kiamichi</code></pre>
<p>The conversion for Kiamichi took about 1.75 hours (running concurrently with the Cimarron subset). The total size (which includes duplicated data) is about 50 GB; the size of all the yearly files is about 25 GB.</p>
<h3 id="cimarron-1">Cimarron</h3>
<p>Run the script to convert the subset data into Envision format:</p>
<pre class="sourceCode bash"><code class="sourceCode bash"><span class="kw">bash</span> maca_subset_to_envision.sh Cimarron</code></pre>
<p>The conversion for Cimarron took about 2.8 hours. The total size (which includes duplicated data) is about 299 GB; the size of all the yearly files is about 150 GB.</p>
<h2 id="converting-for-swat-models">Converting for SWAT Models</h2>
<p>For our SWAT models, these are the variables that we'll need:</p>
<table>
<thead>
<tr class="header">
<th align="left">Variable</th>
<th align="left">Units</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">maximum temperature</td>
<td align="left">°C</td>
</tr>
<tr class="even">
<td align="left">minimum temperature</td>
<td align="left">°C</td>
</tr>
<tr class="odd">
<td align="left">solar radiation</td>
<td align="left">W m<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msup><mi></mi><mrow><mo>−</mo><mn>2</mn></mrow></msup></mrow></math></td>
</tr>
<tr class="even">
<td align="left">relative humidity</td>
<td align="left">%</td>
</tr>
<tr class="odd">
<td align="left">wind speed</td>
<td align="left">m s<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msup><mi></mi><mrow><mo>−</mo><mn>1</mn></mrow></msup></mrow></math></td>
</tr>
<tr class="even">
<td align="left">precipitation</td>
<td align="left">mm day<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msup><mi></mi><mrow><mo>−</mo><mn>1</mn></mrow></msup></mrow></math></td>
</tr>
</tbody>
</table>
<p>The MACAv2-METDATA dataset has all of these except for relative humidity and wind speed, but we can derive them from the provided variables.</p>
<p>From examining the header data for one of each variable's netCDF files, we can see that we'll need to do some unit conversion on the temperature variables; other than those everything is already in the units we want. (The wind <em>vectors</em> which we'll use to derive wind speed are also in our desired units.)</p>
<p>Other than relative humidity (Envision used <em>specific</em> humidity), everything we need for SWAT is already available in the data we converted for Envision, so we'll use our Envision files as a base for the SWAT conversions.</p>
<p>All the models except CCSM4 and NorESM1-M have variables for maximum and minimum relative humidity, so we'll have to exclude these models. For the rest, we'll calculate daily relative humidity as the mean of the maximum and minimum values.</p>
<p>Each of the watersheds has sub-polygons (hydrologic response units?) and we want one SWAT &quot;weather station&quot; for each of them. To make things simple and avoid introducing any unnecessary error, we'll use data for the centroids of each sub-polygon instead of trying to calculate (area-weighted) average values of its component raster cells.</p>
<p>The MACA <a href="https://climate.northwestknowledge.net/MACA/MACAfaq.php">FAQ page</a> recommends that temperature and precipitation variables should be bias corrected for this use. This process is not covered here.</p>
<p>So here are the steps we need to perform:</p>
<ol>
<li>Create relative humidity netCDF files and add them to the Envision outputs<br /></li>
<li>Find centroids of watershed sub-polygons<br /></li>
<li>Find grid cell closest to each centroid<br /></li>
<li>Subset data to each grid cell<br /></li>
<li>Dump to plain text format for SWAT</li>
</ol>
<h3 id="setup-steps">Setup Steps</h3>
<p><strong>Producing Mean Relative Humidity netCDF</strong></p>
<p>We follow essentially the same procedue here that we used to generate the mean temperature files.</p>
<pre><code>nces rhsmax.nc rhsmin.nc rhsmean.nc
ncatted -a long_name,relative_humidity,o,c,&#39;Surface Daily Mean Relative Humidity&#39; -a cell_methods,relative_humidity,o,c,&#39;time: mean(interval: 24 hours)&#39;</code></pre>
<p>Here's the script to create the relative humidity files. If you notice that it looks a lot like the script we used to convert data for Envision, that's because it's the same loop with the guts replaced with our commands to produce a relative humidity file.</p>
<p><strong><code>relative_humidity.sh</code></strong></p>
<pre class="sourceCode bash"><code class="sourceCode bash"><span class="co">#!/bin/bash</span>
<span class="co">#</span>
<span class="co"># Create files for mean daily relative humidity (in the ${watershed_Envision</span>
<span class="co"># directory). This isn&#39;t for Envision itself, but to make data conversion</span>
<span class="co"># for SWAT easier.</span>
<span class="co">#</span>
<span class="co"># This script requires the watershed name to be passed in as a command line</span>
<span class="co"># argument.</span>
<span class="co">#</span>
<span class="co"># Evan Linde, Oklahoma State University, 2017-10-11</span>
<span class="co">#</span>


<span class="kw">if [</span> <span class="st">&quot;</span><span class="ot">$#</span><span class="st">&quot;</span> <span class="ot">-ne</span> 1<span class="kw"> ]</span>; <span class="kw">then</span>
    <span class="kw">echo</span> <span class="st">&quot;This script requires the watershed directory name to be&quot;</span>
    <span class="kw">echo</span> <span class="st">&quot;included as a command line argument.&quot;</span>
    <span class="kw">exit</span> 1<span class="kw">;</span>
<span class="kw">fi</span>
<span class="ot">watershed=</span><span class="st">&quot;</span><span class="ot">$1</span><span class="st">&quot;</span> 

<span class="co"># Parent directories</span>
<span class="ot">sourcepdir=</span><span class="st">&quot;/data/public/datasets/MACA/MACAv2_Derived/</span><span class="ot">${watershed}</span><span class="st">&quot;</span>
<span class="ot">destpdir=</span><span class="st">&quot;/data/public/datasets/MACA/MACAv2_Derived/</span><span class="ot">${watershed}</span><span class="st">_Envision&quot;</span>


<span class="co"># Declare arrays for the variables we&#39;ll need to loop over</span>

<span class="co"># Excluding CCSM4 and NorESM1-M since these don&#39;t have rhsmin and rhsmax</span>
<span class="ot">MODELS=(</span><span class="st">&quot;bcc-csm1-1-m&quot;</span> <span class="st">&quot;bcc-csm1-1&quot;</span> <span class="st">&quot;BNU-ESM&quot;</span> <span class="st">&quot;CanESM2&quot;</span> <span class="st">&quot;CNRM-CM5&quot;</span> 
    <span class="st">&quot;CSIRO-Mk3-6-0&quot;</span> <span class="st">&quot;GFDL-ESM2G&quot;</span> <span class="st">&quot;GFDL-ESM2M&quot;</span> <span class="st">&quot;HadGEM2-CC365&quot;</span> <span class="st">&quot;HadGEM2-ES365&quot;</span> 
    <span class="st">&quot;inmcm4&quot;</span> <span class="st">&quot;IPSL-CM5A-LR&quot;</span> <span class="st">&quot;IPSL-CM5A-MR&quot;</span> <span class="st">&quot;IPSL-CM5B-LR&quot;</span> <span class="st">&quot;MIROC-ESM-CHEM&quot;</span> 
    <span class="st">&quot;MIROC-ESM&quot;</span> <span class="st">&quot;MIROC5&quot;</span> <span class="st">&quot;MRI-CGCM3&quot;</span>)

<span class="ot">YEAR_BLOCKS=(</span><span class="st">&quot;2021_2025&quot;</span> <span class="st">&quot;2026_2030&quot;</span> <span class="st">&quot;2031_2035&quot;</span> <span class="st">&quot;2036_2040&quot;</span> <span class="st">&quot;2041_2045&quot;</span> 
    <span class="st">&quot;2046_2050&quot;</span> <span class="st">&quot;2051_2055&quot;</span> <span class="st">&quot;2056_2060&quot;</span> <span class="st">&quot;2061_2065&quot;</span> <span class="st">&quot;2066_2070&quot;</span> <span class="st">&quot;2071_2075&quot;</span> 
    <span class="st">&quot;2076_2080&quot;</span> <span class="st">&quot;2081_2085&quot;</span> <span class="st">&quot;2086_2090&quot;</span> <span class="st">&quot;2091_2095&quot;</span> <span class="st">&quot;2096_2099&quot;</span>)

<span class="ot">RCPS=(</span><span class="st">&quot;rcp45&quot;</span> <span class="st">&quot;rcp85&quot;</span><span class="ot">)</span>


<span class="co"># Create a temp directory</span>
<span class="ot">tmpdir=$(</span><span class="kw">mktemp</span> -d<span class="ot">)</span>

<span class="kw">for</span> <span class="kw">model</span> in <span class="ot">${MODELS[@]}</span><span class="kw">;</span> <span class="kw">do</span>

    <span class="ot">sourcedir=</span><span class="st">&quot;</span><span class="ot">${sourcepdir}</span><span class="st">/</span><span class="ot">${model}</span><span class="st">&quot;</span>  <span class="co"># subset files for current model</span>
    <span class="ot">destdir=</span><span class="st">&quot;</span><span class="ot">${destpdir}</span><span class="st">/</span><span class="ot">${model}</span><span class="st">&quot;</span>      <span class="co"># output files for current model</span>

    <span class="kw">for</span> <span class="kw">rcp</span> in <span class="ot">${RCPS[@]}</span><span class="kw">;</span> <span class="kw">do</span>

        <span class="kw">for</span> <span class="kw">block</span> in <span class="ot">${YEAR_BLOCKS[@]}</span><span class="kw">;</span> <span class="kw">do</span>

            <span class="ot">subset_rhsmax=</span><span class="st">&quot;</span><span class="ot">${sourcedir}</span><span class="st">/macav2metdata_rhsmax_</span><span class="ot">${model}</span><span class="st">_r1i1p1_</span><span class="ot">${rcp}</span><span class="st">_</span><span class="ot">${block}</span><span class="st">_</span><span class="ot">${watershed}</span><span class="st">_daily.nc&quot;</span>
            <span class="ot">subset_rhsmin=</span><span class="st">&quot;</span><span class="ot">${sourcedir}</span><span class="st">/macav2metdata_rhsmin_</span><span class="ot">${model}</span><span class="st">_r1i1p1_</span><span class="ot">${rcp}</span><span class="st">_</span><span class="ot">${block}</span><span class="st">_</span><span class="ot">${watershed}</span><span class="st">_daily.nc&quot;</span>
            <span class="ot">envision_rhsmean=</span><span class="st">&quot;</span><span class="ot">${destdir}</span><span class="st">/macav2metdata_rhsmean_</span><span class="ot">${model}</span><span class="st">_r1i1p1_</span><span class="ot">${rcp}</span><span class="st">_</span><span class="ot">${block}</span><span class="st">_</span><span class="ot">${watershed}</span><span class="st">_daily.nc&quot;</span>

            <span class="co"># Calculate mean relative humidity</span>
            <span class="kw">nces</span> <span class="ot">${subset_rhsmax}</span> <span class="ot">${subset_rhsmin}</span> <span class="ot">${tmpdir}</span>/rhsmean.nc

            <span class="co"># Fix longitude</span>
            <span class="kw">ncap2</span> -3 -s <span class="st">&#39;lon=lon-360&#39;</span> <span class="ot">${tmpdir}</span>/rhsmean.nc <span class="ot">${envision_rhsmean}</span>

            <span class="co"># Delete temp file</span>
            <span class="kw">rm</span> <span class="ot">${tmpdir}</span>/rhsmean.nc

            <span class="co"># Update metadata</span>
            <span class="kw">ncatted</span> -a long_name,relative_humidity,o,c,<span class="st">&#39;Surface Daily Mean Relative Humidity&#39;</span> -a cell_methods,relative_humidity,o,c,<span class="st">&#39;time: mean(interval: 24 hours)&#39;</span> <span class="ot">${envision_rhsmean}</span> 

        <span class="kw">done</span>  <span class="co"># end of year blocks loop</span>

    <span class="kw">done</span>  <span class="co"># end of rcps loop</span>

<span class="kw">done</span>  <span class="co"># end of models loop</span>

<span class="co"># Cleanup: remove the temp directory we created at the beginning</span>
<span class="kw">rm</span> -rf <span class="ot">${tmpdir}</span></code></pre>
<p>Now we create the relative humidity files:</p>
<pre class="sourceCode bash"><code class="sourceCode bash"><span class="kw">bash</span> relative_humidity.sh Cimarron
<span class="kw">bash</span> relative_humidity.sh Kiamichi</code></pre>
<p>This only takes about 6 minutes for Kiamichi and 33 minutes for Cimarron.</p>
<p><strong>Finding the Centroids</strong></p>
<p>(Done in QGIS 2.14.19-Essen)</p>
<p>Open QGIS and import the shapefile as a vector layer.</p>
<p>Create a new point shapefile of the polygon centroids.</p>
<blockquote>
<ol>
<li>Vector menu --&gt; Geometry Tools --&gt; Polygon Centroids<br /></li>
<li>Choose the existing polygon shapefile's layer name and set the name for the output point shapefile<br /></li>
<li>Allow new layer to be added to canvas</li>
</ol>
</blockquote>
<p>Right click the new centroid layer and choose &quot;Save As&quot; and save as a CSV file.</p>
<p>If we want to extract the data from the original format files (with longitude values all positive), then we'll want to extract and convert the coordinates with a command like this:</p>
<pre><code>awk -F, &#39;FNR &gt; 1 {printf(&quot;%0.13f %0.13f\n&quot;,(360+$1),$2)}&#39; centroids_layer.csv &gt; centroids.txt</code></pre>
<p>Otherwise (if we plan to extract the data from the Envision format files), then we won't need to convert any of the coordinates and we can extract the data like this:</p>
<pre><code>awk -F, &#39;FNR&gt;1{print $1,$2}&#39; centroids_layer.csv &gt; centroids.txt</code></pre>
<p>Explanation:</p>
<blockquote>
<p>The <code>awk</code> commands here extract a subset of the data from a comma-separated values file. The flag <code>-F,</code> specifies to treat commas as the delimiter rather than the default whitespace characters. The script we give to the <code>awk</code> command, <code>FNR&gt;1{print $1,$2}</code>, tells <code>awk</code> to print the first and second fields (by default separated with a space) for every line of the file after the first line.</p>
</blockquote>
<h3 id="the-actual-conversion">The Actual Conversion</h3>
<p>For the steps where we're actually selecting and extracting the data we want for SWAT, we tried a couple of different approaches. Both of these worked, but we were able to use a more efficient algorithm and get much better speed out of the second approach. We'll show both here.</p>
<h3 id="first-approach-bash-methods">First Approach: Bash Methods</h3>
<p>Make some text files listing the distinct latitude and longitude values for each of the watersheds:</p>
<pre class="sourceCode bash"><code class="sourceCode bash"><span class="kw">for</span> <span class="kw">watershed</span> in Cimarron Kiamichi<span class="kw">;</span> <span class="kw">do</span>
    <span class="kw">for</span> <span class="kw">v</span> in lat lon<span class="kw">;</span> <span class="kw">do</span>
        <span class="kw">ncdump</span> -v <span class="ot">$v</span> <span class="ot">${watershed}</span>_Envision/CCSM4/macav2metdata_pr_CCSM4_r6i1p1_rcp85_2021_2025_<span class="ot">${watershed}</span>_daily.nc <span class="kw">|</span> <span class="kw">sed</span> -n <span class="st">&#39;/^ &#39;&quot;</span><span class="ot">$v</span><span class="st">&quot;&#39; =/,/\;/{s/ *&#39;&quot;</span><span class="ot">$v</span><span class="st">&quot;&#39; = *//g;p}&#39;</span> <span class="kw">|</span> <span class="kw">tr</span> -d <span class="st">&#39;\n &#39;</span> <span class="kw">|</span> <span class="kw">tr</span> <span class="st">&#39;,;&#39;</span> <span class="st">&#39;\n\n&#39;</span> <span class="kw">&gt;</span> <span class="ot">${watershed}</span>_<span class="ot">${v}</span>s.txt
    <span class="kw">done</span>
<span class="kw">done</span></code></pre>
<p>Make a &quot;table&quot; of the file name variables and netcdf variable names:</p>
<h4 id="vars.txt"><code>vars.txt</code></h4>
<pre><code>rhsmean relative_humidity
rsds surface_downwelling_shortwave_flux_in_air
tasmax air_temperature
tasmin air_temperature
tasmean air_temperature
pr precipitation
wind wind_speed</code></pre>
<p><strong><code>envision_to_swat.sh</code></strong></p>
<pre class="sourceCode bash"><code class="sourceCode bash">
<span class="co">#!/bin/bash</span>

<span class="ot">watershed=</span><span class="st">&quot;</span><span class="ot">$1</span><span class="st">&quot;</span>
<span class="ot">centroids_file=</span><span class="st">&quot;</span><span class="ot">${watershed}</span><span class="st">_centroids.txt&quot;</span>
<span class="ot">lons_file=</span><span class="st">&quot;</span><span class="ot">${watershed}</span><span class="st">_lons.txt&quot;</span>
<span class="ot">lats_file=</span><span class="st">&quot;</span><span class="ot">${watershed}</span><span class="st">_lats.txt&quot;</span>
<span class="ot">var_map=</span><span class="st">&quot;vars.txt&quot;</span>

<span class="ot">MODELS=(</span><span class="st">&quot;bcc-csm1-1-m&quot;</span> <span class="st">&quot;bcc-csm1-1&quot;</span> <span class="st">&quot;BNU-ESM&quot;</span> <span class="st">&quot;CanESM2&quot;</span> <span class="st">&quot;CCSM4&quot;</span> <span class="st">&quot;CNRM-CM5&quot;</span> <span class="st">&quot;CSIRO-Mk3-6-0&quot;</span> <span class="st">&quot;GFDL-ESM2G&quot;</span> <span class="st">&quot;GFDL-ESM2M&quot;</span> <span class="st">&quot;HadGEM2-CC365&quot;</span> <span class="st">&quot;HadGEM2-ES365&quot;</span> <span class="st">&quot;inmcm4&quot;</span> <span class="st">&quot;IPSL-CM5A-LR&quot;</span> <span class="st">&quot;IPSL-CM5A-MR&quot;</span> <span class="st">&quot;IPSL-CM5B-LR&quot;</span> <span class="st">&quot;MIROC-ESM-CHEM&quot;</span> <span class="st">&quot;MIROC-ESM&quot;</span> <span class="st">&quot;MIROC5&quot;</span> <span class="st">&quot;MRI-CGCM3&quot;</span> <span class="st">&quot;NorESM1-M&quot;</span><span class="ot">)</span>
<span class="ot">YEAR_BLOCKS=(</span><span class="st">&quot;2021_2025&quot;</span> <span class="st">&quot;2026_2030&quot;</span> <span class="st">&quot;2031_2035&quot;</span> <span class="st">&quot;2036_2040&quot;</span> <span class="st">&quot;2041_2045&quot;</span> <span class="st">&quot;2046_2050&quot;</span> <span class="st">&quot;2051_2055&quot;</span> <span class="st">&quot;2056_2060&quot;</span> <span class="st">&quot;2061_2065&quot;</span> <span class="st">&quot;2066_2070&quot;</span> <span class="st">&quot;2071_2075&quot;</span> <span class="st">&quot;2076_2080&quot;</span> <span class="st">&quot;2081_2085&quot;</span> <span class="st">&quot;2086_2090&quot;</span> <span class="st">&quot;2091_2095&quot;</span> <span class="st">&quot;2096_2099&quot;</span><span class="ot">)</span>
<span class="ot">RCPS=(</span><span class="st">&quot;rcp45&quot;</span> <span class="st">&quot;rcp85&quot;</span><span class="ot">)</span>

<span class="ot">sourcepdir=</span><span class="st">&quot;/data/public/datasets/MACA/MACAv2_Derived/</span><span class="ot">${watershed}</span><span class="st">_Envision&quot;</span>
<span class="ot">destpdir=</span><span class="st">&quot;/data/public/datasets/MACA/MACAv2_Derived/</span><span class="ot">${watershed}</span><span class="st">_SWAT&quot;</span>


<span class="kw">function</span><span class="fu"> closest_point</span> <span class="kw">{</span>
    <span class="ot">point=$1</span>
    <span class="ot">file=$2</span>
    <span class="co">#echo &quot;point: ${point}, file: ${file}&quot; 1&gt;&amp;2</span>
    <span class="kw">awk</span> -v point=<span class="ot">${point}</span> <span class="st">&#39;function abs(v) {return v &lt; 0 ? -v : v} abs($1 - point) &lt; 0.02083 {closest=$1} END{print closest}&#39;</span> <span class="ot">${file}</span>
<span class="kw">}</span>


<span class="ot">tmpdir=$(</span><span class="kw">mktemp</span> -d<span class="ot">)</span>

<span class="kw">while</span> <span class="kw">read</span> <span class="ot">lon</span> <span class="ot">lat</span>; <span class="kw">do</span>
    <span class="co">#blah</span>
    <span class="ot">closest_lon=$(</span><span class="kw">closest_point</span> <span class="ot">${lon}</span> <span class="ot">${lons_file})</span>
    <span class="ot">closest_lat=$(</span><span class="kw">closest_point</span> <span class="ot">${lat}</span> <span class="ot">${lats_file})</span>
    <span class="ot">lonrounded=$(</span><span class="kw">printf</span> <span class="st">&quot;%.3f\n&quot;</span> <span class="ot">${closest_lon})</span>
    <span class="ot">latrounded=$(</span><span class="kw">printf</span> <span class="st">&quot;%.3f\n&quot;</span> <span class="ot">${closest_lat})</span>
    <span class="ot">eb=$(</span><span class="kw">echo</span> <span class="st">&quot;</span><span class="ot">${lonrounded}</span><span class="st"> + 0.001&quot;</span> <span class="kw">|</span> <span class="kw">bc</span><span class="ot">)</span>  <span class="co"># east boundary</span>
    <span class="ot">wb=$(</span><span class="kw">echo</span> <span class="st">&quot;</span><span class="ot">${lonrounded}</span><span class="st"> - 0.001&quot;</span> <span class="kw">|</span> <span class="kw">bc</span><span class="ot">)</span>  <span class="co"># west boundary</span>
    <span class="ot">nb=$(</span><span class="kw">echo</span> <span class="st">&quot;</span><span class="ot">${latrounded}</span><span class="st"> + 0.001&quot;</span> <span class="kw">|</span> <span class="kw">bc</span><span class="ot">)</span>  <span class="co"># north boundary</span>
    <span class="ot">sb=$(</span><span class="kw">echo</span> <span class="st">&quot;</span><span class="ot">${latrounded}</span><span class="st"> - 0.001&quot;</span> <span class="kw">|</span> <span class="kw">bc</span><span class="ot">)</span>  <span class="co"># south boundary</span>
    <span class="kw">echo</span> <span class="st">&quot;</span><span class="ot">${lon}</span><span class="st">,</span><span class="ot">${lat}</span><span class="st"> --&gt; </span><span class="ot">${lonrounded}</span><span class="st">,</span><span class="ot">${latrounded}</span><span class="st">&quot;</span>
    <span class="kw">for</span> <span class="kw">model</span> in <span class="ot">${MODELS[@]}</span><span class="kw">;</span> <span class="kw">do</span>
        <span class="ot">sourcedir=</span><span class="st">&quot;</span><span class="ot">${sourcepdir}</span><span class="st">/</span><span class="ot">${model}</span><span class="st">&quot;</span>
        <span class="ot">destdir=</span><span class="st">&quot;</span><span class="ot">${destpdir}</span><span class="st">/</span><span class="ot">${model}</span><span class="st">&quot;</span>
        <span class="kw">mkdir</span> -p <span class="ot">${destdir}</span>
        <span class="kw">for</span> <span class="kw">rcp</span> in <span class="ot">${RCPS[@]}</span><span class="kw">;</span> <span class="kw">do</span>
            <span class="kw">while</span> <span class="kw">read</span> <span class="ot">fvar</span> <span class="ot">ncvar</span>; <span class="kw">do</span>
                <span class="ot">outfile=</span><span class="st">&quot;</span><span class="ot">${destdir}</span><span class="st">/</span><span class="ot">${latrounded}</span><span class="st">_</span><span class="ot">${lonrounded}</span><span class="st">_</span><span class="ot">${fvar}</span><span class="st">_</span><span class="ot">${rcp}</span><span class="st">.txt&quot;</span>
                <span class="kw">echo</span> <span class="st">&quot;20210101&quot;</span> <span class="kw">&gt;</span> <span class="ot">${outfile}</span>
                <span class="kw">for</span> <span class="kw">block</span> in <span class="ot">${YEAR_BLOCKS[@]}</span><span class="kw">;</span> <span class="kw">do</span>
                    <span class="co">#infile=$(echo ${sourcedir}/macav2metdata_${fvar}_${model}_r?i1p1_${rcp}_${watershed}_daily_${year}.nc)</span>
                    <span class="ot">infile=$(</span><span class="kw">echo</span> <span class="ot">${sourcedir}</span>/macav2metdata_<span class="ot">${fvar}</span>_<span class="ot">${model}</span>_r?i1p1_<span class="ot">${rcp}</span>_<span class="ot">${block}</span>_<span class="ot">${watershed}</span>_daily.nc<span class="ot">)</span>
                    <span class="ot">temp_netcdf=</span><span class="st">&quot;</span><span class="ot">${tmpdir}</span><span class="st">/temp.nc&quot;</span>
                    <span class="kw">ncks</span> -d lat,<span class="ot">${sb}</span>,<span class="ot">${nb}</span> -d lon,<span class="ot">${wb}</span>,<span class="ot">${eb}</span> <span class="ot">${infile}</span> <span class="ot">${temp_netcdf}</span> <span class="kw">||</span> <span class="kw">{</span> <span class="kw">echo</span> <span class="ot">${infile}</span><span class="kw">;</span> <span class="kw">exit;</span> <span class="kw">}</span>
                    <span class="kw">ncdump</span> -v <span class="ot">${ncvar}</span> <span class="ot">${temp_netcdf}</span> <span class="kw">|</span> <span class="kw">sed</span> -n <span class="st">&#39;/^ &#39;&quot;</span><span class="ot">${ncvar}</span><span class="st">&quot;&#39; =/,/\;/{s/ *&#39;&quot;</span><span class="ot">${ncvar}</span><span class="st">&quot;&#39; = *//g;p}&#39;</span> <span class="kw">|</span> <span class="kw">tr</span> -d <span class="st">&#39;\n &#39;</span> <span class="kw">|</span> <span class="kw">tr</span> <span class="st">&#39;,;&#39;</span> <span class="st">&#39;\n\n&#39;</span> <span class="kw">&gt;&gt;</span> <span class="ot">${outfile}</span>
                    <span class="kw">rm</span> <span class="ot">${temp_netcdf}</span>
                <span class="kw">done</span>  <span class="co"># end year blocks loop</span>
            <span class="kw">done</span> <span class="kw">&lt;</span> <span class="ot">${var_map}</span>  <span class="co"># end variables loop</span>
        <span class="kw">done</span>  <span class="co"># end rcp loop</span>
    <span class="kw">done</span>  <span class="co"># end model loop</span>
<span class="kw">done</span> <span class="kw">&lt;</span> <span class="ot">${centroids_file}</span>  <span class="co"># end centroids loop</span>

<span class="kw">rm</span> -rf <span class="ot">${tmpdir}</span></code></pre>
<p>To run this bash script, we would use a command like this:</p>
<pre class="sourceCode bash"><code class="sourceCode bash"><span class="kw">bash</span> envision_to_swat.sh Cimarron</code></pre>
<p>However, this script turns out to be embarassingly slow, especially compared to all the other things we've done. After running overnight and half the morning, it's still less than half done. So let's look at doing this another way.</p>
<h3 id="second-approach-python-methods">Second Approach: Python Methods</h3>
<p>A netCDF library, <code>netCDF4</code>, is available for python. Using the Anaconda distribution of python, it can be installed with the command <code>conda install netCDF4</code>.</p>
<p>The manual is online at <a href="http://unidata.github.io/netcdf4-python/">http://unidata.github.io/netcdf4-python/</a>.</p>
<p>Here we start taking a look at using the netCDF4 library.</p>
<pre class="sourceCode python"><code class="sourceCode python"><span class="ch">from</span> netCDF4 <span class="ch">import</span> Dataset
<span class="ch">import</span> numpy <span class="ch">as</span> np</code></pre>
<pre class="sourceCode python"><code class="sourceCode python">ds = Dataset(<span class="st">&#39;/data/public/datasets/MACA/MACAv2_Derived/Kiamichi_Envision/CCSM4/macav2metdata_tasmax_CCSM4_r6i1p1_rcp45_2021_2025_Kiamichi_daily.nc&#39;</span>)</code></pre>
<p>Here we can see that the numpy array for our variable is shaped just like we'd expect based on the netCDF file's header info:</p>
<pre class="sourceCode python"><code class="sourceCode python">ds.variables[<span class="st">&quot;air_temperature&quot;</span>]</code></pre>
<pre><code>&lt;type &#39;netCDF4._netCDF4.Variable&#39;&gt;
float32 air_temperature(time, lat, lon)
    _FillValue: -9999.0
    cell_methods: time: maximum(interval: 24 hours)
    coordinates: time lon lat
    grid_mapping: crs
    height: 2 m
    long_name: Daily Maximum Near-Surface Air Temperature
    standard_name: air_temperature
    units: C
unlimited dimensions: 
current shape = (1826, 23, 35)
filling off</code></pre>
<p>Look at the array of latitude coordinates:</p>
<pre class="sourceCode python"><code class="sourceCode python">ds.variables[<span class="st">&quot;lat&quot;</span>][:]</code></pre>
<pre><code>array([ 33.89627075,  33.93793869,  33.97960281,  34.02126694,
        34.06293488,  34.104599  ,  34.14626694,  34.18793106,
        34.229599  ,  34.27126312,  34.31293106,  34.35459518,
        34.39626312,  34.43792725,  34.47959518,  34.52125931,
        34.56292725,  34.60459137,  34.64625931,  34.68792343,
        34.72959137,  34.77125549,  34.81292343])</code></pre>
<p>Since the shape for the variable is in the form (time, lat, lon), then we can get the time series for a single latitude, longitude point like this:</p>
<pre class="sourceCode python"><code class="sourceCode python">ds.variables[<span class="st">&quot;air_temperature&quot;</span>][:,<span class="dv">1</span>,<span class="dv">1</span>]</code></pre>
<pre><code>array([ 13.68578529,  15.75792885,  13.57393837, ...,   1.99129641,
         6.9480834 ,  13.55751991], dtype=float32)</code></pre>
<p>Use numpy's <code>where</code> function to find the array elements matching an expression.</p>
<p>Here we're finding the index of a latitude point that's within have a grid cell's width of some specific latitude point, i.e. the index of the latitude row that covers the point.</p>
<p>(Also, <code>np.where(ds.variables[&quot;lat&quot;][:] == 34.47959518)</code> didn't work even though we copied and pasted this number from the lat array -- more floating point fun.)</p>
<pre class="sourceCode python"><code class="sourceCode python">np.where(<span class="dt">abs</span>(<span class="fl">34.47959518</span> - ds.variables[<span class="st">&quot;lat&quot;</span>][:]) &lt; <span class="fl">0.020833</span>)[<span class="dv">0</span>][<span class="dv">0</span>]</code></pre>
<pre><code>14</code></pre>
<p>Note how we can adjust the number a bit and still get the same index (since we're still asking for a point covered by that row).</p>
<pre class="sourceCode python"><code class="sourceCode python">np.where(<span class="dt">abs</span>(<span class="fl">34.476</span> - ds.variables[<span class="st">&quot;lat&quot;</span>][:]) &lt; <span class="fl">0.020833</span>)[<span class="dv">0</span>][<span class="dv">0</span>]</code></pre>
<pre><code>14</code></pre>
<p>Look at the longitude variables:</p>
<pre class="sourceCode python"><code class="sourceCode python">ds.variables[<span class="st">&quot;lon&quot;</span>][:]</code></pre>
<pre><code>array([-95.85595703, -95.81430054, -95.77264404, -95.73095703,
       -95.68930054, -95.64764404, -95.60595703, -95.56430054,
       -95.52264404, -95.48095703, -95.43930054, -95.39764404,
       -95.35595703, -95.31430054, -95.27264404, -95.23095703,
       -95.18930054, -95.14764404, -95.10595703, -95.06430054,
       -95.02264404, -94.98095703, -94.93930054, -94.89764404,
       -94.85595703, -94.81430054, -94.77264404, -94.73098755,
       -94.68930054, -94.64764404, -94.60598755, -94.56433105,
       -94.52264404, -94.48098755, -94.43933105])</code></pre>
<p>Test finding a longitude index just like we found a latitude index before:</p>
<pre class="sourceCode python"><code class="sourceCode python">np.where(<span class="dt">abs</span>(-<span class="fl">95.60595703</span> - ds.variables[<span class="st">&quot;lon&quot;</span>][:]) &lt; <span class="fl">0.020833</span>)[<span class="dv">0</span>][<span class="dv">0</span>]</code></pre>
<pre><code>6</code></pre>
<p>Latitude subset of the dataset for a single time index and longitude index:</p>
<pre class="sourceCode python"><code class="sourceCode python">ds.variables[<span class="st">&quot;air_temperature&quot;</span>][<span class="dv">1</span>,:,<span class="dv">1</span>]</code></pre>
<pre><code>array([ 15.60607338,  15.75792885,  15.88686562,  15.98754311,
        16.24001503,  16.20083046,  16.04287148,  15.91567421,
        15.81768227,  15.81286049,  15.60820961,  15.4258976 ,
        15.15578651,  15.10183144,  14.94161415,  15.23198891,
        15.04085732,  14.88985634,  14.92443275,  14.86251259,
        15.06743813,  14.74593544,  15.25005531], dtype=float32)</code></pre>
<p>Longitude subset of the dataset for a single time and latitude index:</p>
<pre class="sourceCode python"><code class="sourceCode python">ds.variables[<span class="st">&quot;air_temperature&quot;</span>][<span class="dv">1</span>,<span class="dv">1</span>,:]</code></pre>
<pre><code>array([ 15.66430092,  15.75792885,  15.85451698,  15.84597206,
        16.00118446,  16.08132362,  16.10830116,  16.17687416,
        16.23089027,  16.00112343,  16.08196449,  16.15154457,
        15.91655922,  15.95504189,  16.03435707,  15.88466835,
        15.92354774,  15.99114418,  15.97878456,  15.98876381,
        15.98821449,  15.91942787,  15.97271156,  16.09459877,
        15.98848915,  16.09612465,  16.07363319,  16.05709267,
        16.06871986,  16.09444618,  15.97747231,  16.02349281,
        16.04174232,  16.0028019 ,  16.10527992], dtype=float32)</code></pre>
<pre class="sourceCode python"><code class="sourceCode python">ds2=ds.variables[<span class="st">&quot;air_temperature&quot;</span>][:] + <span class="fl">273.15</span>
ds2[<span class="dv">1</span>,<span class="dv">1</span>,:]</code></pre>
<pre><code>array([ 288.81430054,  288.90792847,  289.0045166 ,  288.99597168,
        289.15118408,  289.23132324,  289.25830078,  289.32687378,
        289.38088989,  289.15112305,  289.23196411,  289.30154419,
        289.06655884,  289.1050415 ,  289.18435669,  289.03466797,
        289.07354736,  289.1411438 ,  289.12878418,  289.13876343,
        289.13821411,  289.06942749,  289.12271118,  289.24459839,
        289.13848877,  289.24612427,  289.22363281,  289.20709229,
        289.21871948,  289.2444458 ,  289.12747192,  289.17349243,
        289.19174194,  289.15280151,  289.25527954], dtype=float32)</code></pre>
<p>Now we know enough about reading a netCDF file and selecting data out of it to write a useful script.</p>
<p><strong><code>point_subset.py</code></strong></p>
<pre class="sourceCode python"><code class="sourceCode python"><span class="co">#</span>
<span class="co"># Extract data from netCDF files for use in SWAT.</span>
<span class="co">#</span>
<span class="co"># The netCDF files we&#39;re extracting from are the MACAv2-METDATA data with units</span>
<span class="co"># converted for use with Envision and longitudes in the Western hemisphere</span>
<span class="co"># converted to negative values. Since Envision and SWAT like the same units,</span>
<span class="co"># we&#39;re not doing any unit conversions here like we would if extracting</span>
<span class="co"># directly from the original MACAv2 netCDFs.</span>
<span class="co">#</span>
<span class="co"># We&#39;ve also added netcdf files of daily mean relative humidity (which we&#39;re</span>
<span class="co"># not using in Envision) into our Envision directories to make things easier </span>
<span class="co"># on our scripts.</span>
<span class="co">#</span>
<span class="co"># This script is intended to process an entire model directory. For each</span>
<span class="co"># combination of climate variable and RCP, it reads all the matching files</span>
<span class="co"># as a time series multi-file dataset, then loops over the set of points</span>
<span class="co"># we want to use as SWAT weather stations and writes an output file containing</span>
<span class="co"># the data for that point.</span>
<span class="co">#</span>
<span class="co"># This script requires three command line arguments:</span>
<span class="co"># 1. input directory of netCDF files</span>
<span class="co"># 2. output directory where point subsets will be written (must already exist)</span>
<span class="co"># 3. text file of latitude longitude points (&quot;weather stations&quot;)</span>
<span class="co">#</span>
<span class="co">#</span>
<span class="co"># Evan Linde, Oklahoma State University, 2017-10-10</span>
<span class="co">#</span>

<span class="ch">from</span> netCDF4 <span class="ch">import</span> MFDataset
<span class="ch">import</span> numpy <span class="ch">as</span> np
<span class="ch">import</span> sys
<span class="ch">import</span> os

<span class="co"># Get inputs and output locations from command line arguments</span>
in_dir = sys.argv[<span class="dv">1</span>]
out_dir = sys.argv[<span class="dv">2</span>]    <span class="co"># This directory should already exist.</span>
point_file = sys.argv[<span class="dv">3</span>] <span class="co"># Each line should have lon and lat separated </span>
                         <span class="co"># by a space, e.g. &quot;-97.07 36.122&quot;.</span>


rcps = [<span class="st">&quot;rcp45&quot;</span>, <span class="st">&quot;rcp85&quot;</span>]

<span class="co"># Dictionary of climate variables</span>
<span class="co"># Each key is the variable name in the netcdf file name</span>
<span class="co"># Each value is a list containing:</span>
<span class="co">#      0. the variable name *inside* the netcdf file</span>
<span class="co">#      1. string formatter for the variable (for SWAT)</span>
clim_vars = {<span class="st">&#39;pr&#39;</span>:[<span class="st">&#39;precipitation&#39;</span>,<span class="st">&#39;</span><span class="ot">%5.1f</span><span class="st">&#39;</span>], 
             <span class="co">&#39;rhsmean&#39;</span>:[<span class="st">&#39;relative_humidity&#39;</span>,<span class="st">&#39;</span><span class="ot">%8.3f</span><span class="st">&#39;</span>],
             <span class="co">&#39;rsds&#39;</span>:[<span class="st">&#39;surface_downwelling_shortwave_flux_in_air&#39;</span>,<span class="st">&#39;</span><span class="ot">%8.3f</span><span class="st">&#39;</span>], 
             <span class="co">&#39;tasmax&#39;</span>:[<span class="st">&#39;air_temperature&#39;</span>,<span class="st">&#39;</span><span class="ot">%5.1f</span><span class="st">&#39;</span>], 
             <span class="co">&#39;tasmin&#39;</span>:[<span class="st">&#39;air_temperature&#39;</span>,<span class="st">&#39;</span><span class="ot">%5.1f</span><span class="st">&#39;</span>], 
             <span class="co">&#39;wind&#39;</span>:[<span class="st">&#39;wind_speed&#39;</span>,<span class="st">&#39;</span><span class="ot">%8.3f</span><span class="st">&#39;</span>]}

<span class="co"># Read the point file into a numpy array</span>
points = np.loadtxt(point_file)


<span class="kw">for</span> rcp in rcps:
    <span class="kw">for</span> fvar in clim_vars.keys():
        ncvar = clim_vars[fvar][<span class="dv">0</span>]
        varfmt = clim_vars[fvar][<span class="dv">1</span>]

        <span class="co"># expression for base filename of input file containing both</span>
        <span class="co"># string formatters (e.g. &#39;%s&#39;) and wildcards for globbing</span>
        infilename_expr = <span class="st">&#39;macav2metdata_</span><span class="ot">%s</span><span class="st">_*_r?i1p1_</span><span class="ot">%s</span><span class="st">_*.nc&#39;</span>

        <span class="co"># replace the string formatters with values</span>
        infilename_glob = infilename_expr % (fvar, rcp)

        <span class="co"># combine the input directory with filename glob for a full path glob</span>
        infile_glob = os.path.join(in_dir, infilename_glob)

        <span class="co"># read multi-file dataset (all years for our current variable and rcp)</span>
        mfd = MFDataset(infile_glob, aggdim=<span class="st">&#39;time&#39;</span>)

        <span class="kw">for</span> p in points:
            <span class="co"># each point p is itself an array in the form of [lon, lat]</span>
            <span class="co"># assign longitude and latitude to variables</span>
            lon,lat = p[<span class="dv">0</span>],p[<span class="dv">1</span>]

            <span class="co"># base name for output file</span>
            outfilename = <span class="st">&#39;</span><span class="ot">%.3f</span><span class="st">_</span><span class="ot">%.3f</span><span class="st">_</span><span class="ot">%s</span><span class="st">_</span><span class="ot">%s</span><span class="st">.txt&#39;</span> % (lat, lon, fvar, rcp)

            <span class="co"># full path for output file</span>
            outfile = os.path.join(out_dir, outfilename)

            <span class="co"># find the lat and lon indices in the netcdf dataset</span>
            <span class="co"># that are closest to our point</span>
            latindex = np.where(<span class="dt">abs</span>(lat - mfd.variables[<span class="st">&quot;lat&quot;</span>][:]) &lt; <span class="fl">0.020833</span>)[<span class="dv">0</span>][<span class="dv">0</span>]
            lonindex = np.where(<span class="dt">abs</span>(lon - mfd.variables[<span class="st">&quot;lon&quot;</span>][:]) &lt; <span class="fl">0.020833</span>)[<span class="dv">0</span>][<span class="dv">0</span>]

            <span class="co"># the subset of the netcdf dataset that matches our point</span>
            outdata = mfd.variables[ncvar][:,latindex,lonindex]

            <span class="co"># save the point data to a file</span>
            <span class="co"># using &#39;%.3f&#39; instead of varfmt to match example files I was sent</span>
            np.savetxt(outfile, outdata, fmt=<span class="st">&#39;</span><span class="ot">%.3f</span><span class="st">&#39;</span>, newline=<span class="st">&#39;</span><span class="ch">\r\n</span><span class="st">&#39;</span>, header=<span class="st">&#39;20210101&#39;</span>, comments=<span class="st">&#39;&#39;</span>)</code></pre>
<p>And we create a wrapper script to loop over the model directories and combine the maximum and minimum temperatures into a single two-column file.</p>
<p><strong><code>py_envision_to_swat.sh</code></strong></p>
<pre class="sourceCode bash"><code class="sourceCode bash"><span class="co">#!/bin/bash</span>
<span class="co">#</span>
<span class="co"># This script is a wrapper for the python script point_subset.py which </span>
<span class="co"># processes a single model directory. The purpose of this script is to loop</span>
<span class="co"># over the models and create output directories for the python script.</span>
<span class="co">#</span>
<span class="co"># This script requires the watershed name to be passed in as a command</span>
<span class="co"># line argument.</span>
<span class="co">#</span>

<span class="ot">watershed=</span><span class="st">&quot;</span><span class="ot">$1</span><span class="st">&quot;</span>
<span class="ot">centroids_file=</span><span class="st">&quot;</span><span class="ot">${watershed}</span><span class="st">_centroids.txt&quot;</span>

<span class="co"># Skipping CCSM4 and NorESM1-M since these don&#39;t have relative humidity</span>
<span class="ot">MODELS=(</span><span class="st">&quot;bcc-csm1-1-m&quot;</span> <span class="st">&quot;bcc-csm1-1&quot;</span> <span class="st">&quot;BNU-ESM&quot;</span> <span class="st">&quot;CanESM2&quot;</span> <span class="st">&quot;CNRM-CM5&quot;</span> 
    <span class="st">&quot;CSIRO-Mk3-6-0&quot;</span> <span class="st">&quot;GFDL-ESM2G&quot;</span> <span class="st">&quot;GFDL-ESM2M&quot;</span> <span class="st">&quot;HadGEM2-CC365&quot;</span> <span class="st">&quot;HadGEM2-ES365&quot;</span> 
    <span class="st">&quot;inmcm4&quot;</span> <span class="st">&quot;IPSL-CM5A-LR&quot;</span> <span class="st">&quot;IPSL-CM5A-MR&quot;</span> <span class="st">&quot;IPSL-CM5B-LR&quot;</span> <span class="st">&quot;MIROC-ESM-CHEM&quot;</span> 
    <span class="st">&quot;MIROC-ESM&quot;</span> <span class="st">&quot;MIROC5&quot;</span> <span class="st">&quot;MRI-CGCM3&quot;</span>)

<span class="ot">RCPS=(</span><span class="st">&quot;rcp45&quot;</span> <span class="st">&quot;rcp85&quot;</span><span class="ot">)</span>

<span class="ot">sourcepdir=</span><span class="st">&quot;/data/public/datasets/MACA/MACAv2_Derived/</span><span class="ot">${watershed}</span><span class="st">_Envision&quot;</span>
<span class="ot">destpdir=</span><span class="st">&quot;/data/public/datasets/MACA/MACAv2_Derived/</span><span class="ot">${watershed}</span><span class="st">_SWAT&quot;</span>

<span class="ot">prefixes=($(</span><span class="kw">awk</span> <span class="st">&#39;{printf(&quot;%.3f_%.3f\n&quot;,$2,$1)}&#39;</span> <span class="ot">${centroids_file}))</span>

<span class="co"># Set PATH variable so that we use the Anaconda python distribution</span>
<span class="ot">PATH=</span>/opt/anaconda3/bin:<span class="ot">$PATH</span>

<span class="kw">for</span> <span class="kw">model</span> in <span class="ot">${MODELS[@]}</span><span class="kw">;</span> <span class="kw">do</span>
    <span class="ot">sourcedir=</span><span class="st">&quot;</span><span class="ot">${sourcepdir}</span><span class="st">/</span><span class="ot">${model}</span><span class="st">&quot;</span>
    <span class="ot">destdir=</span><span class="st">&quot;</span><span class="ot">${destpdir}</span><span class="st">/</span><span class="ot">${model}</span><span class="st">&quot;</span>
    <span class="kw">mkdir</span> -p <span class="ot">${destdir}</span>
    <span class="kw">python</span> point_subset.py <span class="ot">${sourcedir}</span> <span class="ot">${destdir}</span> <span class="ot">${centroids_file}</span>

    <span class="co"># The point_subset.py script creates files with a single variable</span>
    <span class="co"># SWAT wants a two-column file with tasmax and tasmin separated by commas</span>
    <span class="kw">for</span> <span class="kw">p</span> in <span class="ot">${prefixes[@]}</span><span class="kw">;</span> <span class="kw">do</span>
        <span class="kw">for</span> <span class="kw">rcp</span> in <span class="ot">${RCPS[@]}</span><span class="kw">;</span> <span class="kw">do</span>
            <span class="ot">temperature_file=</span><span class="st">&quot;</span><span class="ot">${destdir}</span><span class="st">/</span><span class="ot">${p}</span><span class="st">_temperature_</span><span class="ot">${rcp}</span><span class="st">.txt&quot;</span>
            <span class="kw">paste</span> -d, <span class="ot">${destdir}</span>/<span class="ot">${p}</span>_tasm<span class="dt">{ax,in}_${rcp}</span>.txt <span class="kw">&gt;</span> <span class="ot">${temperature_file}</span>
            <span class="co"># Get rid of the duplicated header (remove the comma and everything</span>
            <span class="co"># after it on the first line) and remove mid-line carriage returns </span>
            <span class="co"># on all lines.</span>
            <span class="kw">sed</span> -i <span class="st">&#39;1s/,.*//;s/\r,/,/g&#39;</span> <span class="ot">${temperature_file}</span>
        <span class="kw">done</span>  <span class="co"># end rcp loop</span>
    <span class="kw">done</span>  <span class="co"># end point prefix loop</span>
<span class="kw">done</span>  <span class="co"># end model loop</span></code></pre>
<h3 id="cimarron-2">Cimarron</h3>
<p>Run the script to extract data for SWAT:</p>
<pre class="sourceCode bash"><code class="sourceCode bash"><span class="kw">bash</span> py_envision_to_swat.sh Cimarron</code></pre>
<p>This took 64 minutes and the output totals about 2.6 GB.</p>
<h1 id="metdata">METDATA</h1>
<p>(MACA Training Set)</p>
<p>The METDATA dataset is the observational counterpart to MACAv2-METDATA.</p>
<p>We want to repeat the same process for METDATA that we used for MACAv2-METDATA. Since the dataset is observational, GCMs and RCPs won't figure in, meaning there's a lot less looping. Additionally the set of variables provided differs from MACAv2-METDATA and source file names are constructed differently (and much more simply).</p>
<p>We want to get the same variables that we're using in Envision and SWAT: specific humidity (sph), surface radiation (srad), precipitation (pr), minimum and maximum temperature (tmmn and tmmx), minimum and maximum relative humidity (rmin and rmax), and wind speed (vs).</p>
<h2 id="downloading-1">Downloading</h2>
<p>This dataset can be found at <a href="https://climate.northwestknowledge.net/METDATA/">https://climate.northwestknowledge.net/METDATA/</a>.</p>
<p>By clicking the Direct Download link, we can see that the file URLs are all in the form <code>https://www.northwestknowledge.net/metdata/data/{var}_{year}.nc</code>.</p>
<p>This makes our download script very simple. All we need to do is loop over the variables and years to get all the files.</p>
<p><strong><code>download.sh</code></strong></p>
<pre class="sourceCode bash"><code class="sourceCode bash"><span class="co">#!/bin/bash</span>
<span class="kw">for</span> <span class="kw">v</span> in <span class="dt">{pr,sph,srad,tmmx,tmmn,rmax,rmin,vs}</span><span class="kw">;</span> <span class="kw">do</span>
    <span class="kw">for</span> <span class="kw">y</span> in <span class="dt">{1979..2017}</span><span class="kw">;</span> <span class="kw">do</span>
        <span class="ot">url=</span><span class="st">&quot;http://www.northwestknowledge.net/metdata/data/</span><span class="ot">${v}</span><span class="st">_</span><span class="ot">${y}</span><span class="st">.nc&quot;</span>
        <span class="kw">wget</span> <span class="ot">${url}</span>
    <span class="kw">done</span>
<span class="kw">done</span></code></pre>
<p>Just like with MACAv2-METDATA, we run this script from within a <code>screen</code> session on TIGER's data transfer node to take advantage of the fast connection and mitigate the risk of having the script killed because of a connection interruption.</p>
<pre class="sourceCode bash"><code class="sourceCode bash"><span class="kw">bash</span> download.sh</code></pre>
<p>The download completed in about 80 minutes and totalled about 343 GB.</p>
<h2 id="examining-the-data-1">Examining the Data</h2>
<p>As with MACAv2-METDATA, we examine the file headers to see what we're working with:</p>
<pre class="sourceCode bash"><code class="sourceCode bash"><span class="kw">ncdump</span> -h vs_2017.nc</code></pre>
<p>Here's the header for the file:</p>
<pre><code>vs_2017.nc 
netcdf vs_2017 {
dimensions:
    lon = 1386 ;
    lat = 585 ;
    day = 283 ;
variables:
    double lon(lon) ;
        lon:units = &quot;degrees_east&quot; ;
        lon:description = &quot;longitude&quot; ;
    double lat(lat) ;
        lat:units = &quot;degrees_north&quot; ;
        lat:description = &quot;latitude&quot; ;
    float day(day) ;
        day:units = &quot;days since 1900-01-01 00:00:00&quot; ;
        day:calendar = &quot;gregorian&quot; ;
        day:description = &quot;days since 1900-01-01&quot; ;
    float wind_speed(day, lon, lat) ;
        wind_speed:units = &quot;m/s&quot; ;
        wind_speed:description = &quot;Daily Mean Wind Speed&quot; ;
        wind_speed:_FillValue = -32767.f ;
        wind_speed:esri_pe_string = &quot;GEOGCS[\\\&quot;GCS_WGS_1984\\\&quot;,DATUM[\\\&quot;D_WGS_1984\\\&quot;,SPHEROID[\\\&quot;WGS_1984\\\&quot;,6378137.0,298.257223563]],PRIMEM[\\\&quot;Greenwich\\\&quot;,0.0],UNIT[\\\&quot;Degree\\\&quot;,0.0174532925199433]]&quot; ;
        wind_speed:coordinates = &quot;lon lat&quot; ;
        wind_speed:height = &quot;10 m&quot; ;
        wind_speed:missing_value = -32767. ;

// global attributes:
        :author = &quot;John Abatzoglou - University of Idaho, jabatzoglou@uidaho.edu&quot; ;
        :datee = &quot;11 October 2017&quot; ;
        :note1 = &quot;The projection information for this file is: GCS WGS 1984.&quot; ;
        :note2 = &quot;Citation: Abatzoglou, J.T., 2013, Development of gridded surface meteorological data for ecological applications and modeling, International Journal of Climatology, DOI: 10.1002/joc.3413&quot; ;
        :last_permanent_slice = &quot;223&quot; ;
        :last_provisional_slice = &quot;277&quot; ;
        :note3 = &quot;Data in slices after last_permanent_slice (1-based) are considered provisional and subject to change with subsequent updates&quot; ;
        :note4 = &quot;Data in slices after last_provisional_slice (1-based) are considered early and subject to change with subsequent updates&quot; ;
        :note5 = &quot;Days correspond approximately to calendar days ending at midnight, Mountain Standard Time (7 UTC the next calendar day)&quot; ;
}</code></pre>
<p>We notice that the temporal dimension is called <code>day</code> instead of <code>time</code> and that it is defined against the same reference data as in MACAv2-METDATA.</p>
<p>The order of the dimensions in the variable definition (i.e. <code>float wind_speed(day, lon, lat) ;</code>) is different than in MACAv2-METDATA where the order was time, lat, lon.</p>
<p>From looking at files for each of the variables, we see that the dimension order and dimension names are consistent and that all the climate variable names declared inside the netCDF files match MACAv2-METDATA except for precipitation which METDATA has named <code>precipitation_amount</code>.</p>
<p>Looking at the latitude and longitude values with commands <code>ncdump -v lat vs_2017.nc</code> and <code>ncdump -v lon vs_2017.nc</code>, we see that (most noticeably) Western hemisphere longitudes have negative values (unlile MACAv2-METDATA) and that overall the points are essentially the same as those found in MACAv2-METDATA, but not <em>exactly</em> the same. They seem to generally match up to three or four digits past the decimal point.</p>
<h2 id="subsetting-1">Subsetting</h2>
<p>Unlike MACAv2-METDATA, the METDATA files already use negative longitude values for the Western hemisphere, and the latitude and longitude coordinates only match MACAv2-METDATA up to the first 3 digits or so past the decimal point, so we need to re-figure our subset commands.</p>
<p>The generic command form to greate a geographic subset of the METDATA netcdf files is like this:</p>
<pre><code>ncks -d lat,min_value,max_value -d lon,min_value,max_value infile outfile</code></pre>
<p>We can get the distinct latitude coordinates in the files by dumping the <code>lat</code> variable:</p>
<pre><code>ncdump -v lat pr_2017.nc</code></pre>
<p>We also do the same for longitude by dumping the <code>lon</code> variable:</p>
<pre><code>ncdump -v lon pr_2017.nc</code></pre>
<h3 id="cimarron-3">Cimarron</h3>
<p>From our earlier examination of the shapefile in QGIS, we have the bounds of the Cimarron watershed shape:</p>
<ul>
<li>N bound: 37.3697°<br /></li>
<li>S bound: 35.3783°<br /></li>
<li>E bound: -95.9538°<br /></li>
<li>W bound: -100.119°</li>
</ul>
<p>The associated file coordinates are:</p>
<ul>
<li>Closest to N bound: 37.3543561299642; next higher: 37.3960227966309<br /></li>
<li>Closest to S bound: 35.3960227966309; next lower: 35.3543561299642<br /></li>
<li>Closest to W bound: -100.105496724447; next lower: -100.147163391113<br /></li>
<li>Closest to E bound: -95.93883005778; next higher: -95.8971633911133</li>
</ul>
<p>So our command to create a Cimarron subset should look like this:</p>
<pre><code>ncks -d lat,35.3960227966309,37.3543561299642 -d lon,-100.105496724447,-95.93883005778 infile outfile</code></pre>
<p>Test:</p>
<pre><code>ncks -d lat,35.3960227966309,37.3543561299642 -d lon,-100.105496724447,-95.93883005778 ../METDATA/pr_2017.nc cimtest.nc</code></pre>
<p>Based on our previous experience with the MACAv2-METDATA, we want to check whether our minimum and maximum latitude and longitude values were included. If not then we'll need to revise our subset command.</p>
<p>To get the maximum and minimum longitude values in our test file, we run this command:</p>
<pre class="sourceCode bash"><code class="sourceCode bash"><span class="kw">ncdump</span> -v lon cimtest.nc <span class="kw">|</span> <span class="kw">sed</span> -n <span class="st">&#39;/^ lon = /,/;/p&#39;</span> <span class="kw">|</span> <span class="kw">tr</span> -d <span class="st">&#39;\n&#39;</span> <span class="kw">|</span> <span class="kw">awk</span> <span class="st">&#39;{print $3,$(NF-1)}&#39;</span></code></pre>
<p>Here are the results: <code>-100.105496724447, -95.9804967244466</code>. We see that the Eastern boundary was not included.</p>
<p>Similarly, this command will show us the maximum and minimum latitude in our test file:</p>
<pre class="sourceCode bash"><code class="sourceCode bash"><span class="kw">ncdump</span> -v lat cimtest.nc <span class="kw">|</span> <span class="kw">sed</span> -n <span class="st">&#39;/^ lat = /,/;/p&#39;</span> <span class="kw">|</span> <span class="kw">tr</span> -d <span class="st">&#39;\n&#39;</span> <span class="kw">|</span> <span class="kw">awk</span> <span class="st">&#39;{print $3,$(NF-1)}&#39;</span></code></pre>
<p>Here are the results: <code>37.3543561299642, 35.4376894632975</code>. We see that the Southern boundary was not included.</p>
<p>Since the Eastern and Southern boundaries were not included, we adjust our subset command using the next coordinates:</p>
<pre><code>ncks -d lat,35.3543561299642,37.3543561299642 -d lon,-100.105496724447,-95.8971633911133 infile outfile</code></pre>
<p>Test again:</p>
<pre class="sourceCode bash"><code class="sourceCode bash"><span class="kw">rm</span> cimtest.nc
<span class="kw">ncks</span> -d lat,35.3543561299642,37.3543561299642 -d lon,-100.105496724447,-95.8971633911133 ../METDATA/pr_2017.nc cimtest.nc
<span class="kw">ncdump</span> -v lon cimtest.nc <span class="kw">|</span> <span class="kw">sed</span> -n <span class="st">&#39;/^ lon = /,/;/p&#39;</span> <span class="kw">|</span> <span class="kw">tr</span> -d <span class="st">&#39;\n&#39;</span> <span class="kw">|</span> <span class="kw">awk</span> <span class="st">&#39;{print $3,$(NF-1)}&#39;</span>
<span class="kw">ncdump</span> -v lat cimtest.nc <span class="kw">|</span> <span class="kw">sed</span> -n <span class="st">&#39;/^ lat = /,/;/p&#39;</span> <span class="kw">|</span> <span class="kw">tr</span> -d <span class="st">&#39;\n&#39;</span> <span class="kw">|</span> <span class="kw">awk</span> <span class="st">&#39;{print $3,$(NF-1)}&#39;</span></code></pre>
<p>This time we see that our test file's longitude endpoints are <code>-100.105496724447, -95.93883005778</code> and its latitude endpoints are <code>37.3543561299642, 35.3960227966309</code>. We're still missing the Southern and Eastern endpoints defined in the command but we have the endpoints we actually want.</p>
<p>Create the Cimarron subset files:</p>
<pre class="sourceCode bash"><code class="sourceCode bash"><span class="kw">mkdir</span> Cimarron
<span class="kw">for</span> <span class="kw">v</span> in <span class="dt">{pr,sph,srad,tmmx,tmmn,rmax,rmin,vs}</span><span class="kw">;</span> <span class="kw">do</span>
    <span class="kw">for</span> <span class="kw">y</span> in <span class="dt">{1979..2017}</span><span class="kw">;</span> <span class="kw">do</span>
        <span class="ot">filename=</span><span class="st">&quot;</span><span class="ot">${v}</span><span class="st">_</span><span class="ot">${y}</span><span class="st">.nc&quot;</span>
        <span class="kw">ncks</span> -3 -d lat,35.3543561299642,37.3543561299642 -d lon,-100.105496724447,-95.8971633911133 ../METDATA/<span class="ot">${filename}</span> Cimarron/<span class="ot">${filename}</span>
    <span class="kw">done</span>
<span class="kw">done</span></code></pre>
<p>This took about 48 minutes. The size of the Cimarron subset is about 2.1 GB.</p>
<h3 id="kiamichi-2">Kiamichi</h3>
<p>From our earlier examination of the shapefile in QGIS, we have the bounds of the Kiamichi watershed shape:</p>
<ul>
<li>N bound: 34.8217°<br /></li>
<li>S bound: 33.9145°<br /></li>
<li>E bound: -94.456°<br /></li>
<li>W bound: -95.8222°</li>
</ul>
<p>The associated file coordinates are:</p>
<ul>
<li>Closest to N bound: 34.8126894632975; next higher: 34.8543561299642; --&gt; 34.813<br /></li>
<li>Closest to S bound: 33.8960227966309; next lower: 33.8543561299642; --&gt; 33.895<br /></li>
<li>Closest to W bound: -95.81383005778; next lower: -95.8554967244466; --&gt; -95.814<br /></li>
<li>Closest to E bound: -94.43883005778; next higher: -94.3971633911133; --&gt; -94.437</li>
</ul>
<p>So our command to create a Kiamichi subset should look like this:</p>
<pre><code>ncks -d lat,33.8960227966309,34.8126894632975 -d lon,-95.81383005778,-94.43883005778 infile outfile</code></pre>
<p>Test:</p>
<pre><code>ncks -d lat,33.8960227966309,34.8126894632975 -d lon,-95.81383005778,-94.43883005778 ../METDATA/pr_2017.nc kiatest.nc</code></pre>
<p>Now we check the longitude and latitude endpoints</p>
<pre><code>ncdump -v lon kiatest.nc | sed -n &#39;/^ lon = /,/;/p&#39; | tr -d &#39;\n&#39; | awk &#39;{print $3,$(NF-1)}&#39;
ncdump -v lat kiatest.nc | sed -n &#39;/^ lat = /,/;/p&#39; | tr -d &#39;\n&#39; | awk &#39;{print $3,$(NF-1)}&#39;</code></pre>
<p>The test file has longitude endpoints <code>-95.81383005778, -94.4804967244466</code> and latitude endpoints <code>34.7710227966309, 33.9376894632975</code>. We see that it is missing the Northern, Southern, and Eastern boundaries defined in our subset command, so we'll need to adjust our subset command with the next coordinates.</p>
<pre><code>ncks -d lat,33.8543561299642,34.8543561299642 -d lon,-95.81383005778,-94.3971633911133 infile outfile</code></pre>
<p>So we test again:</p>
<pre class="sourceCode bash"><code class="sourceCode bash"><span class="kw">rm</span> kiatest.nc
<span class="kw">ncks</span> -d lat,33.8543561299642,34.8543561299642 -d lon,-95.81383005778,-94.3971633911133 ../METDATA/pr_2017.nc kiatest.nc
<span class="kw">ncdump</span> -v lon kiatest.nc <span class="kw">|</span> <span class="kw">sed</span> -n <span class="st">&#39;/^ lon = /,/;/p&#39;</span> <span class="kw">|</span> <span class="kw">tr</span> -d <span class="st">&#39;\n&#39;</span> <span class="kw">|</span> <span class="kw">awk</span> <span class="st">&#39;{print $3,$(NF-1)}&#39;</span>
<span class="kw">ncdump</span> -v lat kiatest.nc <span class="kw">|</span> <span class="kw">sed</span> -n <span class="st">&#39;/^ lat = /,/;/p&#39;</span> <span class="kw">|</span> <span class="kw">tr</span> -d <span class="st">&#39;\n&#39;</span> <span class="kw">|</span> <span class="kw">awk</span> <span class="st">&#39;{print $3,$(NF-1)}&#39;</span></code></pre>
<p>Now the test file has longitude endpoints <code>-95.81383005778, -94.43883005778</code> and latitude endpoints <code>34.8543561299642, 33.8960227966309</code>. We notice that the Northern endpoint is exactly what we specified (and one farther than we need) and that the other endpoints are exactly what we want.</p>
<p>Since these coordinate problems are likely due to floating point rounding errors, we try picking coordinates $\pm$0.001 (depending on direction) from our desired endpoints. (The distance between points is approximately 0.0417 degrees, so this is still very close to the target coordinate.)</p>
<pre><code>ncks -d lat,33.895,34.813 -d lon,-95.814,-94.437 infile outfile</code></pre>
<p>And we test yet again:</p>
<pre class="sourceCode bash"><code class="sourceCode bash"><span class="kw">rm</span> kiatest.nc
<span class="kw">ncks</span> -d lat,33.895,34.813 -d lon,-95.814,-94.437 ../METDATA/pr_2017.nc kiatest.nc
<span class="kw">ncdump</span> -v lon kiatest.nc <span class="kw">|</span> <span class="kw">sed</span> -n <span class="st">&#39;/^ lon = /,/;/p&#39;</span> <span class="kw">|</span> <span class="kw">tr</span> -d <span class="st">&#39;\n&#39;</span> <span class="kw">|</span> <span class="kw">awk</span> <span class="st">&#39;{print $3,$(NF-1)}&#39;</span>
<span class="kw">ncdump</span> -v lat kiatest.nc <span class="kw">|</span> <span class="kw">sed</span> -n <span class="st">&#39;/^ lat = /,/;/p&#39;</span> <span class="kw">|</span> <span class="kw">tr</span> -d <span class="st">&#39;\n&#39;</span> <span class="kw">|</span> <span class="kw">awk</span> <span class="st">&#39;{print $3,$(NF-1)}&#39;</span></code></pre>
<p>And we see that the test file's longitude endpoints are <code>-95.81383005778, -94.43883005778</code> and it's latitude endpoints are <code>34.8126894632975, 33.8960227966309</code>, exactly what we wanted.</p>
<p>Create the Kiamichi subset files:</p>
<pre class="sourceCode bash"><code class="sourceCode bash"><span class="kw">mkdir</span> Kiamichi
<span class="kw">for</span> <span class="kw">v</span> in <span class="dt">{pr,sph,srad,tmmx,tmmn,rmax,rmin,vs}</span><span class="kw">;</span> <span class="kw">do</span>
    <span class="kw">for</span> <span class="kw">y</span> in <span class="dt">{1979..2017}</span><span class="kw">;</span> <span class="kw">do</span>
        <span class="ot">filename=</span><span class="st">&quot;</span><span class="ot">${v}</span><span class="st">_</span><span class="ot">${y}</span><span class="st">.nc&quot;</span>
        <span class="kw">ncks</span> -3 -d lat,33.895,34.813 -d lon,-95.814,-94.437 ../METDATA/<span class="ot">${filename}</span> Kiamichi/<span class="ot">${filename}</span>
    <span class="kw">done</span>
<span class="kw">done</span></code></pre>
<p>This took about 37 minutes. The size of the Kiamichi subset is about 341 MB.</p>
<h2 id="converting-for-envision">Converting for Envision</h2>
<p>The METDATA dataset already has longitude coordinates that Envision can use and the wind speed variable is provided, so all we need to do is convert Kelvins to degrees Celsius and calculate the mean daily temperature.</p>
<p>However, there is a format issue with the files themselves that we'll also need to correct. The files don't adhere to CF-conventions and have the variable dimensions ordered in a way that GDAL doesn't interpret correctly. CF-conventions for dimension ordering are to define a variable like this <code>float varname(time, lat, lon)</code> but the METDATA files have their variables declared like <code>float varname(day, lon, lat)</code>. ArcGIS has no problem interpreting these files correctly, but GDAL (and QGIS) swap the latitude and longitude.</p>
<p><strong>Re-order dimensions</strong></p>
<p>Ensure that the <code>lat</code> dimension is ordered before the <code>lon</code> dimension. We can do this with the <a href="http://nco.sourceforge.net/nco.html#ncpdq"><code>ncpdq</code></a> (netCDF Permute Dimensions Quickly) command:</p>
<pre><code>ncpdq -a lat,lon infile.nc outfile.nc</code></pre>
<p>The <code>-a</code> flag specifies that we want to re-order the dimensions and we specify the dimensions to be re-ordered as a comma-separated list (with no spaces). It isn't necessary to list all the dimensions, just the ones whose order needs to be changed.</p>
<p><strong>Temperature conversion</strong></p>
<p>Convert Kelvins to degrees Celsius.</p>
<pre><code>ncap2 -s &#39;air_temperature=(air_temperature - 273.15)&#39; infile.nc outfile.nc
ncatted -a units,air_temperature,o,c,&#39;C&#39; infile.nc [outfile.nc]</code></pre>
<p><strong>Producing mean temperature netCDF</strong></p>
<p>We don't have to change variable names here because both tasmin and tasmax files use the same variable name <code>air_temperature</code>.</p>
<pre><code>nces tmmx.nc tmmn.nc tmmean.nc
ncatted -a description,air_temperature,o,c,&#39;Daily Mean Temperature&#39; tmmean.nc</code></pre>
<p><strong>Producing mean relative humidity netCDF</strong></p>
<pre><code>nces rmax.nc rmin.nc rmean.nc
ncatted -a description,relative_humidity,o,c,&#39;Daily Mean Relative Humidity&#39; -a cell_methods,relative_humidity,o,c,&#39;time: mean(interval: 24 hours)&#39; rmean.nc</code></pre>
<p>Now that we have our basic steps, we write the script for the conversion.</p>
<p><strong><code>metdata_subset_to_envision.sh</code></strong></p>
<pre class="sourceCode bash"><code class="sourceCode bash"><span class="co">#!/bin/bash</span>
<span class="co">#</span>
<span class="co"># Convert watershed-specific subsets from METDATA into a format usable</span>
<span class="co"># in Envision. Envision-compatible netCDF files need to be in netCDF 3</span>
<span class="co"># (classic) format and require dimension ordering like (time, lat, lon)</span>
<span class="co"># instead of (time, lon, lat).</span>
<span class="co">#</span>
<span class="co"># Also convert relative humidity (for later conversion to SWAT) even though</span>
<span class="co"># it&#39;s not needed for Envision.</span>
<span class="co">#</span>
<span class="co"># Additionally we will need to change some units and derive new variables.</span>
<span class="co">#</span>
<span class="co"># Convert temperature units from Kelvins to degrees Celsius.</span>
<span class="co">#</span>
<span class="co"># Calculate mean daily temperatures as:</span>
<span class="co">#     tasmean = mean(tasmax, tasmin)</span>
<span class="co">#</span>
<span class="co">#</span>
<span class="co"># This script requires the watershed name to be passed in as a command line</span>
<span class="co"># argument.</span>
<span class="co">#</span>
<span class="co"># </span>
<span class="co"># Evan Linde, Oklahoma State University, 2017-10-09</span>
<span class="co">#</span>


<span class="kw">if [</span> <span class="st">&quot;</span><span class="ot">$#</span><span class="st">&quot;</span> <span class="ot">-ne</span> 1<span class="kw"> ]</span>; <span class="kw">then</span>
    <span class="kw">echo</span> <span class="st">&quot;This script requires the watershed directory name to be&quot;</span>
    <span class="kw">echo</span> <span class="st">&quot;included as a command line argument.&quot;</span>
    <span class="kw">exit</span> 1<span class="kw">;</span>
<span class="kw">fi</span>
<span class="ot">watershed=</span><span class="st">&quot;</span><span class="ot">$1</span><span class="st">&quot;</span> 

<span class="co"># directories</span>
<span class="ot">sourcedir=</span><span class="st">&quot;/data/public/datasets/MACA/METDATA_Derived/</span><span class="ot">${watershed}</span><span class="st">&quot;</span>
<span class="ot">destdir=</span><span class="st">&quot;/data/public/datasets/MACA/METDATA_Derived/</span><span class="ot">${watershed}</span><span class="st">_Envision&quot;</span>
<span class="kw">mkdir</span> -p <span class="ot">${destdir}</span>


<span class="co"># Create a temp directory</span>
<span class="ot">tmpdir=$(</span><span class="kw">mktemp</span> -d<span class="ot">)</span>

<span class="kw">for</span> <span class="kw">year</span> in <span class="dt">{1979..2017}</span><span class="kw">;</span> <span class="kw">do</span>

    <span class="co"># Only correct dimensions for these variables</span>
    <span class="kw">for</span> <span class="kw">var</span> in <span class="dt">{pr,sph,srad,vs}</span><span class="kw">;</span> <span class="kw">do</span>
        <span class="ot">infile=</span><span class="st">&quot;</span><span class="ot">${sourcedir}</span><span class="st">/</span><span class="ot">${var}</span><span class="st">_</span><span class="ot">${year}</span><span class="st">.nc&quot;</span>
        <span class="ot">outfile=</span><span class="st">&quot;</span><span class="ot">${destdir}</span><span class="st">/</span><span class="ot">${var}</span><span class="st">_</span><span class="ot">${year}</span><span class="st">.nc&quot;</span>
        <span class="kw">ncpdq</span> -3 -a lat,lon <span class="ot">${infile}</span> <span class="ot">${outfile}</span>
    <span class="kw">done</span>

    <span class="co"># Rename precipitation variable so it will match the MACAv2 data</span>
    <span class="co"># This isn&#39;t strictly necessary for Envision but makes it easier</span>
    <span class="co"># to use both datasets.</span>
    <span class="kw">ncrename</span> -v precipitation_amount,precipitation <span class="ot">${destdir}</span>/pr_<span class="ot">${year}</span>.nc

    <span class="co"># Convert temperatures from Kelvins to degrees Celsius</span>
    <span class="co"># and re-order dimensions</span>
    <span class="kw">for</span> <span class="kw">var</span> in tmmn tmmx<span class="kw">;</span> <span class="kw">do</span>
        <span class="ot">infile=</span><span class="st">&quot;</span><span class="ot">${sourcedir}</span><span class="st">/</span><span class="ot">${var}</span><span class="st">_</span><span class="ot">${year}</span><span class="st">.nc&quot;</span>
        <span class="ot">outfile=</span><span class="st">&quot;</span><span class="ot">${destdir}</span><span class="st">/</span><span class="ot">${var}</span><span class="st">_</span><span class="ot">${year}</span><span class="st">.nc&quot;</span>
        <span class="ot">tmpfile=</span><span class="st">&quot;</span><span class="ot">${tmpdir}</span><span class="st">/</span><span class="ot">${var}</span><span class="st">_</span><span class="ot">${year}</span><span class="st">.nc&quot;</span>
        <span class="kw">ncap2</span> -s <span class="st">&#39;air_temperature=(air_temperature - 273.15)&#39;</span> <span class="ot">${infile}</span> <span class="ot">${tmpfile}</span>
        <span class="kw">ncatted</span> -a units,air_temperature,o,char,<span class="st">&#39;C&#39;</span> <span class="ot">${tmpfile}</span>
        <span class="kw">ncpdq</span> -3 -a lat,lon <span class="ot">${tmpfile}</span> <span class="ot">${outfile}</span>
        <span class="kw">rm</span> <span class="ot">${tmpfile}</span>
    <span class="kw">done</span>

    <span class="co"># Create the mean temperature file</span>
    <span class="ot">infiles=(${destdir}</span>/tmm<span class="dt">{x,n}_${year}</span>.nc<span class="ot">)</span>  <span class="co"># array of two filenames</span>
    <span class="ot">outfile=</span><span class="st">&quot;</span><span class="ot">${destdir}</span><span class="st">/tmmean_</span><span class="ot">${year}</span><span class="st">.nc&quot;</span>
    <span class="kw">nces</span> <span class="ot">${infiles[@]}</span> <span class="ot">${outfile}</span>
    <span class="co"># Update metadata</span>
    <span class="kw">ncatted</span> -a description,air_temperature,o,c,<span class="st">&#39;Daily Mean Temperature&#39;</span> <span class="ot">${outfile}</span>

    <span class="co"># Create mean relative humidity file</span>
    <span class="ot">infiles=(${sourcedir}</span>/rm<span class="dt">{in,ax}_${year}</span>.nc<span class="ot">)</span>  <span class="co"># array of two filenames</span>
    <span class="ot">outfile=</span><span class="st">&quot;</span><span class="ot">${destdir}</span><span class="st">/rmean_</span><span class="ot">${year}</span><span class="st">.nc&quot;</span>
    <span class="ot">tmpfile=</span><span class="st">&quot;</span><span class="ot">${tmpdir}</span><span class="st">/rmean_</span><span class="ot">${year}</span><span class="st">.nc&quot;</span>
    <span class="kw">nces</span> <span class="ot">${infiles[@]}</span> <span class="ot">${tmpfile}</span>
    <span class="kw">ncpdq</span> -3 -a lat,lon <span class="ot">${tmpfile}</span> <span class="ot">${outfile}</span>
    <span class="kw">rm</span> <span class="ot">${tmpfile}</span>
    <span class="kw">ncatted</span> -a description,relative_humidity,o,c,<span class="st">&#39;Daily Mean Relative Humidity&#39;</span> -a cell_methods,relative_humidity,o,c,<span class="st">&#39;time: mean(interval: 24 hours)&#39;</span> <span class="ot">${outfile}</span>

<span class="kw">done</span>  <span class="co"># end of years loop</span>

<span class="co"># Cleanup: remove the temp directory we created at the beginning</span>
<span class="kw">rm</span> -rf <span class="ot">${tmpdir}</span></code></pre>
<h3 id="kiamichi-3">Kiamichi</h3>
<p>Run the script to convert the subset data into Envision format:</p>
<pre class="sourceCode bash"><code class="sourceCode bash"><span class="kw">bash</span> metdata_subset_to_envision.sh Kiamichi</code></pre>
<p>The conversion took about 33 seconds. The total size is about 602 MB.</p>
<p>Now that we have the files' dimensions reordered where GDAL can interpret everything properly, we can see how the shapefile looks over the netCDF raster. Note that we did better on our coordinates this time; there aren't any rows or columns of cells that the shape doesn't intersect.</p>
<p><img src="METDATA_Derived/Kiamichi.png" alt="METDATA_Derived/Kiamichi.png" /></p>
<h3 id="cimarron-4">Cimarron</h3>
<p>Run the script to convert the subset data into Envision format:</p>
<pre class="sourceCode bash"><code class="sourceCode bash"><span class="kw">bash</span> subset_to_envision.sh Cimarron</code></pre>
<p>The conversion took about two minutes. The total size is about 2.1 GB.</p>
<p>Here is the Cimarron shape on top of one of the Cimarron Envision netCDF rasters. Note that there aren't any rows or columns of cells that the shape doesn't intersect.</p>
<p><img src="METDATA_Derived/Cimarron.png" alt="METDATA_Derived/Cimarron.png" /></p>
<h2 id="converting-for-swat">Converting for SWAT</h2>
<p>Converting METDATA to SWAT will be basically the same as converting MACAv2-METDATA to SWAT.</p>
<p>Here are the major steps:</p>
<ol>
<li>Find centroids of watershed sub-polygons<br /></li>
<li>Find grid cell closest to centroid<br /></li>
<li>Subset data to each grid cell<br /></li>
<li>Dump to plain text format for SWAT</li>
</ol>
<p>Since we've already done MACA and we created netCDF files of relative humidity earlier in during our Envision data conversion, all we need to do is grab the centroids file(s) we already created and make a copy of our python and bash wrapper scripts for extracting point data that we can adjust for METDATA.</p>
<pre class="sourceCode bash"><code class="sourceCode bash"><span class="kw">cp</span> ../MACAv2_Derived/Cimarron_centroids.txt ./
<span class="kw">cp</span> ../MACAv2_Derived/point_subset.py ./metdata_point_subset.py
<span class="kw">cp</span> ../MACAv2_Derived/py_envision_to_swat.sh ./metdata_envision_to_swat.sh</code></pre>
<p>The main things we'll need to change in these scripts are:</p>
<ol>
<li>Variable names that appear in file names<br /></li>
<li>Overall file name pattern<br /></li>
<li>Remove model loops<br /></li>
<li>Remove RCP loops</li>
</ol>
<p><strong><code>metdata_point_subset.py</code></strong></p>
<pre class="sourceCode python"><code class="sourceCode python"><span class="co">#</span>
<span class="co"># Extract data from netCDF files for use in SWAT.</span>
<span class="co">#</span>
<span class="co"># The netCDF files we&#39;re extracting from are the METDATA data with units</span>
<span class="co"># converted for use with Envision. Since Envision and SWAT like the same units,</span>
<span class="co"># we&#39;re not doing any unit conversions here like we would if extracting</span>
<span class="co"># directly from the original METDATA netCDFs.</span>
<span class="co">#</span>
<span class="co"># We&#39;ve also added netcdf files of daily mean relative humidity (which we&#39;re</span>
<span class="co"># not using in Envision) into our Envision directories to make things easier </span>
<span class="co"># on our scripts.</span>
<span class="co">#</span>
<span class="co"># This script is intended to process a geographical subset of METDATA. For each</span>
<span class="co"># climate variable, it reads all the matching files as a time series multi-file</span>
<span class="co"># dataset, then loops over the set of points we want to use as SWAT weather </span>
<span class="co"># stations and writes an output file containing the data for that point.</span>
<span class="co">#</span>
<span class="co"># This script requires three command line arguments:</span>
<span class="co"># 1. input directory of netCDF files</span>
<span class="co"># 2. output directory where point subsets will be written (must already exist)</span>
<span class="co"># 3. text file of latitude longitude points (&quot;weather stations&quot;)</span>
<span class="co">#</span>
<span class="co">#</span>
<span class="co"># Evan Linde, Oklahoma State University, 2017-10-11</span>
<span class="co">#</span>

<span class="ch">from</span> netCDF4 <span class="ch">import</span> MFDataset
<span class="ch">import</span> numpy <span class="ch">as</span> np
<span class="ch">import</span> sys
<span class="ch">import</span> os

<span class="co"># Get inputs and output locations from command line arguments</span>
in_dir = sys.argv[<span class="dv">1</span>]
out_dir = sys.argv[<span class="dv">2</span>]    <span class="co"># This directory should already exist.</span>
point_file = sys.argv[<span class="dv">3</span>] <span class="co"># Each line should have lon and lat separated </span>
                         <span class="co"># by a space, e.g. &quot;-97.07 36.122&quot;.</span>

<span class="co"># Dictionary of climate variables</span>
<span class="co"># Each key is the variable name in the netcdf file name</span>
<span class="co"># Each value is the variable name *inside* the netcdf file</span>
clim_vars = {<span class="st">&#39;pr&#39;</span>:<span class="st">&#39;precipitation&#39;</span>,
             <span class="co">&#39;rmean&#39;</span>:<span class="st">&#39;relative_humidity&#39;</span>,
             <span class="co">&#39;srad&#39;</span>:<span class="st">&#39;surface_downwelling_shortwave_flux_in_air&#39;</span>,
             <span class="co">&#39;tmmx&#39;</span>:<span class="st">&#39;air_temperature&#39;</span>,
             <span class="co">&#39;tmmn&#39;</span>:<span class="st">&#39;air_temperature&#39;</span>,
             <span class="co">&#39;vs&#39;</span>:<span class="st">&#39;wind_speed&#39;</span>}

<span class="co"># Read the point file into a numpy array</span>
points = np.loadtxt(point_file)


<span class="kw">for</span> fvar in clim_vars.keys():
    ncvar = clim_vars[fvar]

    <span class="co"># file names will have the form of fvar_year.nc</span>
    infilename_glob = fvar+<span class="st">&#39;_*.nc&#39;</span> 

    <span class="co"># combine the input directory with filename glob for a full path glob</span>
    infile_glob = os.path.join(in_dir, infilename_glob)

    <span class="co"># read multi-file dataset (all years for our current variable)</span>
    mfd = MFDataset(infile_glob, aggdim=<span class="st">&#39;day&#39;</span>)

    <span class="kw">for</span> p in points:
        <span class="co"># each point p is itself an array in the form of [lon, lat]</span>
        <span class="co"># assign longitude and latitude to variables</span>
        lon,lat = p[<span class="dv">0</span>],p[<span class="dv">1</span>]

        <span class="co"># base name for output file</span>
        outfilename = <span class="st">&#39;</span><span class="ot">%.3f</span><span class="st">_</span><span class="ot">%.3f</span><span class="st">_</span><span class="ot">%s</span><span class="st">.txt&#39;</span> % (lat, lon, fvar)

        <span class="co"># full path for output file</span>
        outfile = os.path.join(out_dir, outfilename)

        <span class="co"># find the lat and lon indices in the netcdf dataset</span>
        <span class="co"># that are closest to our point</span>
        latindex = np.where(<span class="dt">abs</span>(lat - mfd.variables[<span class="st">&quot;lat&quot;</span>][:]) &lt; <span class="fl">0.020833</span>)[<span class="dv">0</span>][<span class="dv">0</span>]
        lonindex = np.where(<span class="dt">abs</span>(lon - mfd.variables[<span class="st">&quot;lon&quot;</span>][:]) &lt; <span class="fl">0.020833</span>)[<span class="dv">0</span>][<span class="dv">0</span>]

        <span class="co"># the subset of the netcdf dataset that matches our point</span>
        outdata = mfd.variables[ncvar][:,latindex,lonindex]

        <span class="co"># save the point data to a file</span>
        np.savetxt(outfile, outdata, fmt=<span class="st">&#39;</span><span class="ot">%.3f</span><span class="st">&#39;</span>, newline=<span class="st">&#39;</span><span class="ch">\r\n</span><span class="st">&#39;</span>, header=<span class="st">&#39;19790101&#39;</span>, comments=<span class="st">&#39;&#39;</span>)</code></pre>
<p>Just for fun, here's the same script in 5 lines. No comments, no unnecessary whitespace, and no variables explicitly declared unless they have to be. If anyone gives you code like this they must either hate you or be trying way too hard to show off.</p>
<p><strong><code>evil_metdata_point_subset.py</code></strong></p>
<pre class="sourceCode python"><code class="sourceCode python"><span class="ch">import</span> netCDF4,numpy,sys,os
<span class="kw">for</span> fvar,ncvar in {<span class="st">&#39;pr&#39;</span>:<span class="st">&#39;precipitation&#39;</span>,<span class="st">&#39;rmean&#39;</span>:<span class="st">&#39;relative_humidity&#39;</span>,<span class="st">&#39;srad&#39;</span>:<span class="st">&#39;surface_downwelling_shortwave_flux_in_air&#39;</span>,<span class="st">&#39;tmmx&#39;</span>:<span class="st">&#39;air_temperature&#39;</span>,<span class="st">&#39;tmmn&#39;</span>:<span class="st">&#39;air_temperature&#39;</span>,<span class="st">&#39;vs&#39;</span>:<span class="st">&#39;wind_speed&#39;</span>}.items():
    mfd=netCDF4.MFDataset(os.path.join(sys.argv[<span class="dv">1</span>],fvar+<span class="st">&#39;*.nc&#39;</span>),aggdim=<span class="st">&#39;day&#39;</span>)
    <span class="kw">for</span> p in numpy.loadtxt(sys.argv[<span class="dv">3</span>]):
        numpy.savetxt(os.path.join(sys.argv[<span class="dv">2</span>],<span class="st">&#39;</span><span class="ot">%.3f</span><span class="st">_</span><span class="ot">%.3f</span><span class="st">_</span><span class="ot">%s</span><span class="st">.txt&#39;</span>%(p[<span class="dv">1</span>],p[<span class="dv">0</span>],fvar)),mfd.variables[ncvar][:,numpy.where(<span class="dt">abs</span>(p[<span class="dv">1</span>]-mfd.variables[<span class="st">&quot;lat&quot;</span>][:])&lt;<span class="fl">0.020833</span>)[<span class="dv">0</span>][<span class="dv">0</span>],numpy.where(<span class="dt">abs</span>(p[<span class="dv">0</span>]-mfd.variables[<span class="st">&quot;lon&quot;</span>][:])&lt;<span class="fl">0.020833</span>)[<span class="dv">0</span>][<span class="dv">0</span>]],fmt=<span class="st">&#39;</span><span class="ot">%.3f</span><span class="st">&#39;</span>,newline=<span class="st">&#39;</span><span class="ch">\r\n</span><span class="st">&#39;</span>,header=<span class="st">&#39;19790101&#39;</span>,comments=<span class="st">&#39;&#39;</span>)</code></pre>
<p><strong><code>metdata_envision_to_swat.sh</code></strong></p>
<pre class="sourceCode bash"><code class="sourceCode bash"><span class="co">#!/bin/bash</span>
<span class="co">#</span>
<span class="co"># This script is a wrapper for the python script metdata_point_subset.py </span>
<span class="co"># which processes a directory containing a geographical subset of the METDATA</span>
<span class="co"># dataset.</span>
<span class="co">#</span>
<span class="co"># This script performs some post-processing to create a SWAT temperature</span>
<span class="co"># file with max and min temperatures.</span>
<span class="co">#</span>
<span class="co"># This script requires the watershed name to be passed in as a command</span>
<span class="co"># line argument.</span>
<span class="co">#</span>

<span class="ot">watershed=</span><span class="st">&quot;</span><span class="ot">$1</span><span class="st">&quot;</span>
<span class="ot">centroids_file=</span><span class="st">&quot;</span><span class="ot">${watershed}</span><span class="st">_centroids.txt&quot;</span>

<span class="ot">sourcedir=</span><span class="st">&quot;/data/public/datasets/MACA/METDATA_Derived/</span><span class="ot">${watershed}</span><span class="st">_Envision&quot;</span>
<span class="ot">destdir=</span><span class="st">&quot;/data/public/datasets/MACA/METDATA_Derived/</span><span class="ot">${watershed}</span><span class="st">_SWAT&quot;</span>
<span class="kw">mkdir</span> -p <span class="ot">${destdir}</span>

<span class="ot">prefixes=($(</span><span class="kw">awk</span> <span class="st">&#39;{printf(&quot;%.3f_%.3f\n&quot;,$2,$1)}&#39;</span> <span class="ot">${centroids_file}))</span>

<span class="co"># Set PATH variable so that we use the Anaconda python distribution</span>
<span class="ot">PATH=</span>/opt/anaconda3/bin:<span class="ot">$PATH</span>

<span class="kw">python</span> metdata_point_subset.py <span class="ot">${sourcedir}</span> <span class="ot">${destdir}</span> <span class="ot">${centroids_file}</span>

<span class="co"># The point_subset.py script creates files with a single variable</span>
<span class="co"># SWAT wants a two-column file with tasmax and tasmin separated by commas</span>
<span class="kw">for</span> <span class="kw">p</span> in <span class="ot">${prefixes[@]}</span><span class="kw">;</span> <span class="kw">do</span>
    <span class="ot">temperature_file=</span><span class="st">&quot;</span><span class="ot">${destdir}</span><span class="st">/</span><span class="ot">${p}</span><span class="st">_temperature.txt&quot;</span>
    <span class="kw">paste</span> -d, <span class="ot">${destdir}</span>/<span class="ot">${p}</span>_tmm<span class="dt">{x,n}</span>.txt <span class="kw">&gt;</span> <span class="ot">${temperature_file}</span>
    <span class="co"># Get rid of the duplicated header (remove the comma and everything</span>
    <span class="co"># after it on the first line) and remove mid-line carriage returns </span>
    <span class="co"># on all lines.</span>
    <span class="kw">sed</span> -i <span class="st">&#39;1s/,.*//;s/\r,/,/g&#39;</span> <span class="ot">${temperature_file}</span>
<span class="kw">done</span>  <span class="co"># end point prefix loop</span></code></pre>
<h3 id="cimarron-5">Cimarron</h3>
<p>Run the script to convert the data into SWAT format:</p>
<pre class="sourceCode bash"><code class="sourceCode bash"><span class="kw">bash</span> metdata_envision_to_swat.sh Cimarron</code></pre>
<p>This took about 42 seconds. The total size of Cimarron_SWAT is about 28 MB. (The evil version fared the same.)</p>
<h1 id="conclusion">Conclusion</h1>
<h2 id="subsets">Subsets</h2>
<p>We used up a lot of narrative describing selecting proper subsets, particularly in selecting minimum and maximum latitude and longitude values for the <code>ncks</code> subset commands. This is a reflection of what we <em>actually</em> did and not what we <em>wish</em> we had done.</p>
<p>Our final procedure for selecting good subsets, or at least the coordinates to use for our subset commands, was to first pick the coordinate points <em>closest</em> to our desired boundary points, then to add or subtract 0.001 (depending on direction) and truncate to three places after the decimal point. This was sufficient to eliminate floating point error and make our subset commands not have such ridiculously long numbers.</p>
<h2 id="ensembles">Ensembles</h2>
<p>With more attention to the MACA documentation at the beginning, particularly the <a href="https://climate.northwestknowledge.net/MACA/GCMs.php">GCMs page</a>, we would have known that the the strings <code>r1i1p1</code> and <code>r6i1p1</code> refer to the ensemble used for each model's data and that <code>r6i1p1</code> is only used for the CCSM4 model.</p>
<p>With this knowledge we could have added a single line to our MACAv2-METDATA download script and used the same line in the script where we converted the subsets for Envision instead of <a href="https://en.wikipedia.org/wiki/Glob_(programming)">globbing</a> for the correct file names.</p>
<pre class="sourceCode bash"><code class="sourceCode bash"><span class="kw">[[</span> <span class="st">&quot;</span><span class="ot">${model}</span><span class="st">&quot;</span> <span class="ot">==</span> <span class="st">&quot;CCSM4&quot;</span><span class="kw"> ]]</span> <span class="kw">&amp;&amp;</span> <span class="ot">ensemble=</span><span class="st">&quot;r6i1p1&quot;</span> <span class="kw">||</span> <span class="ot">ensemble=</span><span class="st">&quot;r1i1p1&quot;</span></code></pre>
<p>The line is functionally equivalent to this more readable <code>if</code> statement:</p>
<pre class="sourceCode bash"><code class="sourceCode bash"><span class="kw">if [[</span> <span class="st">&quot;</span><span class="ot">${model}</span><span class="st">&quot;</span> <span class="ot">==</span> <span class="st">&quot;CCSM4&quot;</span><span class="kw"> ]]</span>; <span class="kw">then</span>
    <span class="ot">ensemble=</span><span class="st">&quot;r6i1p1&quot;</span>
<span class="kw">else</span>
    <span class="ot">ensemble=</span><span class="st">&quot;r1i1p1&quot;</span>
<span class="kw">fi</span></code></pre>
<p>Explanation for both of these can be found in the <code>bash</code> manual under the topics <a href="https://www.gnu.org/software/bash/manual/html_node/Conditional-Constructs.html">Conditional Constructs</a> and <a href="https://www.gnu.org/software/bash/manual/html_node/Bash-Conditional-Expressions.html">Bash Conditional Expressions</a>.</p>
<p>Additionally, we would have reused the same fully correct loop for the subset step instead of creating the literal list of files.</p>
<h2 id="missing-variables">Missing Variables</h2>
<p>Similarly to how we could have handled the ensembles, we could also have handled the missing <code>rhsmax</code> and <code>rhsmin</code> variables for the CCSM4 and NorESM1-M models in the download and subset scripts by adding a line like this inside (at the beginning of) the variables (<code>${wvar}</code>) loop.</p>
<pre class="sourceCode bash"><code class="sourceCode bash"><span class="kw">[[</span> <span class="st">&quot;</span><span class="ot">${wvar}</span><span class="st">&quot;</span> <span class="ot">==</span> <span class="st">&quot;rhsmax&quot;</span> || <span class="st">&quot;</span><span class="ot">${wvar}</span><span class="st">&quot;</span> <span class="ot">==</span> <span class="st">&quot;rhsmin&quot;</span><span class="kw"> ]]</span> <span class="kw">&amp;&amp; [[</span> <span class="st">&quot;</span><span class="ot">${model}</span><span class="st">&quot;</span> <span class="ot">==</span> <span class="st">&quot;CCSM4&quot;</span> || <span class="st">&quot;</span><span class="ot">${model}</span><span class="st">&quot;</span> <span class="ot">==</span> <span class="st">&quot;NorESM1-M&quot;</span><span class="kw"> ]]</span> <span class="kw">&amp;&amp;</span> <span class="kw">continue</span></code></pre>
<p>The <code>continue</code> command causes the loop to jump to the next iteration. With this line added just inside the <code>${wvar}</code> loop, we would skip to the next <code>${wvar}</code>. This command is explained in the <code>bash</code> manual under the topic <a href="https://www.gnu.org/software/bash/manual/html_node/Bourne-Shell-Builtins.html">Bourne Shell Builtins</a>.</p>
<p>The line above is logically equivalent to this slightly more readable code:</p>
<pre class="sourceCode bash"><code class="sourceCode bash"><span class="kw">if [[</span> <span class="st">&quot;</span><span class="ot">${wvar}</span><span class="st">&quot;</span> <span class="ot">==</span> <span class="st">&quot;rhsmax&quot;</span> || <span class="st">&quot;</span><span class="ot">${wvar}</span><span class="st">&quot;</span> <span class="ot">==</span> <span class="st">&quot;rhsmin&quot;</span><span class="kw"> ]]</span>; <span class="kw">then</span>
    <span class="kw">if [[</span> <span class="st">&quot;</span><span class="ot">${model}</span><span class="st">&quot;</span> <span class="ot">==</span> <span class="st">&quot;CCSM4&quot;</span> || <span class="st">&quot;</span><span class="ot">${model}</span><span class="st">&quot;</span> <span class="ot">==</span> <span class="st">&quot;NorESM1-M&quot;</span><span class="kw"> ]]</span>; <span class="kw">then</span>
        <span class="kw">continue</span>
    <span class="kw">fi</span>
<span class="kw">fi</span></code></pre>
<h2 id="useful-services">Useful Services</h2>
<p>We could have used existing services to make subsets rather than downloading entire datasets. If we hadn't had sufficient free space available (about 12.5 TB for the unmodified originals, plus almost 1 TB for subsets and converted data so far), this would have been a necessity rather than a choice.</p>
<p>If using any of these subset services for METDATA or MACAv2-METDATA and you don't already know the closest coordinates to your shapefile boundary, a good rule of thumb would be to add or subtract 0.05 (depending on direction) from the shapefile's boundary coordinates. This will be sufficient to cover the shapefile and shouldn't get you more than an extra column or row of cells than you need in each direction. (Remember that the cell size is <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mfrac><mn>1</mn><mn>24</mn></mfrac></mrow></math> degree; i.e. <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mn>0</mn><mo>.</mo><mn>041</mn><mover><mn>6</mn><mo accent="true">¯</mo></mover></mrow></math>.)</p>
<h3 id="maca-data-portal">MACA Data Portal</h3>
<p>On MACA's <a href="https://climate.northwestknowledge.net/MACA/data_portal.php">data portal</a> under the domain option, you can choose a rectangular subset and enter your own coordinates (best to add a little padding as mentioned above). Then download a bash script of <code>curl</code> or <code>wget</code> commands that you can run to download all the matching files.</p>
<p>Note that for the time period, you may see the option to choose the full range of years in a single file (for each model, rcp, and variable combination). This may work very well if your subset is small, but may give you trouble with some of the NCO operations. (Some NCO operations require enough available RAM to accommodate the size of the file(s) involved, but I don't recall if this includes anything we've demonstrated here.) More clicking is involved, but it's also possible to choose each of the year blocks instead of a monolithic range, resulting in a subset comparable to what we started with when converting data for Envision.</p>
<p>Setting the domain to a point location allows you to download CSV files that are relatively easy to process for SWAT. The CSV files should look similar to this:</p>
<pre><code>#Variables:
#pr(mm):Precipitation
&quot;#Data Extracted for Average over Grid Cell w/ center: 41.9795 Latitude, -110.9807 Longitude (closest value to Latitude:42.0, Longitude:-111.0) &quot;
#Original Data File(s):
#macav2metdata_pr_CCSM4_r6i1p1_rcp85_2091_2095_CONUS_daily.nc 
#===============================================
&quot;yyyy-mm-dd,pr(mm)&quot;
&quot;&quot;
yyyy-mm-dd,pr(mm)
2091-01-01,0.000000
2091-01-02,5.652015
2091-01-03,7.541029
2091-01-04,1.810457
...</code></pre>
<p>If you need to download multiple points, you could edit the script of <code>wget</code> or <code>curl</code> commands and replace the literal latitude and longitude values with variables and then loop over your set of points, however as mentioned in the <a href="https://climate.northwestknowledge.net/MACA/data_portal.php#bestPractices">best practices</a> section, if you have very many points, it may be better to create a rectangular subset and then extract your points from that.</p>
<p>The MACA Data Portal itself serves as a nicer front-end to THREDDS. Links to the THREDDS can be found at <a href="https://climate.northwestknowledge.net/MACA/data_catalogs.php">https://climate.northwestknowledge.net/MACA/data_catalogs.php</a>.</p>
<h3 id="thredds-catalog-for-metdata">THREDDS Catalog for METDATA</h3>
<p>Subsets of METDATA are available via THREDDS. To access METDATA via THREDDS, go to the main METDATA page, <a href="https://climate.northwestknowledge.net/METDATA/">https://climate.northwestknowledge.net/METDATA/</a>, click the &quot;DOWNLOAD DATA&quot; tab, and then click the link <a href="http://thredds.northwestknowledge.net:8080/thredds/reacch_climate_MET_aggregated_catalog.html">Aggregated THREDDS Catalog</a>.</p>
<p>For each variable you're interested in downloading, follow the link for that variable, then go to the NetcdfSubset link, and do the following:</p>
<ol>
<li>Click the checkbox for the variable<br /></li>
<li>On the right side of the screen uncheck the &quot;Disable horizontal subsetting&quot; checkbox<br /></li>
<li>Enter the desired latitude and longitude coordinates (with some extra padding, see note above)<br /></li>
<li>Choose your time range<br /></li>
<li>Click the option to &quot;Add Lat/Lon variables&quot;<br /></li>
<li>Choose the output format</li>
</ol>
<p>You'll notice that a long URL appears at the bottom of the screen. This URL changes with your selections.</p>
<p>Click the Submit button to start downloading the data you requested.</p>
<p>Or you could copy the URL and download it with a command like this. (Note the quotes around the URL itself.)</p>
<pre class="sourceCode bash"><code class="sourceCode bash"><span class="kw">curl</span> -o whatever_you_want_to_call_it.nc <span class="st">&#39;http://thredds.northwestknowledge.net:8080/thredds/ncss/agg_met_vs_1979_CurrentYear_CONUS.nc?var=daily_mean_wind_speed&amp;north=38&amp;west=-104&amp;east=-94&amp;south=33&amp;disableProjSubset=on&amp;horizStride=1&amp;time_start=2015-01-01T00%3A00%3A00Z&amp;time_end=2016-12-31T00%3A00%3A00Z&amp;timeStride=1&amp;addLatLon=true&amp;accept=netcdf&#39;</span></code></pre>
<p>The variable names inside the subset files downloaded this way don't all match the directly downloaded files, and they're not not already divided up into yearly chunks, so using this method would require some changes to other parts of our procedure.</p>
<h3 id="opendap">OPeNDAP</h3>
<p>Both MACAv2-METDATA and METDATA can be accessed online using <a href="https://en.wikipedia.org/wiki/OPeNDAP">OPeNDAP</a> at their THREDDS server. This allows you to access the data online without having to download it first. The MACA site provides a <a href="https://climate.northwestknowledge.net/MACA/OPENDAP.php">tutorial</a> with examples for Python, R, and Matlab.</p>
<p>This would be particularly useful for creating the SWAT files.</p>
<h2 id="lessons-learned">Lessons Learned</h2>
<p>Check a subset file to make sure you got what you were aiming for <em>before</em> running against the entire dataset. The <code>ncdump</code> command is great for this and viewing geographic subsets against your shapefile in QGIS or ArcGIS is even better.</p>
<p>And pay attention to the metadata, both the netCDF metadata and what you find online. It really is helpful; you may even find nice tools that will do useful things for you.</p>
</body>
</html>
